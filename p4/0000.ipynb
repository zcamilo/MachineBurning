{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: TkAgg\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab\n",
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load data and shuffle it\n",
    "train_labeled = pd.read_hdf(\"train_labeled.h5\", \"train\")\n",
    "train_labeled=train_labeled.iloc[np.random.permutation(len(train_labeled))]\n",
    "\n",
    "train_unlabeled = pd.read_hdf(\"train_unlabeled.h5\", \"train\")\n",
    "train_unlabeled=train_unlabeled.iloc[np.random.permutation(len(train_unlabeled))]\n",
    "\n",
    "test = pd.read_hdf(\"test.h5\", \"test\")\n",
    "\n",
    "#slice labeled set into training and validation set\n",
    "validation=train_labeled.iloc[:500]\n",
    "train_labeled=train_labeled.iloc[500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>...</th>\n",
       "      <th>x119</th>\n",
       "      <th>x120</th>\n",
       "      <th>x121</th>\n",
       "      <th>x122</th>\n",
       "      <th>x123</th>\n",
       "      <th>x124</th>\n",
       "      <th>x125</th>\n",
       "      <th>x126</th>\n",
       "      <th>x127</th>\n",
       "      <th>x128</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11625</th>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.777344</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.992188</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3848</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.992188</td>\n",
       "      <td>0.414062</td>\n",
       "      <td>0.351562</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.261719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15079</th>\n",
       "      <td>5</td>\n",
       "      <td>0.609375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.988281</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7063</th>\n",
       "      <td>8</td>\n",
       "      <td>0.988281</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.984375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.957031</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.164062</td>\n",
       "      <td>0.988281</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.292969</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12519</th>\n",
       "      <td>5</td>\n",
       "      <td>0.984375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.472656</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.171875</td>\n",
       "      <td>0.859375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       y        x1   x2        x3   x4   x5        x6        x7        x8  \\\n",
       "11625  7  0.000000  0.0  0.000000  0.0  0.0  0.000000  0.000000  0.000000   \n",
       "3848   4  0.000000  0.0  0.000000  0.0  0.0  0.992188  0.414062  0.351562   \n",
       "15079  5  0.609375  0.0  0.000000  0.0  0.0  0.000000  0.000000  0.000000   \n",
       "7063   8  0.988281  0.0  0.984375  0.0  0.0  0.957031  0.250000  0.000000   \n",
       "12519  5  0.984375  0.0  0.472656  0.0  0.0  0.000000  0.000000  0.000000   \n",
       "\n",
       "        x9  ...   x119  x120      x121      x122  x123  x124      x125  x126  \\\n",
       "11625  0.0  ...    0.0   0.0  0.777344  0.000000   0.0   0.0  0.992188   0.0   \n",
       "3848   0.0  ...    0.0   0.0  0.261719  0.000000   0.0   0.0  0.000000   0.0   \n",
       "15079  0.0  ...    0.0   0.0  0.000000  0.000000   0.0   0.0  0.000000   0.0   \n",
       "7063   0.0  ...    0.0   0.0  0.164062  0.988281   0.0   0.0  0.000000   0.0   \n",
       "12519  0.0  ...    0.0   0.0  0.171875  0.859375   0.0   0.0  0.000000   0.0   \n",
       "\n",
       "           x127  x128  \n",
       "11625  0.000000   0.0  \n",
       "3848   0.000000   0.0  \n",
       "15079  0.988281   0.0  \n",
       "7063   0.292969   0.0  \n",
       "12519  0.000000   0.0  \n",
       "\n",
       "[5 rows x 129 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# head is only there to only show the first five rows. without it you'd see the whole dataset\n",
    "train_labeled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>...</th>\n",
       "      <th>x119</th>\n",
       "      <th>x120</th>\n",
       "      <th>x121</th>\n",
       "      <th>x122</th>\n",
       "      <th>x123</th>\n",
       "      <th>x124</th>\n",
       "      <th>x125</th>\n",
       "      <th>x126</th>\n",
       "      <th>x127</th>\n",
       "      <th>x128</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8500.000000</td>\n",
       "      <td>8500.000000</td>\n",
       "      <td>8500.000000</td>\n",
       "      <td>8500.000000</td>\n",
       "      <td>8500.000000</td>\n",
       "      <td>8500.000000</td>\n",
       "      <td>8500.000000</td>\n",
       "      <td>8500.000000</td>\n",
       "      <td>8500.000000</td>\n",
       "      <td>8500.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>8500.000000</td>\n",
       "      <td>8500.000000</td>\n",
       "      <td>8500.000000</td>\n",
       "      <td>8500.000000</td>\n",
       "      <td>8500.000000</td>\n",
       "      <td>8500.000000</td>\n",
       "      <td>8500.000000</td>\n",
       "      <td>8500.0</td>\n",
       "      <td>8500.000000</td>\n",
       "      <td>8500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.490588</td>\n",
       "      <td>0.384602</td>\n",
       "      <td>0.000871</td>\n",
       "      <td>0.487811</td>\n",
       "      <td>0.036042</td>\n",
       "      <td>0.028556</td>\n",
       "      <td>0.491888</td>\n",
       "      <td>0.331702</td>\n",
       "      <td>0.134625</td>\n",
       "      <td>0.063714</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014632</td>\n",
       "      <td>0.007489</td>\n",
       "      <td>0.501658</td>\n",
       "      <td>0.441359</td>\n",
       "      <td>0.003090</td>\n",
       "      <td>0.000759</td>\n",
       "      <td>0.079637</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.106024</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.890768</td>\n",
       "      <td>0.433039</td>\n",
       "      <td>0.025540</td>\n",
       "      <td>0.431405</td>\n",
       "      <td>0.160712</td>\n",
       "      <td>0.142925</td>\n",
       "      <td>0.432394</td>\n",
       "      <td>0.421568</td>\n",
       "      <td>0.303752</td>\n",
       "      <td>0.214728</td>\n",
       "      <td>...</td>\n",
       "      <td>0.105642</td>\n",
       "      <td>0.074550</td>\n",
       "      <td>0.437681</td>\n",
       "      <td>0.430702</td>\n",
       "      <td>0.045984</td>\n",
       "      <td>0.021884</td>\n",
       "      <td>0.238977</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.273621</td>\n",
       "      <td>0.004974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.082031</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.507812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.544922</td>\n",
       "      <td>0.332031</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.945312</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.984375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.984375</td>\n",
       "      <td>0.851562</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.984375</td>\n",
       "      <td>0.976562</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.996094</td>\n",
       "      <td>0.988281</td>\n",
       "      <td>0.996094</td>\n",
       "      <td>0.996094</td>\n",
       "      <td>0.996094</td>\n",
       "      <td>0.996094</td>\n",
       "      <td>0.996094</td>\n",
       "      <td>0.996094</td>\n",
       "      <td>0.996094</td>\n",
       "      <td>...</td>\n",
       "      <td>0.996094</td>\n",
       "      <td>0.996094</td>\n",
       "      <td>0.996094</td>\n",
       "      <td>0.996094</td>\n",
       "      <td>0.988281</td>\n",
       "      <td>0.996094</td>\n",
       "      <td>0.996094</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.996094</td>\n",
       "      <td>0.355469</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 y           x1           x2           x3           x4  \\\n",
       "count  8500.000000  8500.000000  8500.000000  8500.000000  8500.000000   \n",
       "mean      4.490588     0.384602     0.000871     0.487811     0.036042   \n",
       "std       2.890768     0.433039     0.025540     0.431405     0.160712   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       2.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       4.000000     0.082031     0.000000     0.500000     0.000000   \n",
       "75%       7.000000     0.945312     0.000000     0.984375     0.000000   \n",
       "max       9.000000     0.996094     0.988281     0.996094     0.996094   \n",
       "\n",
       "                x5           x6           x7           x8           x9  \\\n",
       "count  8500.000000  8500.000000  8500.000000  8500.000000  8500.000000   \n",
       "mean      0.028556     0.491888     0.331702     0.134625     0.063714   \n",
       "std       0.142925     0.432394     0.421568     0.303752     0.214728   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.507812     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.984375     0.851562     0.000000     0.000000   \n",
       "max       0.996094     0.996094     0.996094     0.996094     0.996094   \n",
       "\n",
       "          ...              x119         x120         x121         x122  \\\n",
       "count     ...       8500.000000  8500.000000  8500.000000  8500.000000   \n",
       "mean      ...          0.014632     0.007489     0.501658     0.441359   \n",
       "std       ...          0.105642     0.074550     0.437681     0.430702   \n",
       "min       ...          0.000000     0.000000     0.000000     0.000000   \n",
       "25%       ...          0.000000     0.000000     0.000000     0.000000   \n",
       "50%       ...          0.000000     0.000000     0.544922     0.332031   \n",
       "75%       ...          0.000000     0.000000     0.984375     0.976562   \n",
       "max       ...          0.996094     0.996094     0.996094     0.996094   \n",
       "\n",
       "              x123         x124         x125    x126         x127         x128  \n",
       "count  8500.000000  8500.000000  8500.000000  8500.0  8500.000000  8500.000000  \n",
       "mean      0.003090     0.000759     0.079637     0.0     0.106024     0.000100  \n",
       "std       0.045984     0.021884     0.238977     0.0     0.273621     0.004974  \n",
       "min       0.000000     0.000000     0.000000     0.0     0.000000     0.000000  \n",
       "25%       0.000000     0.000000     0.000000     0.0     0.000000     0.000000  \n",
       "50%       0.000000     0.000000     0.000000     0.0     0.000000     0.000000  \n",
       "75%       0.000000     0.000000     0.000000     0.0     0.000000     0.000000  \n",
       "max       0.988281     0.996094     0.996094     0.0     0.996094     0.355469  \n",
       "\n",
       "[8 rows x 129 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just playing with pandas\n",
    "train_labeled.describe()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# values turns it into a numpy array, and we can plot it\n",
    "#plt.plot(train_labeled.describe().loc[['mean']].values[0])\n",
    "counter=0\n",
    "zero_cols=[]\n",
    "for i in range(len(train_labeled.describe().loc[['mean']].values[0])):\n",
    "    if train_labeled.describe().loc[['mean']].values[0][i]==0:\n",
    "        zero_cols.append(i)\n",
    "        counter+=1\n",
    "zero_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Format training data\n",
    "DATA_x_train=train_labeled.loc[:,'x1':'x128'].values\n",
    "tmp_DATA_y_=train_labeled['y'].values\n",
    "DATA_y_=[]\n",
    "for label in tmp_DATA_y_:\n",
    "    DATA_y_.append(zeros(10))\n",
    "    DATA_y_[-1][label]=1\n",
    "DATA_y_\n",
    "\n",
    "# Format unlabeled training data\n",
    "DATA_x_train_u=train_unlabeled.loc[:,'x1':'x128'].values\n",
    "\n",
    "# Format validation data\n",
    "valDATA_x=validation.loc[:,'x1':'x128'].values\n",
    "tmp_valDATA_y_=validation['y'].values\n",
    "valDATA_y_=[]\n",
    "for label in tmp_valDATA_y_:\n",
    "    valDATA_y_.append(zeros(10))\n",
    "    valDATA_y_[-1][label]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29500, 128)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nzero_cols=[]\\nDATA_x_train_zusammen=DATA_x_train\\nfor i in range(128):\\n    # dimension level\\n    zero_counter=0\\n    for data_value in DATA_x_train_zusammen[:,i]:\\n        if data_value == 0.:\\n            zero_counter+=1\\n    if zero_counter!=0:\\n        zero_cols.append(zero_counter)\\nzero_cols'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_x_train_zusammen = DATA_x_train.tolist() + DATA_x_train_u.tolist()\n",
    "DATA_x_train_zusammen = np.asarray(DATA_x_train_zusammen)\n",
    "print(shape(DATA_x_train_zusammen))\n",
    "\"\"\"\n",
    "zero_cols=[]\n",
    "DATA_x_train_zusammen=DATA_x_train\n",
    "for i in range(128):\n",
    "    # dimension level\n",
    "    zero_counter=0\n",
    "    for data_value in DATA_x_train_zusammen[:,i]:\n",
    "        if data_value == 0.:\n",
    "            zero_counter+=1\n",
    "    if zero_counter!=0:\n",
    "        zero_cols.append(zero_counter)\n",
    "zero_cols\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Are there zero columns?\n",
    "def is_zero_vector(x):\n",
    "    count = 0\n",
    "    for i in arange(len(x)):\n",
    "        if x[i] == 0.:\n",
    "            count += 1\n",
    "    if count == len(x):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "X_total = DATA_x_train.tolist() + DATA_x_train_u.tolist()\n",
    "zero_i = list([])\n",
    "for k in arange(len(X_total[0,:])):   \n",
    "    if is_zero_vector(X_total[:,k]):\n",
    "        zero_i.append(k)\n",
    "zero_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Start tensorflow session\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Abstract placeholders\n",
    "x = tf.placeholder(tf.float32, shape=[None, 128])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initializing model parameters, W weights and b biases\n",
    "#W=tf.Variable(tf.zeros([128,10]))\n",
    "#b=tf.Variable(tf.zeros([10]))\n",
    "\n",
    "W=tf.Variable(tf.truncated_normal([128,10], stddev=0.1))\n",
    "b=tf.Variable(tf.zeros([10]))\n",
    "\n",
    "V=tf.Variable(tf.zeros([10,10]))\n",
    "c=tf.Variable(tf.zeros([10]))\n",
    "\n",
    "S=tf.Variable(tf.zeros([10,10]))\n",
    "d=tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Defining my network, including the loss function, cross_entropy\n",
    "sess.run(tf.initialize_all_variables())\n",
    "#y=tf.nn.softmax(tf.matmul(x,W)+b)\n",
    "z=tf.nn.relu(tf.matmul(x,W)+b)\n",
    "zz=tf.nn.relu(tf.matmul(z,S)+d)\n",
    "y=tf.nn.softmax(tf.matmul(zz,V)+c)\n",
    "cross_entropy=tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y),reduction_indices=[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.126\n"
     ]
    }
   ],
   "source": [
    "# Gradient Descent with 0.5 step, run a given amount of times. NOTE: I removed the batching part that was in the tf tutorial.\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "for i in range(1):\n",
    "    #batch = mnist.train.next_batch(50)\n",
    "    train_step.run(feed_dict={x:DATA_x_train,y_:DATA_y_})\n",
    "    \n",
    "# Evaluate based on the validation set that we separated in the beginning\n",
    "correct_prediction=tf.equal(tf.argmax(y,1),tf.argmax(y_,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "print(accuracy.eval(feed_dict={x:valDATA_x,y_:valDATA_y_}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means Scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.4375    ,  0.        ,  0.046875  , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ..., \n",
       "       [ 0.        ,  0.        ,  0.94140625, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.921875  ,  0.        ,  0.4921875 , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.00390625, ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_ids=np.zeros(10)\n",
    "labels_tmp=train_labeled['y'].values\n",
    "for i in range(len(labels_tmp)):\n",
    "    for j in range(10):\n",
    "        if labels_tmp[i]==j:\n",
    "            initial_ids[j]=int(i)\n",
    "initial_ids = [int(i) for i in initial_ids]\n",
    "initial_ids\n",
    "init=[]\n",
    "for i in range(10):\n",
    "    init.append(X[initial_ids[i]].tolist())\n",
    "init=np.asarray(init)\n",
    "init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52.247058823529414 % accuracy\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "X=DATA_x_train\n",
    "Y=DATA_y_\n",
    "\n",
    "estimator=KMeans(n_clusters=10,init=init,n_init=1,max_iter=1000000)\n",
    "estimator.fit(X)\n",
    "labels=estimator.labels_\n",
    "tmp_labels=labels\n",
    "PRED_labels=[]\n",
    "for label in tmp_labels:\n",
    "    PRED_labels.append(zeros(10))\n",
    "    PRED_labels[-1][label]=1\n",
    "no_correct=0\n",
    "for j in range(len(PRED_labels)):\n",
    "    label_vector=PRED_labels[j]\n",
    "    correctness=1\n",
    "    for i in range(len(label_vector)):\n",
    "        if DATA_y_[j][i]!=label_vector[i]:\n",
    "            correctness=0\n",
    "    if correctness==1:\n",
    "        no_correct+=1\n",
    "accuracy=no_correct/shape(DATA_x_train)[0]\n",
    "print(100*accuracy,\"% accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8500,)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels=[]\n",
    "for i in DATA_y_:\n",
    "    for j in i:\n",
    "        if j!=0:\n",
    "            labels.append(j)\n",
    "shape(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tamas/anaconda3/envs/tensorflow/lib/python3.5/site-packages/sklearn/preprocessing/data.py:167: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "/home/tamas/anaconda3/envs/tensorflow/lib/python3.5/site-packages/sklearn/preprocessing/data.py:184: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "data = scale(DATA_x_train)\n",
    "pca = PCA(n_components=10).fit(data)\n",
    "reduced_data = PCA(n_components=2).fit_transform(data)\n",
    "shape(reduced_data)\n",
    "for i in range(len(reduced_data)):\n",
    "    plt.scatter(reduced_data[i][0],reduced_data[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tamas/anaconda3/envs/tensorflow/lib/python3.5/site-packages/sklearn/preprocessing/data.py:167: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "/home/tamas/anaconda3/envs/tensorflow/lib/python3.5/site-packages/sklearn/preprocessing/data.py:184: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_digits: 10, \t n_samples 8500, \t n_features 128\n",
      "_______________________________________________________________________________\n",
      "init    time  inertia    homo   compl  v-meas     ARI AMI  silhouette\n",
      "ACCURACY= 0.064\n",
      "k-means++   2.43s    771306   1.000   0.000   0.000   0.000   0.000    0.006\n",
      "ACCURACY= 0.22317647058823528\n",
      "   random   1.72s    771685   1.000   0.000   0.000   0.000   0.000    0.038\n",
      "ACCURACY= 0.06682352941176471\n",
      "PCA-based   0.68s    779155   1.000   0.000   0.000   0.000   0.000    0.014\n",
      "_______________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "#digits = load_digits()\n",
    "data = scale(DATA_x_train)\n",
    "\n",
    "n_samples, n_features = data.shape\n",
    "n_digits = 10#len(np.unique(digits.target))\n",
    "\n",
    "labels=[]\n",
    "for i in DATA_y_:\n",
    "    for j in i:\n",
    "        if j!=0:\n",
    "            labels.append(j)\n",
    "            \n",
    "#labels = DATA_y_#digits.target NOT GOOD TypE\n",
    "\n",
    "sample_size = 300\n",
    "\n",
    "print(\"n_digits: %d, \\t n_samples %d, \\t n_features %d\"\n",
    "      % (n_digits, n_samples, n_features))\n",
    "\n",
    "\n",
    "print(79 * '_')\n",
    "print('% 9s' % 'init'\n",
    "      '    time  inertia    homo   compl  v-meas     ARI AMI  silhouette')\n",
    "\n",
    "def accuracy(labels,labels_):\n",
    "    \n",
    "    no_correct=0\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i]==labels_[i]:\n",
    "            no_correct+=1\n",
    "    acc=no_correct/len(labels)\n",
    "    return acc\n",
    "\n",
    "\n",
    "def bench_k_means(estimator, name, data):\n",
    "    t0 = time()\n",
    "    estimator.fit(data)\n",
    "    print(\"ACCURACY=\",accuracy(labels,estimator.labels_))\n",
    "    print('% 9s   %.2fs    %i   %.3f   %.3f   %.3f   %.3f   %.3f    %.3f'\n",
    "          % (name, (time() - t0), estimator.inertia_,\n",
    "             metrics.homogeneity_score(labels, estimator.labels_),\n",
    "             metrics.completeness_score(labels, estimator.labels_),\n",
    "             metrics.v_measure_score(labels, estimator.labels_),\n",
    "             metrics.adjusted_rand_score(labels, estimator.labels_),\n",
    "             metrics.adjusted_mutual_info_score(labels,  estimator.labels_),\n",
    "             metrics.silhouette_score(data, estimator.labels_,\n",
    "                                      metric='euclidean',\n",
    "                                      sample_size=sample_size)))\n",
    "\n",
    "bench_k_means(KMeans(init='k-means++', n_clusters=n_digits, n_init=10),\n",
    "              name=\"k-means++\", data=data)\n",
    "\n",
    "bench_k_means(KMeans(init='random', n_clusters=n_digits, n_init=10),\n",
    "              name=\"random\", data=data)\n",
    "\n",
    "# in this case the seeding of the centers is deterministic, hence we run the\n",
    "# kmeans algorithm only once with n_init=1\n",
    "pca = PCA(n_components=n_digits).fit(data)\n",
    "bench_k_means(KMeans(init=pca.components_, n_clusters=n_digits, n_init=1),\n",
    "              name=\"PCA-based\",\n",
    "              data=data)\n",
    "print(79 * '_')\n",
    "\n",
    "###############################################################################\n",
    "# Visualize the results on PCA-reduced data\n",
    "\n",
    "reduced_data = PCA(n_components=2).fit_transform(data)\n",
    "kmeans = KMeans(init='k-means++', n_clusters=n_digits, n_init=10)\n",
    "kmeans.fit(reduced_data)\n",
    "\n",
    "# Step size of the mesh. Decrease to increase the quality of the VQ.\n",
    "h = .02     # point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\n",
    "y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "# Obtain labels for each point in mesh. Use last trained model.\n",
    "Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(1)\n",
    "plt.clf()\n",
    "plt.imshow(Z, interpolation='nearest',\n",
    "           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "           cmap=plt.cm.Paired,\n",
    "           aspect='auto', origin='lower')\n",
    "\n",
    "plt.plot(reduced_data[:, 0], reduced_data[:, 1], 'k.', markersize=2)\n",
    "# Plot the centroids as a white X\n",
    "centroids = kmeans.cluster_centers_\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "            marker='x', s=169, linewidths=3,\n",
    "            color='w', zorder=10)\n",
    "plt.title('K-means clustering on the digits dataset (PCA-reduced data)\\n'\n",
    "          'Centroids are marked with white cross')\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.529411764705882 % accuracy\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_features = 128\n",
    "n_clusters = 10\n",
    "\n",
    "centroids, samples = create_samples(n_clusters, n_samples_per_cluster, n_features, embiggen_factor, seed)\n",
    "\n",
    "model = tf.initialize_all_variables()\n",
    "with tf.Session() as session:\n",
    "    sample_values = session.run(samples)\n",
    "    centroid_values = session.run(centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'samples_29:0' shape=(1500, 2) dtype=float32>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_samples(n_clusters, n_samples_per_cluster, n_features, embiggen_factor, seed):\n",
    "    np.random.seed(seed)\n",
    "    slices = []\n",
    "    centroids = []\n",
    "    # Create samples for each cluster\n",
    "    for i in range(n_clusters):\n",
    "        samples = tf.random_normal((n_samples_per_cluster, n_features),\n",
    "                               mean=0.0, stddev=5.0, dtype=tf.float32, seed=seed, name=\"cluster_{}\".format(i))\n",
    "        current_centroid = (np.random.random((1, n_features)) * embiggen_factor) - (embiggen_factor/2)\n",
    "        centroids.append(current_centroid)\n",
    "        samples += current_centroid\n",
    "        slices.append(samples)\n",
    "    # Create a big \"samples\" dataset\n",
    "    samples = tf.concat(0, slices, name='samples')\n",
    "    centroids = tf.concat(0, centroids, name='centroids')\n",
    "    return centroids, samples\n",
    "\n",
    "n_features = 2\n",
    "n_clusters = 3\n",
    "n_samples_per_cluster = 500\n",
    "seed = 700\n",
    "embiggen_factor = 70\n",
    "\n",
    "np.random.seed(seed)\n",
    "slices = []\n",
    "centroids = []\n",
    "# Create samples for each cluster\n",
    "for i in range(n_clusters):\n",
    "    samples = tf.random_normal((n_samples_per_cluster, n_features),\n",
    "                           mean=0.0, stddev=5.0, dtype=tf.float32, seed=seed, name=\"cluster_{}\".format(i))\n",
    "    current_centroid = (np.random.random((1, n_features)) * embiggen_factor) - (embiggen_factor/2)\n",
    "    centroids.append(current_centroid)\n",
    "    samples += current_centroid\n",
    "    slices.append(samples)\n",
    "# Create a big \"samples\" dataset\n",
    "samples = tf.concat(0, slices, name='samples')\n",
    "centroids = tf.concat(0, centroids, name='centroids')\n",
    "#print(shape(samples))\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8500, 128) (10, 128)\n"
     ]
    }
   ],
   "source": [
    "n_features = 2\n",
    "n_clusters = 3\n",
    "n_samples_per_cluster = 500\n",
    "seed = 700\n",
    "embiggen_factor = 70\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "centroids, samples = create_samples(n_clusters, n_samples_per_cluster, n_features, embiggen_factor, seed)\n",
    "\n",
    "model = tf.initialize_all_variables()\n",
    "with tf.Session() as session:\n",
    "    sample_values = session.run(samples)\n",
    "    centroid_values = session.run(centroids)\n",
    "    \n",
    "sample_values=DATA_x_train\n",
    "centroid_values=[DATA_x_train[np.random.choice(128,1)[0]] for i in range(10)]\n",
    "print(shape(sample_values),shape(centroid_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Fetch argument array([ 0.7890625 ,  0.        ,  0.59375   ,  0.        ,  0.        ,\n        0.984375  ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.5546875 ,  0.        ,  0.59765625,  0.        ,  0.        ,\n        0.984375  ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.91796875,  0.        ,  0.        ,  0.        ,\n        0.765625  ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.09765625,  0.984375  ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.98828125,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.9296875 ,  0.984375  ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.38671875,  0.984375  ,  0.953125  ,  0.82421875,  0.        ,\n        0.984375  ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.95703125,  0.        ,  0.98828125,  0.        ,  0.        ,\n        0.        ,  0.90234375,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.484375  ,\n        0.        ,  0.        ,  0.2109375 ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.984375  ,\n        0.        ,  0.5078125 ,  0.        ,  0.        ,  0.        ,\n        0.9453125 ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.5546875 ,  0.        ,  0.54296875,  0.2109375 ,\n        0.2109375 ,  0.        ,  0.98828125,  0.984375  ,  0.19921875,\n        0.94921875,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.1875    ,  0.        ,  0.        ,\n        0.        ,  0.25      ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ], dtype=float32) of array([ 0.7890625 ,  0.        ,  0.59375   ,  0.        ,  0.        ,\n        0.984375  ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.5546875 ,  0.        ,  0.59765625,  0.        ,  0.        ,\n        0.984375  ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.91796875,  0.        ,  0.        ,  0.        ,\n        0.765625  ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.09765625,  0.984375  ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.98828125,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.9296875 ,  0.984375  ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.38671875,  0.984375  ,  0.953125  ,  0.82421875,  0.        ,\n        0.984375  ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.95703125,  0.        ,  0.98828125,  0.        ,  0.        ,\n        0.        ,  0.90234375,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.484375  ,\n        0.        ,  0.        ,  0.2109375 ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.984375  ,\n        0.        ,  0.5078125 ,  0.        ,  0.        ,  0.        ,\n        0.9453125 ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.5546875 ,  0.        ,  0.54296875,  0.2109375 ,\n        0.2109375 ,  0.        ,  0.98828125,  0.984375  ,  0.19921875,\n        0.94921875,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.1875    ,  0.        ,  0.        ,\n        0.        ,  0.25      ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ], dtype=float32) has invalid type <class 'numpy.ndarray'>, must be a string or Tensor. (Can not convert a ndarray into a Tensor or Operation.)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/tamas/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_process_fetches\u001b[1;34m(self, fetches)\u001b[0m\n\u001b[0;32m    479\u001b[0m           fetch_t = self.graph.as_graph_element(subfetch, allow_tensor=True,\n\u001b[1;32m--> 480\u001b[1;33m                                                 allow_operation=True)\n\u001b[0m\u001b[0;32m    481\u001b[0m           \u001b[0mfetch_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch_t\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/tamas/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[1;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[0;32m   2300\u001b[0m       raise TypeError(\"Can not convert a %s into a %s.\"\n\u001b[1;32m-> 2301\u001b[1;33m                       % (type(obj).__name__, types_str))\n\u001b[0m\u001b[0;32m   2302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Can not convert a ndarray into a Tensor or Operation.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-8255f916dd5c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mupdated_centroid_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitial_centroids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[0mupdate_centroids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_values\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0massign_to_nearest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_values\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mupdate_centroid_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/tamas/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    338\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 340\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    341\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/tamas/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m     \u001b[1;31m# Validate and process fetches.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 523\u001b[1;33m     \u001b[0mprocessed_fetches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_fetches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    524\u001b[0m     \u001b[0munique_fetches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocessed_fetches\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m     \u001b[0mtarget_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocessed_fetches\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/tamas/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_process_fetches\u001b[1;34m(self, fetches)\u001b[0m\n\u001b[0;32m    491\u001b[0m           raise TypeError('Fetch argument %r of %r has invalid type %r, '\n\u001b[0;32m    492\u001b[0m                           \u001b[1;34m'must be a string or Tensor. (%s)'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m                           % (subfetch, fetch, type(subfetch), str(e)))\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m           raise ValueError('Fetch argument %r of %r cannot be interpreted as a '\n",
      "\u001b[1;31mTypeError\u001b[0m: Fetch argument array([ 0.7890625 ,  0.        ,  0.59375   ,  0.        ,  0.        ,\n        0.984375  ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.5546875 ,  0.        ,  0.59765625,  0.        ,  0.        ,\n        0.984375  ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.91796875,  0.        ,  0.        ,  0.        ,\n        0.765625  ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.09765625,  0.984375  ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.98828125,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.9296875 ,  0.984375  ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.38671875,  0.984375  ,  0.953125  ,  0.82421875,  0.        ,\n        0.984375  ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.95703125,  0.        ,  0.98828125,  0.        ,  0.        ,\n        0.        ,  0.90234375,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.484375  ,\n        0.        ,  0.        ,  0.2109375 ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.984375  ,\n        0.        ,  0.5078125 ,  0.        ,  0.        ,  0.        ,\n        0.9453125 ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.5546875 ,  0.        ,  0.54296875,  0.2109375 ,\n        0.2109375 ,  0.        ,  0.98828125,  0.984375  ,  0.19921875,\n        0.94921875,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.1875    ,  0.        ,  0.        ,\n        0.        ,  0.25      ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ], dtype=float32) of array([ 0.7890625 ,  0.        ,  0.59375   ,  0.        ,  0.        ,\n        0.984375  ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.5546875 ,  0.        ,  0.59765625,  0.        ,  0.        ,\n        0.984375  ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.91796875,  0.        ,  0.        ,  0.        ,\n        0.765625  ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.09765625,  0.984375  ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.98828125,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.9296875 ,  0.984375  ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.38671875,  0.984375  ,  0.953125  ,  0.82421875,  0.        ,\n        0.984375  ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.95703125,  0.        ,  0.98828125,  0.        ,  0.        ,\n        0.        ,  0.90234375,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.484375  ,\n        0.        ,  0.        ,  0.2109375 ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.984375  ,\n        0.        ,  0.5078125 ,  0.        ,  0.        ,  0.        ,\n        0.9453125 ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.5546875 ,  0.        ,  0.54296875,  0.2109375 ,\n        0.2109375 ,  0.        ,  0.98828125,  0.984375  ,  0.19921875,\n        0.94921875,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.1875    ,  0.        ,  0.        ,\n        0.        ,  0.25      ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ], dtype=float32) has invalid type <class 'numpy.ndarray'>, must be a string or Tensor. (Can not convert a ndarray into a Tensor or Operation.)"
     ]
    }
   ],
   "source": [
    "def update_centroids(samples, nearest_indices, n_clusters):\n",
    "    # Updates the centroid to be the mean of all samples associated with it.\n",
    "    nearest_indices = tf.to_int32(nearest_indices)\n",
    "    partitions = tf.dynamic_partition(samples, nearest_indices, n_clusters)\n",
    "    new_centroids = tf.concat(0, [tf.expand_dims(tf.reduce_mean(partition, 0), 0) for partition in partitions])\n",
    "    return new_centroids\n",
    "\n",
    "def assign_to_nearest(samples, centroids):\n",
    "    # Finds the nearest centroid for each sample\n",
    "\n",
    "    # START from http://esciencegroup.com/2016/01/05/an-encounter-with-googles-tensorflow/\n",
    "    expanded_vectors = tf.expand_dims(samples, 0)\n",
    "    expanded_centroids = tf.expand_dims(centroids, 1)\n",
    "    distances = tf.reduce_sum( tf.square(\n",
    "               tf.sub(expanded_vectors, expanded_centroids)), 2)\n",
    "    mins = tf.argmin(distances, 0)\n",
    "    # END from http://esciencegroup.com/2016/01/05/an-encounter-with-googles-tensorflow/\n",
    "    nearest_indices = mins\n",
    "    return nearest_indices\n",
    "\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, 128])\n",
    "\n",
    "sample_values=DATA_x_train\n",
    "initial_centroids=[DATA_x_train[np.random.choice(128,1)[0]] for i in range(10)]\n",
    "\n",
    "model = tf.initialize_all_variables()\n",
    "\n",
    "with tf.Session() as session:\n",
    "    for i in range(1000):\n",
    "        updated_centroid_value = session.run(initial_centroids)\n",
    "        update_centroids(sample_values,assign_to_nearest(sample_values,update_centroid_value),10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
