{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: TkAgg\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab\n",
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load data and shuffle it\n",
    "train_labeled = pd.read_hdf(\"train_labeled.h5\", \"train\")\n",
    "train_labeled=train_labeled.iloc[np.random.permutation(len(train_labeled))]\n",
    "\n",
    "train_unlabeled = pd.read_hdf(\"train_unlabeled.h5\", \"train\")\n",
    "train_unlabeled=train_unlabeled.iloc[np.random.permutation(len(train_unlabeled))]\n",
    "\n",
    "test = pd.read_hdf(\"test.h5\", \"test\")\n",
    "\n",
    "#slice labeled set into training and validation set\n",
    "validation=train_labeled.iloc[:500]\n",
    "train_labeled=train_labeled.iloc[500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>...</th>\n",
       "      <th>x119</th>\n",
       "      <th>x120</th>\n",
       "      <th>x121</th>\n",
       "      <th>x122</th>\n",
       "      <th>x123</th>\n",
       "      <th>x124</th>\n",
       "      <th>x125</th>\n",
       "      <th>x126</th>\n",
       "      <th>x127</th>\n",
       "      <th>x128</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10545</th>\n",
       "      <td>7</td>\n",
       "      <td>0.992188</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.964844</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.406250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.960938</td>\n",
       "      <td>0.960938</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26430</th>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.996094</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.996094</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.996094</td>\n",
       "      <td>0.996094</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17120</th>\n",
       "      <td>3</td>\n",
       "      <td>0.953125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.660156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.613281</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6672</th>\n",
       "      <td>7</td>\n",
       "      <td>0.988281</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.988281</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.988281</td>\n",
       "      <td>0.882812</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.988281</td>\n",
       "      <td>0.988281</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16178</th>\n",
       "      <td>1</td>\n",
       "      <td>0.941406</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.546875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.101562</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       y        x1   x2        x3   x4   x5        x6        x7   x8   x9  \\\n",
       "10545  7  0.992188  0.0  0.964844  0.0  0.0  0.406250  0.000000  0.0  0.0   \n",
       "26430  8  0.000000  0.0  0.996094  0.0  0.0  0.996094  0.000000  0.0  0.0   \n",
       "17120  3  0.953125  0.0  0.000000  0.0  0.0  0.660156  0.000000  0.0  0.0   \n",
       "6672   7  0.988281  0.0  0.988281  0.0  0.0  0.988281  0.882812  0.0  0.0   \n",
       "16178  1  0.941406  0.0  0.000000  0.0  0.0  0.546875  0.000000  0.0  0.0   \n",
       "\n",
       "       ...   x119  x120      x121      x122  x123  x124  x125  x126  x127  \\\n",
       "10545  ...    0.0   0.0  0.960938  0.960938   0.0   0.0   0.0   0.0   0.0   \n",
       "26430  ...    0.0   0.0  0.996094  0.996094   0.0   0.0   0.0   0.0   0.0   \n",
       "17120  ...    0.0   0.0  0.613281  0.000000   0.0   0.0   0.0   0.0   0.0   \n",
       "6672   ...    0.0   0.0  0.988281  0.988281   0.0   0.0   0.0   0.0   0.0   \n",
       "16178  ...    0.0   0.0  0.101562  0.000000   0.0   0.0   0.0   0.0   0.0   \n",
       "\n",
       "       x128  \n",
       "10545   0.0  \n",
       "26430   0.0  \n",
       "17120   0.0  \n",
       "6672    0.0  \n",
       "16178   0.0  \n",
       "\n",
       "[5 rows x 129 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# head is only there to only show the first five rows. without it you'd see the whole dataset\n",
    "train_labeled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>...</th>\n",
       "      <th>x119</th>\n",
       "      <th>x120</th>\n",
       "      <th>x121</th>\n",
       "      <th>x122</th>\n",
       "      <th>x123</th>\n",
       "      <th>x124</th>\n",
       "      <th>x125</th>\n",
       "      <th>x126</th>\n",
       "      <th>x127</th>\n",
       "      <th>x128</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8500.000000</td>\n",
       "      <td>8500.000000</td>\n",
       "      <td>8500.000000</td>\n",
       "      <td>8500.000000</td>\n",
       "      <td>8500.000000</td>\n",
       "      <td>8500.000000</td>\n",
       "      <td>8500.000000</td>\n",
       "      <td>8500.000000</td>\n",
       "      <td>8500.000000</td>\n",
       "      <td>8500.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>8500.000000</td>\n",
       "      <td>8500.000000</td>\n",
       "      <td>8500.000000</td>\n",
       "      <td>8500.000000</td>\n",
       "      <td>8500.000000</td>\n",
       "      <td>8500.000000</td>\n",
       "      <td>8500.000000</td>\n",
       "      <td>8500.0</td>\n",
       "      <td>8500.000000</td>\n",
       "      <td>8500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.489059</td>\n",
       "      <td>0.384905</td>\n",
       "      <td>0.000946</td>\n",
       "      <td>0.489205</td>\n",
       "      <td>0.035846</td>\n",
       "      <td>0.028178</td>\n",
       "      <td>0.491007</td>\n",
       "      <td>0.331681</td>\n",
       "      <td>0.134516</td>\n",
       "      <td>0.064077</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014275</td>\n",
       "      <td>0.007764</td>\n",
       "      <td>0.502237</td>\n",
       "      <td>0.441481</td>\n",
       "      <td>0.003027</td>\n",
       "      <td>0.000861</td>\n",
       "      <td>0.079183</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.105472</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.890722</td>\n",
       "      <td>0.433216</td>\n",
       "      <td>0.026455</td>\n",
       "      <td>0.431846</td>\n",
       "      <td>0.160082</td>\n",
       "      <td>0.141971</td>\n",
       "      <td>0.432068</td>\n",
       "      <td>0.421576</td>\n",
       "      <td>0.303972</td>\n",
       "      <td>0.215596</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104766</td>\n",
       "      <td>0.076258</td>\n",
       "      <td>0.438154</td>\n",
       "      <td>0.431328</td>\n",
       "      <td>0.044735</td>\n",
       "      <td>0.023816</td>\n",
       "      <td>0.238038</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.273019</td>\n",
       "      <td>0.004974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.082031</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.501953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.550781</td>\n",
       "      <td>0.332031</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.949219</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.984375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.984375</td>\n",
       "      <td>0.851562</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.984375</td>\n",
       "      <td>0.980469</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.996094</td>\n",
       "      <td>0.988281</td>\n",
       "      <td>0.996094</td>\n",
       "      <td>0.996094</td>\n",
       "      <td>0.996094</td>\n",
       "      <td>0.996094</td>\n",
       "      <td>0.996094</td>\n",
       "      <td>0.996094</td>\n",
       "      <td>0.996094</td>\n",
       "      <td>...</td>\n",
       "      <td>0.996094</td>\n",
       "      <td>0.996094</td>\n",
       "      <td>0.996094</td>\n",
       "      <td>0.996094</td>\n",
       "      <td>0.988281</td>\n",
       "      <td>0.996094</td>\n",
       "      <td>0.996094</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.996094</td>\n",
       "      <td>0.355469</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 y           x1           x2           x3           x4  \\\n",
       "count  8500.000000  8500.000000  8500.000000  8500.000000  8500.000000   \n",
       "mean      4.489059     0.384905     0.000946     0.489205     0.035846   \n",
       "std       2.890722     0.433216     0.026455     0.431846     0.160082   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       2.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       4.000000     0.082031     0.000000     0.500000     0.000000   \n",
       "75%       7.000000     0.949219     0.000000     0.984375     0.000000   \n",
       "max       9.000000     0.996094     0.988281     0.996094     0.996094   \n",
       "\n",
       "                x5           x6           x7           x8           x9  \\\n",
       "count  8500.000000  8500.000000  8500.000000  8500.000000  8500.000000   \n",
       "mean      0.028178     0.491007     0.331681     0.134516     0.064077   \n",
       "std       0.141971     0.432068     0.421576     0.303972     0.215596   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.501953     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.984375     0.851562     0.000000     0.000000   \n",
       "max       0.996094     0.996094     0.996094     0.996094     0.996094   \n",
       "\n",
       "          ...              x119         x120         x121         x122  \\\n",
       "count     ...       8500.000000  8500.000000  8500.000000  8500.000000   \n",
       "mean      ...          0.014275     0.007764     0.502237     0.441481   \n",
       "std       ...          0.104766     0.076258     0.438154     0.431328   \n",
       "min       ...          0.000000     0.000000     0.000000     0.000000   \n",
       "25%       ...          0.000000     0.000000     0.000000     0.000000   \n",
       "50%       ...          0.000000     0.000000     0.550781     0.332031   \n",
       "75%       ...          0.000000     0.000000     0.984375     0.980469   \n",
       "max       ...          0.996094     0.996094     0.996094     0.996094   \n",
       "\n",
       "              x123         x124         x125    x126         x127         x128  \n",
       "count  8500.000000  8500.000000  8500.000000  8500.0  8500.000000  8500.000000  \n",
       "mean      0.003027     0.000861     0.079183     0.0     0.105472     0.000100  \n",
       "std       0.044735     0.023816     0.238038     0.0     0.273019     0.004974  \n",
       "min       0.000000     0.000000     0.000000     0.0     0.000000     0.000000  \n",
       "25%       0.000000     0.000000     0.000000     0.0     0.000000     0.000000  \n",
       "50%       0.000000     0.000000     0.000000     0.0     0.000000     0.000000  \n",
       "75%       0.000000     0.000000     0.000000     0.0     0.000000     0.000000  \n",
       "max       0.988281     0.996094     0.996094     0.0     0.996094     0.355469  \n",
       "\n",
       "[8 rows x 129 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just playing with pandas\n",
    "train_labeled.describe()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# values turns it into a numpy array, and we can plot it\n",
    "#plt.plot(train_labeled.describe().loc[['mean']].values[0])\n",
    "counter=0\n",
    "zero_cols=[]\n",
    "for i in range(len(train_labeled.describe().loc[['mean']].values[0])):\n",
    "    if train_labeled.describe().loc[['mean']].values[0][i]==0:\n",
    "        zero_cols.append(i)\n",
    "        counter+=1\n",
    "zero_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Format training data\n",
    "DATA_x_train=train_labeled.loc[:,'x1':'x128'].values\n",
    "tmp_DATA_y_=train_labeled['y'].values\n",
    "DATA_y_=[]\n",
    "for label in tmp_DATA_y_:\n",
    "    DATA_y_.append(zeros(10))\n",
    "    DATA_y_[-1][label]=1\n",
    "DATA_y_\n",
    "\n",
    "# Format unlabeled training data\n",
    "DATA_x_train_u=train_unlabeled.loc[:,'x1':'x128'].values\n",
    "\n",
    "# Format validation data\n",
    "valDATA_x=validation.loc[:,'x1':'x128'].values\n",
    "tmp_valDATA_y_=validation['y'].values\n",
    "valDATA_y_=[]\n",
    "for label in tmp_valDATA_y_:\n",
    "    valDATA_y_.append(zeros(10))\n",
    "    valDATA_y_[-1][label]=1\n",
    "    \n",
    "# Format test data\n",
    "DATA_x_test=test.loc[:,'x1':'x128'].values\n",
    "DATA_id_test=[30000+i for i in range(len(DATA_x_test))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29500, 128)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nzero_cols=[]\\nDATA_x_train_zusammen=DATA_x_train\\nfor i in range(128):\\n    # dimension level\\n    zero_counter=0\\n    for data_value in DATA_x_train_zusammen[:,i]:\\n        if data_value == 0.:\\n            zero_counter+=1\\n    if zero_counter!=0:\\n        zero_cols.append(zero_counter)\\nzero_cols'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_x_train_zusammen = DATA_x_train.tolist() + DATA_x_train_u.tolist()\n",
    "DATA_x_train_zusammen = np.asarray(DATA_x_train_zusammen)\n",
    "print(shape(DATA_x_train_zusammen))\n",
    "\"\"\"\n",
    "zero_cols=[]\n",
    "DATA_x_train_zusammen=DATA_x_train\n",
    "for i in range(128):\n",
    "    # dimension level\n",
    "    zero_counter=0\n",
    "    for data_value in DATA_x_train_zusammen[:,i]:\n",
    "        if data_value == 0.:\n",
    "            zero_counter+=1\n",
    "    if zero_counter!=0:\n",
    "        zero_cols.append(zero_counter)\n",
    "zero_cols\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-156eb3d7e306>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mX_total\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDATA_x_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mDATA_x_train_u\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mzero_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_total\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mis_zero_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_total\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mzero_i\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "#Are there zero columns?\n",
    "def is_zero_vector(x):\n",
    "    count = 0\n",
    "    for i in arange(len(x)):\n",
    "        if x[i] == 0.:\n",
    "            count += 1\n",
    "    if count == len(x):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "X_total = DATA_x_train.tolist() + DATA_x_train_u.tolist()\n",
    "zero_i = list([])\n",
    "for k in arange(len(X_total[0,:])):   \n",
    "    if is_zero_vector(X_total[:,k]):\n",
    "        zero_i.append(k)\n",
    "zero_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Start tensorflow session\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Abstract placeholders\n",
    "x = tf.placeholder(tf.float32, shape=[None, 128])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initializing model parameters, W weights and b biases\n",
    "#W=tf.Variable(tf.zeros([128,10]))\n",
    "#b=tf.Variable(tf.zeros([10]))\n",
    "\n",
    "W=tf.Variable(tf.truncated_normal([128,10], stddev=0.1))\n",
    "b=tf.Variable(tf.zeros([10]))\n",
    "\n",
    "V=tf.Variable(tf.zeros([10,10]))\n",
    "c=tf.Variable(tf.zeros([10]))\n",
    "\n",
    "S=tf.Variable(tf.zeros([10,10]))\n",
    "d=tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Defining my network, including the loss function, cross_entropy\n",
    "sess.run(tf.initialize_all_variables())\n",
    "#y=tf.nn.softmax(tf.matmul(x,W)+b)\n",
    "z=tf.nn.relu(tf.matmul(x,W)+b)\n",
    "zz=tf.nn.relu(tf.matmul(z,S)+d)\n",
    "y=tf.nn.softmax(tf.matmul(zz,V)+c)\n",
    "cross_entropy=tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y),reduction_indices=[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.126\n"
     ]
    }
   ],
   "source": [
    "# Gradient Descent with 0.5 step, run a given amount of times. NOTE: I removed the batching part that was in the tf tutorial.\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "for i in range(1):\n",
    "    #batch = mnist.train.next_batch(50)\n",
    "    train_step.run(feed_dict={x:DATA_x_train,y_:DATA_y_})\n",
    "    \n",
    "# Evaluate based on the validation set that we separated in the beginning\n",
    "correct_prediction=tf.equal(tf.argmax(y,1),tf.argmax(y_,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "print(accuracy.eval(feed_dict={x:valDATA_x,y_:valDATA_y_}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means Scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X=DATA_x_train\n",
    "Y=DATA_y_\n",
    "\n",
    "labels=[]\n",
    "for label_array in (DATA_y_):\n",
    "    for j in range(len(label_array)):\n",
    "        if label_array[j]>0.5:\n",
    "            labels.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  7.69907041e-01,   0.00000000e+00,   6.86367682e-01, ...,\n",
       "          0.00000000e+00,   4.70549842e-01,   0.00000000e+00],\n",
       "       [  3.24442313e-01,   0.00000000e+00,   2.84245740e-01, ...,\n",
       "          0.00000000e+00,   1.21536811e-03,   0.00000000e+00],\n",
       "       [  6.19667159e-01,   0.00000000e+00,   4.76904522e-01, ...,\n",
       "          0.00000000e+00,   5.78197005e-02,   0.00000000e+00],\n",
       "       ..., \n",
       "       [  1.56354591e-01,   0.00000000e+00,   4.00170074e-01, ...,\n",
       "          0.00000000e+00,   1.11680552e-01,   1.86444994e-04],\n",
       "       [  5.28527891e-01,   0.00000000e+00,   6.43248037e-01, ...,\n",
       "          0.00000000e+00,   1.82324691e-01,   1.65119263e-04],\n",
       "       [  6.95338746e-02,   0.00000000e+00,   6.66219030e-01, ...,\n",
       "          0.00000000e+00,   5.63759798e-02,   3.98061310e-04]])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_randomly():\n",
    "    labels=[]\n",
    "    for label_array in (DATA_y_):\n",
    "        for j in range(len(label_array)):\n",
    "            if label_array[j]>0.5:\n",
    "                labels.append(j)\n",
    "    initial_ids=np.zeros(10)\n",
    "    labels_tmp=train_labeled['y'].values\n",
    "    #for i in range(len(labels_tmp)):\n",
    "    init=[]\n",
    "    while min(initial_ids)==0:\n",
    "        i=np.random.randint(0,len(labels_tmp))\n",
    "        for j in range(10):\n",
    "            if labels_tmp[i]==j and initial_ids[j]==0:\n",
    "                initial_ids[j]=int(i)\n",
    "\n",
    "    initial_ids = [int(i) for i in initial_ids]\n",
    "    init=[]\n",
    "    for i in range(10):\n",
    "        init.append(X[initial_ids[i]].tolist())\n",
    "    init=np.asarray(init)\n",
    "    return init\n",
    "\n",
    "def init_average():\n",
    "    labels=[]\n",
    "    for label_array in (DATA_y_):\n",
    "        for j in range(len(label_array)):\n",
    "            if label_array[j]>0.5:\n",
    "                labels.append(j)\n",
    "    init=[]\n",
    "    for i in range(10):\n",
    "        counter=0\n",
    "        aggregate=np.zeros([128])\n",
    "        for j in range(shape(X)[0]):\n",
    "            #print(i,j)\n",
    "            if labels[j]==i:\n",
    "                counter+=1\n",
    "                aggregate=np.add(X[j],aggregate)\n",
    "                #print(np.sum(aggregate))\n",
    "                #print(counter)\n",
    "        aggregate=[aggregate[k]/counter for k in range(len(aggregate))]\n",
    "        init.append(aggregate)\n",
    "    init=np.asarray(init)\n",
    "    return init\n",
    "init_average()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53.141176470588235 % accuracy\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "X=DATA_x_train\n",
    "Y=DATA_y_\n",
    "\n",
    "#init=init_randomly()\n",
    "init=init_average()\n",
    "\n",
    "estimator=KMeans(n_clusters=10,init=init,n_init=1,max_iter=1000000)\n",
    "estimator.fit(X)\n",
    "labels=estimator.labels_\n",
    "tmp_labels=labels\n",
    "PRED_labels=[]\n",
    "\n",
    "for label in tmp_labels:\n",
    "    PRED_labels.append(zeros(10))\n",
    "    PRED_labels[-1][label]=1\n",
    "\n",
    "no_correct=0\n",
    "for j in range(len(PRED_labels)):\n",
    "    label_vector=PRED_labels[j]\n",
    "    correctness=1\n",
    "    for i in range(len(label_vector)):\n",
    "        if DATA_y_[j][i]!=label_vector[i]:\n",
    "            correctness=0\n",
    "    if correctness==1:\n",
    "        no_correct+=1\n",
    "accuracy=no_correct/shape(DATA_x_train)[0]\n",
    "print(100*accuracy,\"% accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 8, 3, ..., 3, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "X=DATA_x_train\n",
    "Y=DATA_y_\n",
    "\n",
    "#init=init_randomly()\n",
    "init=init_average()\n",
    "\n",
    "X=DATA_x_train_u\n",
    "Y=DATA_y_\n",
    "\n",
    "estimator=KMeans(n_clusters=10,init=init,n_init=1,max_iter=1000000)\n",
    "estimator.fit(X)\n",
    "PRED_y=estimator.predict(DATA_x_test)\n",
    "PRED_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tamas/anaconda3/envs/tensorflow/lib/python3.5/site-packages/sklearn/preprocessing/data.py:167: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "/home/tamas/anaconda3/envs/tensorflow/lib/python3.5/site-packages/sklearn/preprocessing/data.py:184: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "data = scale(DATA_x_train)\n",
    "pca = PCA(n_components=10).fit(data)\n",
    "reduced_data = PCA(n_components=2).fit_transform(data)\n",
    "shape(reduced_data)\n",
    "for i in range(len(reduced_data)):\n",
    "    plt.scatter(reduced_data[i][0],reduced_data[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tamas/anaconda3/envs/tensorflow/lib/python3.5/site-packages/sklearn/preprocessing/data.py:167: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "/home/tamas/anaconda3/envs/tensorflow/lib/python3.5/site-packages/sklearn/preprocessing/data.py:184: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_digits: 10, \t n_samples 8500, \t n_features 128\n",
      "_______________________________________________________________________________\n",
      "init    time  inertia    homo   compl  v-meas     ARI AMI  silhouette\n",
      "ACCURACY= 0.05764705882352941\n",
      "k-means++   2.57s    766487   0.392   0.446   0.418   0.324   0.391    0.015\n",
      "ACCURACY= 0.068\n",
      "   random   2.28s    772612   0.371   0.428   0.398   0.300   0.370    0.012\n",
      "ACCURACY= 0.09\n",
      "PCA-based   0.37s    773437   0.367   0.407   0.386   0.270   0.366    0.003\n",
      "_______________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "#digits = load_digits()\n",
    "data = scale(DATA_x_train)\n",
    "\n",
    "n_samples, n_features = data.shape\n",
    "n_digits = 10#len(np.unique(digits.target))\n",
    "\n",
    "labels=[]\n",
    "for label_array in (DATA_y_):\n",
    "    for j in range(len(label_array)):\n",
    "        if label_array[j]>0.5:\n",
    "            labels.append(j)\n",
    "            \n",
    "#labels = DATA_y_#digits.target NOT GOOD TypE\n",
    "\n",
    "sample_size = 300\n",
    "\n",
    "print(\"n_digits: %d, \\t n_samples %d, \\t n_features %d\"\n",
    "      % (n_digits, n_samples, n_features))\n",
    "\n",
    "\n",
    "print(79 * '_')\n",
    "print('% 9s' % 'init'\n",
    "      '    time  inertia    homo   compl  v-meas     ARI AMI  silhouette')\n",
    "\n",
    "def accuracy(labels,labels_):\n",
    "    \n",
    "    no_correct=0\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i]==labels_[i]:\n",
    "            no_correct+=1\n",
    "    acc=no_correct/len(labels)\n",
    "    return acc\n",
    "\n",
    "\n",
    "def bench_k_means(estimator, name, data):\n",
    "    t0 = time()\n",
    "    estimator.fit(data)\n",
    "    print(\"ACCURACY=\",accuracy(labels,estimator.labels_))\n",
    "    print('% 9s   %.2fs    %i   %.3f   %.3f   %.3f   %.3f   %.3f    %.3f'\n",
    "          % (name, (time() - t0), estimator.inertia_,\n",
    "             metrics.homogeneity_score(labels, estimator.labels_),\n",
    "             metrics.completeness_score(labels, estimator.labels_),\n",
    "             metrics.v_measure_score(labels, estimator.labels_),\n",
    "             metrics.adjusted_rand_score(labels, estimator.labels_),\n",
    "             metrics.adjusted_mutual_info_score(labels,  estimator.labels_),\n",
    "             metrics.silhouette_score(data, estimator.labels_,\n",
    "                                      metric='euclidean',\n",
    "                                      sample_size=sample_size)))\n",
    "\n",
    "bench_k_means(KMeans(init='k-means++', n_clusters=n_digits, n_init=10),\n",
    "              name=\"k-means++\", data=data)\n",
    "\n",
    "bench_k_means(KMeans(init='random', n_clusters=n_digits, n_init=10),\n",
    "              name=\"random\", data=data)\n",
    "\n",
    "# in this case the seeding of the centers is deterministic, hence we run the\n",
    "# kmeans algorithm only once with n_init=1\n",
    "pca = PCA(n_components=n_digits).fit(data)\n",
    "bench_k_means(KMeans(init=pca.components_, n_clusters=n_digits, n_init=1),\n",
    "              name=\"PCA-based\",\n",
    "              data=data)\n",
    "print(79 * '_')\n",
    "\n",
    "###############################################################################\n",
    "# Visualize the results on PCA-reduced data\n",
    "\n",
    "reduced_data = PCA(n_components=2).fit_transform(data)\n",
    "kmeans = KMeans(init='k-means++', n_clusters=n_digits, n_init=10)\n",
    "kmeans.fit(reduced_data)\n",
    "\n",
    "# Step size of the mesh. Decrease to increase the quality of the VQ.\n",
    "h = .02     # point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\n",
    "y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "# Obtain labels for each point in mesh. Use last trained model.\n",
    "Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(1)\n",
    "plt.clf()\n",
    "plt.imshow(Z, interpolation='nearest',\n",
    "           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "           cmap=plt.cm.Paired,\n",
    "           aspect='auto', origin='lower')\n",
    "\n",
    "plt.plot(reduced_data[:, 0], reduced_data[:, 1], 'k.', markersize=2)\n",
    "# Plot the centroids as a white X\n",
    "centroids = kmeans.cluster_centers_\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "            marker='x', s=169, linewidths=3,\n",
    "            color='w', zorder=10)\n",
    "plt.title('K-means clustering on the digits dataset (PCA-reduced data)\\n'\n",
    "          'Centroids are marked with white cross')\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.529411764705882 % accuracy\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_features = 128\n",
    "n_clusters = 10\n",
    "\n",
    "centroids, samples = create_samples(n_clusters, n_samples_per_cluster, n_features, embiggen_factor, seed)\n",
    "\n",
    "model = tf.initialize_all_variables()\n",
    "with tf.Session() as session:\n",
    "    sample_values = session.run(samples)\n",
    "    centroid_values = session.run(centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'samples_29:0' shape=(1500, 2) dtype=float32>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_samples(n_clusters, n_samples_per_cluster, n_features, embiggen_factor, seed):\n",
    "    np.random.seed(seed)\n",
    "    slices = []\n",
    "    centroids = []\n",
    "    # Create samples for each cluster\n",
    "    for i in range(n_clusters):\n",
    "        samples = tf.random_normal((n_samples_per_cluster, n_features),\n",
    "                               mean=0.0, stddev=5.0, dtype=tf.float32, seed=seed, name=\"cluster_{}\".format(i))\n",
    "        current_centroid = (np.random.random((1, n_features)) * embiggen_factor) - (embiggen_factor/2)\n",
    "        centroids.append(current_centroid)\n",
    "        samples += current_centroid\n",
    "        slices.append(samples)\n",
    "    # Create a big \"samples\" dataset\n",
    "    samples = tf.concat(0, slices, name='samples')\n",
    "    centroids = tf.concat(0, centroids, name='centroids')\n",
    "    return centroids, samples\n",
    "\n",
    "n_features = 2\n",
    "n_clusters = 3\n",
    "n_samples_per_cluster = 500\n",
    "seed = 700\n",
    "embiggen_factor = 70\n",
    "\n",
    "np.random.seed(seed)\n",
    "slices = []\n",
    "centroids = []\n",
    "# Create samples for each cluster\n",
    "for i in range(n_clusters):\n",
    "    samples = tf.random_normal((n_samples_per_cluster, n_features),\n",
    "                           mean=0.0, stddev=5.0, dtype=tf.float32, seed=seed, name=\"cluster_{}\".format(i))\n",
    "    current_centroid = (np.random.random((1, n_features)) * embiggen_factor) - (embiggen_factor/2)\n",
    "    centroids.append(current_centroid)\n",
    "    samples += current_centroid\n",
    "    slices.append(samples)\n",
    "# Create a big \"samples\" dataset\n",
    "samples = tf.concat(0, slices, name='samples')\n",
    "centroids = tf.concat(0, centroids, name='centroids')\n",
    "#print(shape(samples))\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8500, 128) (10, 128)\n"
     ]
    }
   ],
   "source": [
    "n_features = 2\n",
    "n_clusters = 3\n",
    "n_samples_per_cluster = 500\n",
    "seed = 700\n",
    "embiggen_factor = 70\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "centroids, samples = create_samples(n_clusters, n_samples_per_cluster, n_features, embiggen_factor, seed)\n",
    "\n",
    "model = tf.initialize_all_variables()\n",
    "with tf.Session() as session:\n",
    "    sample_values = session.run(samples)\n",
    "    centroid_values = session.run(centroids)\n",
    "    \n",
    "sample_values=DATA_x_train\n",
    "centroid_values=[DATA_x_train[np.random.choice(128,1)[0]] for i in range(10)]\n",
    "print(shape(sample_values),shape(centroid_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Fetch argument array([ 0.7890625 ,  0.        ,  0.59375   ,  0.        ,  0.        ,\n        0.984375  ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.5546875 ,  0.        ,  0.59765625,  0.        ,  0.        ,\n        0.984375  ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.91796875,  0.        ,  0.        ,  0.        ,\n        0.765625  ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.09765625,  0.984375  ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.98828125,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.9296875 ,  0.984375  ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.38671875,  0.984375  ,  0.953125  ,  0.82421875,  0.        ,\n        0.984375  ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.95703125,  0.        ,  0.98828125,  0.        ,  0.        ,\n        0.        ,  0.90234375,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.484375  ,\n        0.        ,  0.        ,  0.2109375 ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.984375  ,\n        0.        ,  0.5078125 ,  0.        ,  0.        ,  0.        ,\n        0.9453125 ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.5546875 ,  0.        ,  0.54296875,  0.2109375 ,\n        0.2109375 ,  0.        ,  0.98828125,  0.984375  ,  0.19921875,\n        0.94921875,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.1875    ,  0.        ,  0.        ,\n        0.        ,  0.25      ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ], dtype=float32) of array([ 0.7890625 ,  0.        ,  0.59375   ,  0.        ,  0.        ,\n        0.984375  ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.5546875 ,  0.        ,  0.59765625,  0.        ,  0.        ,\n        0.984375  ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.91796875,  0.        ,  0.        ,  0.        ,\n        0.765625  ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.09765625,  0.984375  ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.98828125,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.9296875 ,  0.984375  ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.38671875,  0.984375  ,  0.953125  ,  0.82421875,  0.        ,\n        0.984375  ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.95703125,  0.        ,  0.98828125,  0.        ,  0.        ,\n        0.        ,  0.90234375,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.484375  ,\n        0.        ,  0.        ,  0.2109375 ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.984375  ,\n        0.        ,  0.5078125 ,  0.        ,  0.        ,  0.        ,\n        0.9453125 ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.5546875 ,  0.        ,  0.54296875,  0.2109375 ,\n        0.2109375 ,  0.        ,  0.98828125,  0.984375  ,  0.19921875,\n        0.94921875,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.1875    ,  0.        ,  0.        ,\n        0.        ,  0.25      ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ], dtype=float32) has invalid type <class 'numpy.ndarray'>, must be a string or Tensor. (Can not convert a ndarray into a Tensor or Operation.)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/tamas/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_process_fetches\u001b[1;34m(self, fetches)\u001b[0m\n\u001b[0;32m    479\u001b[0m           fetch_t = self.graph.as_graph_element(subfetch, allow_tensor=True,\n\u001b[1;32m--> 480\u001b[1;33m                                                 allow_operation=True)\n\u001b[0m\u001b[0;32m    481\u001b[0m           \u001b[0mfetch_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch_t\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/tamas/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[1;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[0;32m   2300\u001b[0m       raise TypeError(\"Can not convert a %s into a %s.\"\n\u001b[1;32m-> 2301\u001b[1;33m                       % (type(obj).__name__, types_str))\n\u001b[0m\u001b[0;32m   2302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Can not convert a ndarray into a Tensor or Operation.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-8255f916dd5c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mupdated_centroid_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitial_centroids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[0mupdate_centroids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_values\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0massign_to_nearest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_values\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mupdate_centroid_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/tamas/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    338\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 340\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    341\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/tamas/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m     \u001b[1;31m# Validate and process fetches.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 523\u001b[1;33m     \u001b[0mprocessed_fetches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_fetches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    524\u001b[0m     \u001b[0munique_fetches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocessed_fetches\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m     \u001b[0mtarget_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocessed_fetches\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/tamas/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_process_fetches\u001b[1;34m(self, fetches)\u001b[0m\n\u001b[0;32m    491\u001b[0m           raise TypeError('Fetch argument %r of %r has invalid type %r, '\n\u001b[0;32m    492\u001b[0m                           \u001b[1;34m'must be a string or Tensor. (%s)'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m                           % (subfetch, fetch, type(subfetch), str(e)))\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m           raise ValueError('Fetch argument %r of %r cannot be interpreted as a '\n",
      "\u001b[1;31mTypeError\u001b[0m: Fetch argument array([ 0.7890625 ,  0.        ,  0.59375   ,  0.        ,  0.        ,\n        0.984375  ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.5546875 ,  0.        ,  0.59765625,  0.        ,  0.        ,\n        0.984375  ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.91796875,  0.        ,  0.        ,  0.        ,\n        0.765625  ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.09765625,  0.984375  ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.98828125,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.9296875 ,  0.984375  ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.38671875,  0.984375  ,  0.953125  ,  0.82421875,  0.        ,\n        0.984375  ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.95703125,  0.        ,  0.98828125,  0.        ,  0.        ,\n        0.        ,  0.90234375,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.484375  ,\n        0.        ,  0.        ,  0.2109375 ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.984375  ,\n        0.        ,  0.5078125 ,  0.        ,  0.        ,  0.        ,\n        0.9453125 ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.5546875 ,  0.        ,  0.54296875,  0.2109375 ,\n        0.2109375 ,  0.        ,  0.98828125,  0.984375  ,  0.19921875,\n        0.94921875,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.1875    ,  0.        ,  0.        ,\n        0.        ,  0.25      ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ], dtype=float32) of array([ 0.7890625 ,  0.        ,  0.59375   ,  0.        ,  0.        ,\n        0.984375  ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.5546875 ,  0.        ,  0.59765625,  0.        ,  0.        ,\n        0.984375  ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.91796875,  0.        ,  0.        ,  0.        ,\n        0.765625  ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.09765625,  0.984375  ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.98828125,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.9296875 ,  0.984375  ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.38671875,  0.984375  ,  0.953125  ,  0.82421875,  0.        ,\n        0.984375  ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.95703125,  0.        ,  0.98828125,  0.        ,  0.        ,\n        0.        ,  0.90234375,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.484375  ,\n        0.        ,  0.        ,  0.2109375 ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.984375  ,\n        0.        ,  0.5078125 ,  0.        ,  0.        ,  0.        ,\n        0.9453125 ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.5546875 ,  0.        ,  0.54296875,  0.2109375 ,\n        0.2109375 ,  0.        ,  0.98828125,  0.984375  ,  0.19921875,\n        0.94921875,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.1875    ,  0.        ,  0.        ,\n        0.        ,  0.25      ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ], dtype=float32) has invalid type <class 'numpy.ndarray'>, must be a string or Tensor. (Can not convert a ndarray into a Tensor or Operation.)"
     ]
    }
   ],
   "source": [
    "def update_centroids(samples, nearest_indices, n_clusters):\n",
    "    # Updates the centroid to be the mean of all samples associated with it.\n",
    "    nearest_indices = tf.to_int32(nearest_indices)\n",
    "    partitions = tf.dynamic_partition(samples, nearest_indices, n_clusters)\n",
    "    new_centroids = tf.concat(0, [tf.expand_dims(tf.reduce_mean(partition, 0), 0) for partition in partitions])\n",
    "    return new_centroids\n",
    "\n",
    "def assign_to_nearest(samples, centroids):\n",
    "    # Finds the nearest centroid for each sample\n",
    "\n",
    "    # START from http://esciencegroup.com/2016/01/05/an-encounter-with-googles-tensorflow/\n",
    "    expanded_vectors = tf.expand_dims(samples, 0)\n",
    "    expanded_centroids = tf.expand_dims(centroids, 1)\n",
    "    distances = tf.reduce_sum( tf.square(\n",
    "               tf.sub(expanded_vectors, expanded_centroids)), 2)\n",
    "    mins = tf.argmin(distances, 0)\n",
    "    # END from http://esciencegroup.com/2016/01/05/an-encounter-with-googles-tensorflow/\n",
    "    nearest_indices = mins\n",
    "    return nearest_indices\n",
    "\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, 128])\n",
    "\n",
    "sample_values=DATA_x_train\n",
    "initial_centroids=[DATA_x_train[np.random.choice(128,1)[0]] for i in range(10)]\n",
    "\n",
    "model = tf.initialize_all_variables()\n",
    "\n",
    "with tf.Session() as session:\n",
    "    for i in range(1000):\n",
    "        updated_centroid_value = session.run(initial_centroids)\n",
    "        update_centroids(sample_values,assign_to_nearest(sample_values,update_centroid_value),10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('result_2.csv', 'w') as fp:\n",
    "    a = csv.writer(fp, delimiter=',')\n",
    "    a.writerow(('Id','y'))\n",
    "    a.writerows(zip(DATA_id_test,PRED_y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
