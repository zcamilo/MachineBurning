{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_train = h5py.File('train.h5','r')\n",
    "f_test = h5py.File('test.h5','r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'train']\n",
      "[u'test']\n"
     ]
    }
   ],
   "source": [
    "print(f_train.keys())\n",
    "print(f_test.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a_train = f_train['train']\n",
    "a_test = f_test['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'axis0', u'axis1', u'block0_items', u'block0_values', u'block1_items', u'block1_values']\n",
      "[u'axis0', u'axis1', u'block0_items', u'block0_values']\n"
     ]
    }
   ],
   "source": [
    "print(a_train.keys())\n",
    "print(a_test.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b = a_test['axis1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8137,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# axis0 -> Labels [shape (101,)]\n",
    "# axis1 -> id column [shape (45324,)]\n",
    "# block0_items -> traits labels [shape (100,)]\n",
    "# block0_values -> traits values [shape (45324,100)]\n",
    "# block1_items -> 'y' label\n",
    "# block1_values -> y column (without label) [shape (45324,1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = a_train['block0_values'][()]\n",
    "y_train = a_train['block1_values'][()]\n",
    "y_train = y_train[:,0]\n",
    "X_test = a_test['block0_values'][()]\n",
    "X_train, X_val = X_train[:-1000], X_train[-1000:]\n",
    "y_train, y_val = y_train[:-1000], y_train[-1000:]\n",
    "ids = a_test['axis1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44324, 100)\n",
      "<type 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(type(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x10df6fc10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAEACAYAAABWLgY0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1038607d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scatter(arange(100), y_train[0:100])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "s = \n",
    "z = 0\n",
    "for i in arange(100):\n",
    "    for j in arange(100):\n",
    "        if b[i,j]==0.:\n",
    "            z=z+1\n",
    "        s += str(b[i,j]) + ' '\n",
    "    s += '/n'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "print(str(z/100.) + '% percentage of zeros in chunk of matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from theano import *\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.0\n"
     ]
    }
   ],
   "source": [
    "x = T.dvector('x') # declare variable\n",
    "y = T.dvector('y')\n",
    "out = y.dot(y) + x.dot(x)         # build symbolic expression\n",
    "fun = theano.function([x,y], out)   # compile function\n",
    "print(fun([2,1],[1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Neural Network: Lasagne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Camilo/anaconda2/lib/python2.7/site-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import time\n",
    "import lasagne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_custom_mlp(input_var=None, depth=2, width=800, drop_input=.2,\n",
    "                     drop_hidden=.5):\n",
    "    # By default, this creates the same network as `build_mlp`, but it can be\n",
    "    # customized with respect to the number and size of hidden layers. This\n",
    "    # mostly showcases how creating a network in Python code can be a lot more\n",
    "    # flexible than a configuration file. Note that to make the code easier,\n",
    "    # all the layers are just called `network` -- there is no need to give them\n",
    "    # different names if all we return is the last one we created anyway; we\n",
    "    # just used different names above for clarity.\n",
    "\n",
    "    # Input layer and dropout (with shortcut `dropout` for `DropoutLayer`):\n",
    "    network = lasagne.layers.InputLayer(shape=(None, 100),\n",
    "                                        input_var=input_var)\n",
    "    if drop_input:\n",
    "        network = lasagne.layers.dropout(network, p=drop_input)\n",
    "    # Hidden layers and dropout:\n",
    "    nonlin = lasagne.nonlinearities.rectify\n",
    "    for _ in range(depth):\n",
    "        network = lasagne.layers.DenseLayer(network, width, nonlinearity=nonlin)\n",
    "        if drop_hidden:\n",
    "            network = lasagne.layers.dropout(network, p=drop_hidden)\n",
    "    # Output layer:\n",
    "    softmax = lasagne.nonlinearities.softmax\n",
    "    network = lasagne.layers.DenseLayer(network, 5, nonlinearity=softmax)\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_nnet(num_epochs=500,depth = 2, width = 100, d_i = 0.3, d_h = 0.5, l_r = 0.1, momentum = 0.9):\n",
    "    # Load the dataset\n",
    "    # print(\"Loading data...\")\n",
    "    # X_train, y_train, X_val, y_val, X_test, y_test = load_dataset()\n",
    "    \n",
    "    # theano.config.optimizer='fast_compile'\n",
    "    # theano.config.exception_verbosity='high'\n",
    "\n",
    "    # Prepare Theano variables for inputs and targets\n",
    "    input_var = T.matrix('inputs')\n",
    "    target_var = T.ivector('targets')\n",
    "\n",
    "    # Create neural network model (depending on first command line parameter)\n",
    "    print(\"Building model and compiling functions...\")\n",
    "    \n",
    "    network = build_custom_mlp(input_var=input_var,depth=depth,width=width,drop_input=d_i,drop_hidden=d_h)\n",
    "\n",
    "    # Create a loss expression for training, i.e., a scalar objective we want\n",
    "    # to minimize (for our multi-class problem, it is the cross-entropy loss):\n",
    "    prediction = lasagne.layers.get_output(network)\n",
    "    loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "    loss = loss.mean()\n",
    "    # We could add some weight decay as well here, see lasagne.regularization.\n",
    "\n",
    "    # Create update expressions for training, i.e., how to modify the\n",
    "    # parameters at each training step. Here, we'll use Stochastic Gradient\n",
    "    # Descent (SGD) with Nesterov momentum, but Lasagne offers plenty more.\n",
    "    params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "    updates = lasagne.updates.nesterov_momentum(loss, params, learning_rate=l_r, momentum=momentum)\n",
    "\n",
    "    # Create a loss expression for validation/testing. The crucial difference\n",
    "    # here is that we do a deterministic forward pass through the network,\n",
    "    # disabling dropout layers.\n",
    "    test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "    test_loss = lasagne.objectives.categorical_crossentropy(test_prediction,target_var)\n",
    "    test_loss = test_loss.mean()\n",
    "    \n",
    "    # As a bonus, also create an expression for the classification accuracy:\n",
    "    test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),dtype=theano.config.floatX)\n",
    "\n",
    "    # Compile a function performing a training step on a mini-batch (by giving\n",
    "    # the updates dictionary) and returning the corresponding training loss:\n",
    "    train_fn = theano.function([input_var, target_var], loss, updates=updates, allow_input_downcast=True)\n",
    "\n",
    "    # Compile a second function computing the validation loss and accuracy:\n",
    "    val_fn = theano.function([input_var, target_var], [test_loss, test_acc], allow_input_downcast=True)\n",
    "\n",
    "    # Finally, launch the training loop.\n",
    "    print(\"Starting training...\")\n",
    "    # We iterate over epochs:\n",
    "    global percs\n",
    "    percs = array([])\n",
    "    for epoch in range(num_epochs):\n",
    "        # In each epoch, we do a full pass over the training data:\n",
    "        train_err = 0\n",
    "        train_batches = 0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for batch in iterate_minibatches(X_train, y_train, 500, shuffle=True):\n",
    "            inputs, targets = batch\n",
    "            train_err += train_fn(inputs, targets)\n",
    "            train_batches += 1\n",
    "\n",
    "        # And a full pass over the validation data:\n",
    "        val_err = 0\n",
    "        val_acc = 0\n",
    "        val_batches = 0\n",
    "        for batch in iterate_minibatches(X_val, y_val, 500, shuffle=False):\n",
    "            inputs, targets = batch\n",
    "            err, acc = val_fn(inputs, targets)\n",
    "            val_err += err\n",
    "            val_acc += acc\n",
    "            val_batches += 1\n",
    "\n",
    "        # Then we print the results for this epoch:\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "            epoch + 1, num_epochs, time.time() - start_time))\n",
    "        print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "        print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "        perc = val_acc / val_batches\n",
    "        percs = append(percs, perc)\n",
    "        print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "            perc * 100))\n",
    "\n",
    "    # After training, we compute and print the test error:\n",
    "    test_err = 0\n",
    "    test_acc = 0\n",
    "    test_batches = 0\n",
    "    for batch in iterate_minibatches(X_test, zeros(len(X_test)), 500, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        err, acc = val_fn(inputs, targets)\n",
    "        test_err += err\n",
    "        test_acc += acc\n",
    "        test_batches += 1\n",
    "    print(\"Final results:\")\n",
    "    print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "    print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "        test_acc / test_batches * 100))\n",
    "    print(\"Percentages average:\\t\\t\\t{:.5f}\".format(mean(percs)))\n",
    "    print(\"\\a\")\n",
    "    global result\n",
    "    result = lasagne.layers.get_output(network, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model and compiling functions...\n",
      "Starting training...\n",
      "Epoch 1 of 500 took 1.793s\n",
      "  training loss:\t\t0.565595\n",
      "  validation loss:\t\t0.266055\n",
      "  validation accuracy:\t\t91.30 %\n",
      "Epoch 2 of 500 took 1.920s\n",
      "  training loss:\t\t0.284025\n",
      "  validation loss:\t\t0.212123\n",
      "  validation accuracy:\t\t93.50 %\n",
      "Epoch 3 of 500 took 3.205s\n",
      "  training loss:\t\t0.245658\n",
      "  validation loss:\t\t0.193914\n",
      "  validation accuracy:\t\t93.70 %\n",
      "Epoch 4 of 500 took 2.159s\n",
      "  training loss:\t\t0.220643\n",
      "  validation loss:\t\t0.178198\n",
      "  validation accuracy:\t\t94.50 %\n",
      "Epoch 5 of 500 took 2.073s\n",
      "  training loss:\t\t0.207295\n",
      "  validation loss:\t\t0.184026\n",
      "  validation accuracy:\t\t93.90 %\n",
      "Epoch 6 of 500 took 1.979s\n",
      "  training loss:\t\t0.194375\n",
      "  validation loss:\t\t0.168133\n",
      "  validation accuracy:\t\t94.60 %\n",
      "Epoch 7 of 500 took 2.021s\n",
      "  training loss:\t\t0.180885\n",
      "  validation loss:\t\t0.155525\n",
      "  validation accuracy:\t\t94.80 %\n",
      "Epoch 8 of 500 took 1.938s\n",
      "  training loss:\t\t0.176671\n",
      "  validation loss:\t\t0.156518\n",
      "  validation accuracy:\t\t95.10 %\n",
      "Epoch 9 of 500 took 1.917s\n",
      "  training loss:\t\t0.170081\n",
      "  validation loss:\t\t0.151601\n",
      "  validation accuracy:\t\t95.10 %\n",
      "Epoch 10 of 500 took 1.923s\n",
      "  training loss:\t\t0.167058\n",
      "  validation loss:\t\t0.156314\n",
      "  validation accuracy:\t\t95.30 %\n",
      "Epoch 11 of 500 took 1.867s\n",
      "  training loss:\t\t0.158723\n",
      "  validation loss:\t\t0.153854\n",
      "  validation accuracy:\t\t94.30 %\n",
      "Epoch 12 of 500 took 1.881s\n",
      "  training loss:\t\t0.154518\n",
      "  validation loss:\t\t0.152510\n",
      "  validation accuracy:\t\t95.20 %\n",
      "Epoch 13 of 500 took 1.897s\n",
      "  training loss:\t\t0.151529\n",
      "  validation loss:\t\t0.157638\n",
      "  validation accuracy:\t\t94.90 %\n",
      "Epoch 14 of 500 took 1.862s\n",
      "  training loss:\t\t0.146701\n",
      "  validation loss:\t\t0.143580\n",
      "  validation accuracy:\t\t95.10 %\n",
      "Epoch 15 of 500 took 1.932s\n",
      "  training loss:\t\t0.143988\n",
      "  validation loss:\t\t0.145679\n",
      "  validation accuracy:\t\t95.00 %\n",
      "Epoch 16 of 500 took 2.176s\n",
      "  training loss:\t\t0.139541\n",
      "  validation loss:\t\t0.143558\n",
      "  validation accuracy:\t\t94.90 %\n",
      "Epoch 17 of 500 took 2.280s\n",
      "  training loss:\t\t0.136931\n",
      "  validation loss:\t\t0.138379\n",
      "  validation accuracy:\t\t95.40 %\n",
      "Epoch 18 of 500 took 2.505s\n",
      "  training loss:\t\t0.136653\n",
      "  validation loss:\t\t0.140428\n",
      "  validation accuracy:\t\t95.30 %\n",
      "Epoch 19 of 500 took 2.285s\n",
      "  training loss:\t\t0.131194\n",
      "  validation loss:\t\t0.144451\n",
      "  validation accuracy:\t\t94.90 %\n",
      "Epoch 20 of 500 took 2.247s\n",
      "  training loss:\t\t0.130861\n",
      "  validation loss:\t\t0.123671\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 21 of 500 took 2.310s\n",
      "  training loss:\t\t0.127213\n",
      "  validation loss:\t\t0.145518\n",
      "  validation accuracy:\t\t94.60 %\n",
      "Epoch 22 of 500 took 2.240s\n",
      "  training loss:\t\t0.127222\n",
      "  validation loss:\t\t0.134667\n",
      "  validation accuracy:\t\t95.60 %\n",
      "Epoch 23 of 500 took 2.224s\n",
      "  training loss:\t\t0.123435\n",
      "  validation loss:\t\t0.132606\n",
      "  validation accuracy:\t\t95.30 %\n",
      "Epoch 24 of 500 took 2.251s\n",
      "  training loss:\t\t0.123829\n",
      "  validation loss:\t\t0.139894\n",
      "  validation accuracy:\t\t95.30 %\n",
      "Epoch 25 of 500 took 1.823s\n",
      "  training loss:\t\t0.120935\n",
      "  validation loss:\t\t0.140815\n",
      "  validation accuracy:\t\t94.60 %\n",
      "Epoch 26 of 500 took 1.976s\n",
      "  training loss:\t\t0.118441\n",
      "  validation loss:\t\t0.142780\n",
      "  validation accuracy:\t\t95.20 %\n",
      "Epoch 27 of 500 took 1.805s\n",
      "  training loss:\t\t0.120057\n",
      "  validation loss:\t\t0.135661\n",
      "  validation accuracy:\t\t95.50 %\n",
      "Epoch 28 of 500 took 1.877s\n",
      "  training loss:\t\t0.116281\n",
      "  validation loss:\t\t0.144218\n",
      "  validation accuracy:\t\t95.10 %\n",
      "Epoch 29 of 500 took 1.836s\n",
      "  training loss:\t\t0.111299\n",
      "  validation loss:\t\t0.138358\n",
      "  validation accuracy:\t\t95.10 %\n",
      "Epoch 30 of 500 took 1.900s\n",
      "  training loss:\t\t0.115508\n",
      "  validation loss:\t\t0.148036\n",
      "  validation accuracy:\t\t94.90 %\n",
      "Epoch 31 of 500 took 1.881s\n",
      "  training loss:\t\t0.115126\n",
      "  validation loss:\t\t0.128920\n",
      "  validation accuracy:\t\t96.00 %\n",
      "Epoch 32 of 500 took 1.937s\n",
      "  training loss:\t\t0.111227\n",
      "  validation loss:\t\t0.138602\n",
      "  validation accuracy:\t\t95.10 %\n",
      "Epoch 33 of 500 took 1.859s\n",
      "  training loss:\t\t0.106696\n",
      "  validation loss:\t\t0.132633\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 34 of 500 took 1.896s\n",
      "  training loss:\t\t0.109333\n",
      "  validation loss:\t\t0.136688\n",
      "  validation accuracy:\t\t95.20 %\n",
      "Epoch 35 of 500 took 1.862s\n",
      "  training loss:\t\t0.105183\n",
      "  validation loss:\t\t0.137685\n",
      "  validation accuracy:\t\t95.40 %\n",
      "Epoch 36 of 500 took 1.860s\n",
      "  training loss:\t\t0.103553\n",
      "  validation loss:\t\t0.144697\n",
      "  validation accuracy:\t\t95.10 %\n",
      "Epoch 37 of 500 took 1.868s\n",
      "  training loss:\t\t0.103011\n",
      "  validation loss:\t\t0.136532\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 38 of 500 took 1.915s\n",
      "  training loss:\t\t0.103463\n",
      "  validation loss:\t\t0.134574\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 39 of 500 took 1.904s\n",
      "  training loss:\t\t0.101159\n",
      "  validation loss:\t\t0.138850\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 40 of 500 took 1.895s\n",
      "  training loss:\t\t0.104167\n",
      "  validation loss:\t\t0.131307\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 41 of 500 took 1.930s\n",
      "  training loss:\t\t0.104625\n",
      "  validation loss:\t\t0.138565\n",
      "  validation accuracy:\t\t95.90 %\n",
      "Epoch 42 of 500 took 1.910s\n",
      "  training loss:\t\t0.100461\n",
      "  validation loss:\t\t0.143831\n",
      "  validation accuracy:\t\t95.60 %\n",
      "Epoch 43 of 500 took 1.964s\n",
      "  training loss:\t\t0.099444\n",
      "  validation loss:\t\t0.127178\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 44 of 500 took 1.986s\n",
      "  training loss:\t\t0.101859\n",
      "  validation loss:\t\t0.145048\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 45 of 500 took 1.954s\n",
      "  training loss:\t\t0.096411\n",
      "  validation loss:\t\t0.140081\n",
      "  validation accuracy:\t\t95.50 %\n",
      "Epoch 46 of 500 took 1.858s\n",
      "  training loss:\t\t0.096613\n",
      "  validation loss:\t\t0.140807\n",
      "  validation accuracy:\t\t95.60 %\n",
      "Epoch 47 of 500 took 1.988s\n",
      "  training loss:\t\t0.096786\n",
      "  validation loss:\t\t0.131762\n",
      "  validation accuracy:\t\t95.40 %\n",
      "Epoch 48 of 500 took 1.907s\n",
      "  training loss:\t\t0.094714\n",
      "  validation loss:\t\t0.135137\n",
      "  validation accuracy:\t\t96.00 %\n",
      "Epoch 49 of 500 took 1.960s\n",
      "  training loss:\t\t0.095572\n",
      "  validation loss:\t\t0.156021\n",
      "  validation accuracy:\t\t95.10 %\n",
      "Epoch 50 of 500 took 1.876s\n",
      "  training loss:\t\t0.092613\n",
      "  validation loss:\t\t0.131111\n",
      "  validation accuracy:\t\t96.00 %\n",
      "Epoch 51 of 500 took 1.944s\n",
      "  training loss:\t\t0.093287\n",
      "  validation loss:\t\t0.138855\n",
      "  validation accuracy:\t\t95.20 %\n",
      "Epoch 52 of 500 took 1.893s\n",
      "  training loss:\t\t0.090100\n",
      "  validation loss:\t\t0.121839\n",
      "  validation accuracy:\t\t95.90 %\n",
      "Epoch 53 of 500 took 2.004s\n",
      "  training loss:\t\t0.092252\n",
      "  validation loss:\t\t0.136828\n",
      "  validation accuracy:\t\t95.90 %\n",
      "Epoch 54 of 500 took 1.950s\n",
      "  training loss:\t\t0.092579\n",
      "  validation loss:\t\t0.135358\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 55 of 500 took 1.944s\n",
      "  training loss:\t\t0.091888\n",
      "  validation loss:\t\t0.147461\n",
      "  validation accuracy:\t\t95.90 %\n",
      "Epoch 56 of 500 took 1.883s\n",
      "  training loss:\t\t0.090349\n",
      "  validation loss:\t\t0.133974\n",
      "  validation accuracy:\t\t95.20 %\n",
      "Epoch 57 of 500 took 1.967s\n",
      "  training loss:\t\t0.092162\n",
      "  validation loss:\t\t0.138659\n",
      "  validation accuracy:\t\t95.60 %\n",
      "Epoch 58 of 500 took 1.876s\n",
      "  training loss:\t\t0.086398\n",
      "  validation loss:\t\t0.130538\n",
      "  validation accuracy:\t\t95.90 %\n",
      "Epoch 59 of 500 took 1.918s\n",
      "  training loss:\t\t0.087847\n",
      "  validation loss:\t\t0.135807\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 60 of 500 took 1.857s\n",
      "  training loss:\t\t0.088948\n",
      "  validation loss:\t\t0.138531\n",
      "  validation accuracy:\t\t96.00 %\n",
      "Epoch 61 of 500 took 1.888s\n",
      "  training loss:\t\t0.084407\n",
      "  validation loss:\t\t0.141937\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 62 of 500 took 1.919s\n",
      "  training loss:\t\t0.088132\n",
      "  validation loss:\t\t0.130409\n",
      "  validation accuracy:\t\t96.00 %\n",
      "Epoch 63 of 500 took 1.936s\n",
      "  training loss:\t\t0.088664\n",
      "  validation loss:\t\t0.143185\n",
      "  validation accuracy:\t\t95.50 %\n",
      "Epoch 64 of 500 took 1.972s\n",
      "  training loss:\t\t0.090062\n",
      "  validation loss:\t\t0.134197\n",
      "  validation accuracy:\t\t96.00 %\n",
      "Epoch 65 of 500 took 1.871s\n",
      "  training loss:\t\t0.082986\n",
      "  validation loss:\t\t0.138835\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 66 of 500 took 1.917s\n",
      "  training loss:\t\t0.083614\n",
      "  validation loss:\t\t0.149419\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 67 of 500 took 1.933s\n",
      "  training loss:\t\t0.082594\n",
      "  validation loss:\t\t0.138567\n",
      "  validation accuracy:\t\t95.90 %\n",
      "Epoch 68 of 500 took 1.981s\n",
      "  training loss:\t\t0.084603\n",
      "  validation loss:\t\t0.138529\n",
      "  validation accuracy:\t\t95.40 %\n",
      "Epoch 69 of 500 took 1.959s\n",
      "  training loss:\t\t0.084693\n",
      "  validation loss:\t\t0.130446\n",
      "  validation accuracy:\t\t96.00 %\n",
      "Epoch 70 of 500 took 1.985s\n",
      "  training loss:\t\t0.081319\n",
      "  validation loss:\t\t0.129853\n",
      "  validation accuracy:\t\t95.90 %\n",
      "Epoch 71 of 500 took 1.873s\n",
      "  training loss:\t\t0.081492\n",
      "  validation loss:\t\t0.147665\n",
      "  validation accuracy:\t\t96.10 %\n",
      "Epoch 72 of 500 took 1.882s\n",
      "  training loss:\t\t0.083974\n",
      "  validation loss:\t\t0.167203\n",
      "  validation accuracy:\t\t95.40 %\n",
      "Epoch 73 of 500 took 1.850s\n",
      "  training loss:\t\t0.080753\n",
      "  validation loss:\t\t0.149353\n",
      "  validation accuracy:\t\t95.40 %\n",
      "Epoch 74 of 500 took 1.984s\n",
      "  training loss:\t\t0.084759\n",
      "  validation loss:\t\t0.145746\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 75 of 500 took 1.864s\n",
      "  training loss:\t\t0.078177\n",
      "  validation loss:\t\t0.146841\n",
      "  validation accuracy:\t\t95.60 %\n",
      "Epoch 76 of 500 took 1.952s\n",
      "  training loss:\t\t0.083412\n",
      "  validation loss:\t\t0.149304\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 77 of 500 took 1.870s\n",
      "  training loss:\t\t0.081414\n",
      "  validation loss:\t\t0.149475\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 78 of 500 took 1.931s\n",
      "  training loss:\t\t0.080890\n",
      "  validation loss:\t\t0.148753\n",
      "  validation accuracy:\t\t95.00 %\n",
      "Epoch 79 of 500 took 1.897s\n",
      "  training loss:\t\t0.076706\n",
      "  validation loss:\t\t0.159642\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 80 of 500 took 1.973s\n",
      "  training loss:\t\t0.076415\n",
      "  validation loss:\t\t0.147381\n",
      "  validation accuracy:\t\t95.60 %\n",
      "Epoch 81 of 500 took 1.926s\n",
      "  training loss:\t\t0.076900\n",
      "  validation loss:\t\t0.142071\n",
      "  validation accuracy:\t\t95.90 %\n",
      "Epoch 82 of 500 took 1.978s\n",
      "  training loss:\t\t0.078405\n",
      "  validation loss:\t\t0.146834\n",
      "  validation accuracy:\t\t95.30 %\n",
      "Epoch 83 of 500 took 1.870s\n",
      "  training loss:\t\t0.078551\n",
      "  validation loss:\t\t0.151710\n",
      "  validation accuracy:\t\t95.50 %\n",
      "Epoch 84 of 500 took 1.919s\n",
      "  training loss:\t\t0.076112\n",
      "  validation loss:\t\t0.130127\n",
      "  validation accuracy:\t\t96.10 %\n",
      "Epoch 85 of 500 took 1.898s\n",
      "  training loss:\t\t0.078476\n",
      "  validation loss:\t\t0.142121\n",
      "  validation accuracy:\t\t95.40 %\n",
      "Epoch 86 of 500 took 1.921s\n",
      "  training loss:\t\t0.076075\n",
      "  validation loss:\t\t0.158831\n",
      "  validation accuracy:\t\t95.50 %\n",
      "Epoch 87 of 500 took 1.909s\n",
      "  training loss:\t\t0.076172\n",
      "  validation loss:\t\t0.150780\n",
      "  validation accuracy:\t\t95.90 %\n",
      "Epoch 88 of 500 took 1.992s\n",
      "  training loss:\t\t0.077622\n",
      "  validation loss:\t\t0.142940\n",
      "  validation accuracy:\t\t95.60 %\n",
      "Epoch 89 of 500 took 1.997s\n",
      "  training loss:\t\t0.076344\n",
      "  validation loss:\t\t0.140502\n",
      "  validation accuracy:\t\t95.90 %\n",
      "Epoch 90 of 500 took 1.925s\n",
      "  training loss:\t\t0.076776\n",
      "  validation loss:\t\t0.145637\n",
      "  validation accuracy:\t\t96.00 %\n",
      "Epoch 91 of 500 took 1.972s\n",
      "  training loss:\t\t0.077957\n",
      "  validation loss:\t\t0.148668\n",
      "  validation accuracy:\t\t95.60 %\n",
      "Epoch 92 of 500 took 1.902s\n",
      "  training loss:\t\t0.076892\n",
      "  validation loss:\t\t0.160909\n",
      "  validation accuracy:\t\t95.40 %\n",
      "Epoch 93 of 500 took 1.917s\n",
      "  training loss:\t\t0.074281\n",
      "  validation loss:\t\t0.160887\n",
      "  validation accuracy:\t\t95.40 %\n",
      "Epoch 94 of 500 took 1.933s\n",
      "  training loss:\t\t0.076979\n",
      "  validation loss:\t\t0.146251\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 95 of 500 took 1.904s\n",
      "  training loss:\t\t0.077240\n",
      "  validation loss:\t\t0.147853\n",
      "  validation accuracy:\t\t95.50 %\n",
      "Epoch 96 of 500 took 1.916s\n",
      "  training loss:\t\t0.073877\n",
      "  validation loss:\t\t0.151827\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 97 of 500 took 1.894s\n",
      "  training loss:\t\t0.075581\n",
      "  validation loss:\t\t0.144455\n",
      "  validation accuracy:\t\t96.20 %\n",
      "Epoch 98 of 500 took 1.860s\n",
      "  training loss:\t\t0.077057\n",
      "  validation loss:\t\t0.141078\n",
      "  validation accuracy:\t\t95.60 %\n",
      "Epoch 99 of 500 took 1.900s\n",
      "  training loss:\t\t0.073224\n",
      "  validation loss:\t\t0.149574\n",
      "  validation accuracy:\t\t95.30 %\n",
      "Epoch 100 of 500 took 1.942s\n",
      "  training loss:\t\t0.074546\n",
      "  validation loss:\t\t0.132861\n",
      "  validation accuracy:\t\t96.10 %\n",
      "Epoch 101 of 500 took 1.928s\n",
      "  training loss:\t\t0.075436\n",
      "  validation loss:\t\t0.131548\n",
      "  validation accuracy:\t\t95.60 %\n",
      "Epoch 102 of 500 took 1.896s\n",
      "  training loss:\t\t0.076001\n",
      "  validation loss:\t\t0.131175\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 103 of 500 took 2.001s\n",
      "  training loss:\t\t0.070884\n",
      "  validation loss:\t\t0.149951\n",
      "  validation accuracy:\t\t95.90 %\n",
      "Epoch 104 of 500 took 1.928s\n",
      "  training loss:\t\t0.072089\n",
      "  validation loss:\t\t0.126648\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 105 of 500 took 1.973s\n",
      "  training loss:\t\t0.069848\n",
      "  validation loss:\t\t0.152100\n",
      "  validation accuracy:\t\t95.50 %\n",
      "Epoch 106 of 500 took 1.919s\n",
      "  training loss:\t\t0.072705\n",
      "  validation loss:\t\t0.144637\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 107 of 500 took 1.884s\n",
      "  training loss:\t\t0.072913\n",
      "  validation loss:\t\t0.148418\n",
      "  validation accuracy:\t\t95.20 %\n",
      "Epoch 108 of 500 took 1.947s\n",
      "  training loss:\t\t0.071460\n",
      "  validation loss:\t\t0.148205\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 109 of 500 took 1.976s\n",
      "  training loss:\t\t0.070675\n",
      "  validation loss:\t\t0.137897\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 110 of 500 took 1.899s\n",
      "  training loss:\t\t0.073552\n",
      "  validation loss:\t\t0.140677\n",
      "  validation accuracy:\t\t95.20 %\n",
      "Epoch 111 of 500 took 1.920s\n",
      "  training loss:\t\t0.069354\n",
      "  validation loss:\t\t0.149757\n",
      "  validation accuracy:\t\t96.10 %\n",
      "Epoch 112 of 500 took 1.888s\n",
      "  training loss:\t\t0.068227\n",
      "  validation loss:\t\t0.139687\n",
      "  validation accuracy:\t\t95.90 %\n",
      "Epoch 113 of 500 took 1.975s\n",
      "  training loss:\t\t0.069238\n",
      "  validation loss:\t\t0.148862\n",
      "  validation accuracy:\t\t95.90 %\n",
      "Epoch 114 of 500 took 1.952s\n",
      "  training loss:\t\t0.070735\n",
      "  validation loss:\t\t0.133519\n",
      "  validation accuracy:\t\t96.00 %\n",
      "Epoch 115 of 500 took 1.959s\n",
      "  training loss:\t\t0.067341\n",
      "  validation loss:\t\t0.143359\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 116 of 500 took 1.902s\n",
      "  training loss:\t\t0.068618\n",
      "  validation loss:\t\t0.140385\n",
      "  validation accuracy:\t\t96.20 %\n",
      "Epoch 117 of 500 took 1.925s\n",
      "  training loss:\t\t0.071757\n",
      "  validation loss:\t\t0.141050\n",
      "  validation accuracy:\t\t96.30 %\n",
      "Epoch 118 of 500 took 1.919s\n",
      "  training loss:\t\t0.067194\n",
      "  validation loss:\t\t0.144382\n",
      "  validation accuracy:\t\t96.20 %\n",
      "Epoch 119 of 500 took 1.905s\n",
      "  training loss:\t\t0.065509\n",
      "  validation loss:\t\t0.150081\n",
      "  validation accuracy:\t\t95.40 %\n",
      "Epoch 120 of 500 took 1.924s\n",
      "  training loss:\t\t0.069557\n",
      "  validation loss:\t\t0.143161\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 121 of 500 took 1.889s\n",
      "  training loss:\t\t0.071435\n",
      "  validation loss:\t\t0.158484\n",
      "  validation accuracy:\t\t95.30 %\n",
      "Epoch 122 of 500 took 1.945s\n",
      "  training loss:\t\t0.069028\n",
      "  validation loss:\t\t0.149066\n",
      "  validation accuracy:\t\t95.60 %\n",
      "Epoch 123 of 500 took 1.910s\n",
      "  training loss:\t\t0.066052\n",
      "  validation loss:\t\t0.143028\n",
      "  validation accuracy:\t\t95.60 %\n",
      "Epoch 124 of 500 took 1.953s\n",
      "  training loss:\t\t0.068178\n",
      "  validation loss:\t\t0.144248\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 125 of 500 took 1.920s\n",
      "  training loss:\t\t0.064913\n",
      "  validation loss:\t\t0.155891\n",
      "  validation accuracy:\t\t95.40 %\n",
      "Epoch 126 of 500 took 1.989s\n",
      "  training loss:\t\t0.070706\n",
      "  validation loss:\t\t0.146781\n",
      "  validation accuracy:\t\t95.40 %\n",
      "Epoch 127 of 500 took 1.873s\n",
      "  training loss:\t\t0.067383\n",
      "  validation loss:\t\t0.141997\n",
      "  validation accuracy:\t\t96.00 %\n",
      "Epoch 128 of 500 took 1.928s\n",
      "  training loss:\t\t0.066999\n",
      "  validation loss:\t\t0.156208\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 129 of 500 took 1.932s\n",
      "  training loss:\t\t0.064985\n",
      "  validation loss:\t\t0.146606\n",
      "  validation accuracy:\t\t96.00 %\n",
      "Epoch 130 of 500 took 2.000s\n",
      "  training loss:\t\t0.065971\n",
      "  validation loss:\t\t0.146909\n",
      "  validation accuracy:\t\t95.90 %\n",
      "Epoch 131 of 500 took 1.961s\n",
      "  training loss:\t\t0.069737\n",
      "  validation loss:\t\t0.135959\n",
      "  validation accuracy:\t\t96.00 %\n",
      "Epoch 132 of 500 took 1.978s\n",
      "  training loss:\t\t0.068515\n",
      "  validation loss:\t\t0.142942\n",
      "  validation accuracy:\t\t96.50 %\n",
      "Epoch 133 of 500 took 1.916s\n",
      "  training loss:\t\t0.067569\n",
      "  validation loss:\t\t0.138216\n",
      "  validation accuracy:\t\t96.30 %\n",
      "Epoch 134 of 500 took 1.963s\n",
      "  training loss:\t\t0.067942\n",
      "  validation loss:\t\t0.132725\n",
      "  validation accuracy:\t\t96.20 %\n",
      "Epoch 135 of 500 took 1.928s\n",
      "  training loss:\t\t0.071074\n",
      "  validation loss:\t\t0.136368\n",
      "  validation accuracy:\t\t96.30 %\n",
      "Epoch 136 of 500 took 1.960s\n",
      "  training loss:\t\t0.066713\n",
      "  validation loss:\t\t0.140361\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 137 of 500 took 1.933s\n",
      "  training loss:\t\t0.064555\n",
      "  validation loss:\t\t0.135801\n",
      "  validation accuracy:\t\t96.00 %\n",
      "Epoch 138 of 500 took 1.941s\n",
      "  training loss:\t\t0.067298\n",
      "  validation loss:\t\t0.141707\n",
      "  validation accuracy:\t\t96.10 %\n",
      "Epoch 139 of 500 took 1.897s\n",
      "  training loss:\t\t0.064630\n",
      "  validation loss:\t\t0.156697\n",
      "  validation accuracy:\t\t96.00 %\n",
      "Epoch 140 of 500 took 1.912s\n",
      "  training loss:\t\t0.065155\n",
      "  validation loss:\t\t0.137312\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 141 of 500 took 1.999s\n",
      "  training loss:\t\t0.065966\n",
      "  validation loss:\t\t0.135164\n",
      "  validation accuracy:\t\t96.00 %\n",
      "Epoch 142 of 500 took 2.017s\n",
      "  training loss:\t\t0.064618\n",
      "  validation loss:\t\t0.151438\n",
      "  validation accuracy:\t\t96.10 %\n",
      "Epoch 143 of 500 took 1.943s\n",
      "  training loss:\t\t0.064591\n",
      "  validation loss:\t\t0.157066\n",
      "  validation accuracy:\t\t95.20 %\n",
      "Epoch 144 of 500 took 1.942s\n",
      "  training loss:\t\t0.064987\n",
      "  validation loss:\t\t0.163070\n",
      "  validation accuracy:\t\t95.30 %\n",
      "Epoch 145 of 500 took 1.887s\n",
      "  training loss:\t\t0.065963\n",
      "  validation loss:\t\t0.128168\n",
      "  validation accuracy:\t\t96.10 %\n",
      "Epoch 146 of 500 took 1.860s\n",
      "  training loss:\t\t0.069879\n",
      "  validation loss:\t\t0.133829\n",
      "  validation accuracy:\t\t95.60 %\n",
      "Epoch 147 of 500 took 1.928s\n",
      "  training loss:\t\t0.063303\n",
      "  validation loss:\t\t0.134983\n",
      "  validation accuracy:\t\t96.20 %\n",
      "Epoch 148 of 500 took 1.924s\n",
      "  training loss:\t\t0.059921\n",
      "  validation loss:\t\t0.164012\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 149 of 500 took 1.952s\n",
      "  training loss:\t\t0.064917\n",
      "  validation loss:\t\t0.149716\n",
      "  validation accuracy:\t\t95.40 %\n",
      "Epoch 150 of 500 took 1.952s\n",
      "  training loss:\t\t0.066263\n",
      "  validation loss:\t\t0.155472\n",
      "  validation accuracy:\t\t95.60 %\n",
      "Epoch 151 of 500 took 1.956s\n",
      "  training loss:\t\t0.063940\n",
      "  validation loss:\t\t0.146887\n",
      "  validation accuracy:\t\t95.30 %\n",
      "Epoch 152 of 500 took 1.937s\n",
      "  training loss:\t\t0.064173\n",
      "  validation loss:\t\t0.144506\n",
      "  validation accuracy:\t\t95.30 %\n",
      "Epoch 153 of 500 took 1.956s\n",
      "  training loss:\t\t0.063273\n",
      "  validation loss:\t\t0.142233\n",
      "  validation accuracy:\t\t96.40 %\n",
      "Epoch 154 of 500 took 1.875s\n",
      "  training loss:\t\t0.061928\n",
      "  validation loss:\t\t0.145796\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 155 of 500 took 1.936s\n",
      "  training loss:\t\t0.066115\n",
      "  validation loss:\t\t0.149910\n",
      "  validation accuracy:\t\t95.30 %\n",
      "Epoch 156 of 500 took 1.965s\n",
      "  training loss:\t\t0.063155\n",
      "  validation loss:\t\t0.157076\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 157 of 500 took 1.994s\n",
      "  training loss:\t\t0.060458\n",
      "  validation loss:\t\t0.145749\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 158 of 500 took 1.962s\n",
      "  training loss:\t\t0.062951\n",
      "  validation loss:\t\t0.161943\n",
      "  validation accuracy:\t\t95.90 %\n",
      "Epoch 159 of 500 took 1.958s\n",
      "  training loss:\t\t0.061242\n",
      "  validation loss:\t\t0.158732\n",
      "  validation accuracy:\t\t96.10 %\n",
      "Epoch 160 of 500 took 1.874s\n",
      "  training loss:\t\t0.063079\n",
      "  validation loss:\t\t0.147793\n",
      "  validation accuracy:\t\t95.30 %\n",
      "Epoch 161 of 500 took 1.942s\n",
      "  training loss:\t\t0.063888\n",
      "  validation loss:\t\t0.144862\n",
      "  validation accuracy:\t\t95.50 %\n",
      "Epoch 162 of 500 took 1.909s\n",
      "  training loss:\t\t0.061383\n",
      "  validation loss:\t\t0.153150\n",
      "  validation accuracy:\t\t95.90 %\n",
      "Epoch 163 of 500 took 1.997s\n",
      "  training loss:\t\t0.060544\n",
      "  validation loss:\t\t0.162693\n",
      "  validation accuracy:\t\t95.30 %\n",
      "Epoch 164 of 500 took 1.945s\n",
      "  training loss:\t\t0.064484\n",
      "  validation loss:\t\t0.155686\n",
      "  validation accuracy:\t\t96.00 %\n",
      "Epoch 165 of 500 took 1.981s\n",
      "  training loss:\t\t0.063248\n",
      "  validation loss:\t\t0.156493\n",
      "  validation accuracy:\t\t96.00 %\n",
      "Epoch 166 of 500 took 1.910s\n",
      "  training loss:\t\t0.061810\n",
      "  validation loss:\t\t0.142397\n",
      "  validation accuracy:\t\t95.50 %\n",
      "Epoch 167 of 500 took 2.001s\n",
      "  training loss:\t\t0.061768\n",
      "  validation loss:\t\t0.153078\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 168 of 500 took 1.937s\n",
      "  training loss:\t\t0.059132\n",
      "  validation loss:\t\t0.149811\n",
      "  validation accuracy:\t\t95.60 %\n",
      "Epoch 169 of 500 took 1.960s\n",
      "  training loss:\t\t0.062672\n",
      "  validation loss:\t\t0.161826\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 170 of 500 took 1.932s\n",
      "  training loss:\t\t0.062893\n",
      "  validation loss:\t\t0.166724\n",
      "  validation accuracy:\t\t95.10 %\n",
      "Epoch 171 of 500 took 2.029s\n",
      "  training loss:\t\t0.059819\n",
      "  validation loss:\t\t0.160183\n",
      "  validation accuracy:\t\t96.10 %\n",
      "Epoch 172 of 500 took 2.036s\n",
      "  training loss:\t\t0.058163\n",
      "  validation loss:\t\t0.159016\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 173 of 500 took 2.000s\n",
      "  training loss:\t\t0.061128\n",
      "  validation loss:\t\t0.153728\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 174 of 500 took 1.940s\n",
      "  training loss:\t\t0.061396\n",
      "  validation loss:\t\t0.163340\n",
      "  validation accuracy:\t\t95.10 %\n",
      "Epoch 175 of 500 took 1.916s\n",
      "  training loss:\t\t0.055766\n",
      "  validation loss:\t\t0.176952\n",
      "  validation accuracy:\t\t96.00 %\n",
      "Epoch 176 of 500 took 1.944s\n",
      "  training loss:\t\t0.061672\n",
      "  validation loss:\t\t0.155967\n",
      "  validation accuracy:\t\t96.20 %\n",
      "Epoch 177 of 500 took 1.930s\n",
      "  training loss:\t\t0.060770\n",
      "  validation loss:\t\t0.178207\n",
      "  validation accuracy:\t\t95.10 %\n",
      "Epoch 178 of 500 took 1.961s\n",
      "  training loss:\t\t0.060640\n",
      "  validation loss:\t\t0.156559\n",
      "  validation accuracy:\t\t95.60 %\n",
      "Epoch 179 of 500 took 1.960s\n",
      "  training loss:\t\t0.061891\n",
      "  validation loss:\t\t0.138927\n",
      "  validation accuracy:\t\t96.10 %\n",
      "Epoch 180 of 500 took 1.860s\n",
      "  training loss:\t\t0.062562\n",
      "  validation loss:\t\t0.141549\n",
      "  validation accuracy:\t\t96.70 %\n",
      "Epoch 181 of 500 took 1.910s\n",
      "  training loss:\t\t0.057521\n",
      "  validation loss:\t\t0.167331\n",
      "  validation accuracy:\t\t95.50 %\n",
      "Epoch 182 of 500 took 1.878s\n",
      "  training loss:\t\t0.061933\n",
      "  validation loss:\t\t0.158040\n",
      "  validation accuracy:\t\t96.00 %\n",
      "Epoch 183 of 500 took 1.952s\n",
      "  training loss:\t\t0.060073\n",
      "  validation loss:\t\t0.156008\n",
      "  validation accuracy:\t\t96.20 %\n",
      "Epoch 184 of 500 took 1.998s\n",
      "  training loss:\t\t0.061807\n",
      "  validation loss:\t\t0.159279\n",
      "  validation accuracy:\t\t96.00 %\n",
      "Epoch 185 of 500 took 1.900s\n",
      "  training loss:\t\t0.061312\n",
      "  validation loss:\t\t0.161450\n",
      "  validation accuracy:\t\t95.30 %\n",
      "Epoch 186 of 500 took 1.955s\n",
      "  training loss:\t\t0.063491\n",
      "  validation loss:\t\t0.172189\n",
      "  validation accuracy:\t\t95.40 %\n",
      "Epoch 187 of 500 took 1.987s\n",
      "  training loss:\t\t0.062947\n",
      "  validation loss:\t\t0.165645\n",
      "  validation accuracy:\t\t95.50 %\n",
      "Epoch 188 of 500 took 1.942s\n",
      "  training loss:\t\t0.061995\n",
      "  validation loss:\t\t0.159745\n",
      "  validation accuracy:\t\t95.20 %\n",
      "Epoch 189 of 500 took 1.991s\n",
      "  training loss:\t\t0.057579\n",
      "  validation loss:\t\t0.165909\n",
      "  validation accuracy:\t\t95.50 %\n",
      "Epoch 190 of 500 took 1.940s\n",
      "  training loss:\t\t0.063993\n",
      "  validation loss:\t\t0.174499\n",
      "  validation accuracy:\t\t95.40 %\n",
      "Epoch 191 of 500 took 1.890s\n",
      "  training loss:\t\t0.061711\n",
      "  validation loss:\t\t0.167774\n",
      "  validation accuracy:\t\t95.50 %\n",
      "Epoch 192 of 500 took 2.074s\n",
      "  training loss:\t\t0.058752\n",
      "  validation loss:\t\t0.152664\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 193 of 500 took 1.894s\n",
      "  training loss:\t\t0.057840\n",
      "  validation loss:\t\t0.173498\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 194 of 500 took 1.967s\n",
      "  training loss:\t\t0.060784\n",
      "  validation loss:\t\t0.161280\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 195 of 500 took 1.887s\n",
      "  training loss:\t\t0.056663\n",
      "  validation loss:\t\t0.166712\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 196 of 500 took 1.902s\n",
      "  training loss:\t\t0.058669\n",
      "  validation loss:\t\t0.141919\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 197 of 500 took 1.908s\n",
      "  training loss:\t\t0.058006\n",
      "  validation loss:\t\t0.178261\n",
      "  validation accuracy:\t\t95.60 %\n",
      "Epoch 198 of 500 took 1.900s\n",
      "  training loss:\t\t0.058839\n",
      "  validation loss:\t\t0.155653\n",
      "  validation accuracy:\t\t95.60 %\n",
      "Epoch 199 of 500 took 1.952s\n",
      "  training loss:\t\t0.057338\n",
      "  validation loss:\t\t0.170154\n",
      "  validation accuracy:\t\t95.90 %\n",
      "Epoch 200 of 500 took 1.933s\n",
      "  training loss:\t\t0.060269\n",
      "  validation loss:\t\t0.153383\n",
      "  validation accuracy:\t\t95.60 %\n",
      "Epoch 201 of 500 took 1.933s\n",
      "  training loss:\t\t0.056699\n",
      "  validation loss:\t\t0.177455\n",
      "  validation accuracy:\t\t95.90 %\n",
      "Epoch 202 of 500 took 1.989s\n",
      "  training loss:\t\t0.060455\n",
      "  validation loss:\t\t0.158846\n",
      "  validation accuracy:\t\t95.40 %\n",
      "Epoch 203 of 500 took 2.004s\n",
      "  training loss:\t\t0.055945\n",
      "  validation loss:\t\t0.171880\n",
      "  validation accuracy:\t\t95.90 %\n",
      "Epoch 204 of 500 took 1.951s\n",
      "  training loss:\t\t0.057383\n",
      "  validation loss:\t\t0.153089\n",
      "  validation accuracy:\t\t96.20 %\n",
      "Epoch 205 of 500 took 1.881s\n",
      "  training loss:\t\t0.060881\n",
      "  validation loss:\t\t0.153627\n",
      "  validation accuracy:\t\t95.40 %\n",
      "Epoch 206 of 500 took 1.990s\n",
      "  training loss:\t\t0.056799\n",
      "  validation loss:\t\t0.165771\n",
      "  validation accuracy:\t\t95.20 %\n",
      "Epoch 207 of 500 took 1.899s\n",
      "  training loss:\t\t0.059883\n",
      "  validation loss:\t\t0.150392\n",
      "  validation accuracy:\t\t95.60 %\n",
      "Epoch 208 of 500 took 1.992s\n",
      "  training loss:\t\t0.060799\n",
      "  validation loss:\t\t0.161716\n",
      "  validation accuracy:\t\t95.30 %\n",
      "Epoch 209 of 500 took 1.905s\n",
      "  training loss:\t\t0.058995\n",
      "  validation loss:\t\t0.148792\n",
      "  validation accuracy:\t\t95.90 %\n",
      "Epoch 210 of 500 took 1.978s\n",
      "  training loss:\t\t0.059989\n",
      "  validation loss:\t\t0.171702\n",
      "  validation accuracy:\t\t95.30 %\n",
      "Epoch 211 of 500 took 1.891s\n",
      "  training loss:\t\t0.060725\n",
      "  validation loss:\t\t0.154103\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 212 of 500 took 1.929s\n",
      "  training loss:\t\t0.059077\n",
      "  validation loss:\t\t0.149500\n",
      "  validation accuracy:\t\t96.20 %\n",
      "Epoch 213 of 500 took 1.844s\n",
      "  training loss:\t\t0.057250\n",
      "  validation loss:\t\t0.167832\n",
      "  validation accuracy:\t\t95.40 %\n",
      "Epoch 214 of 500 took 1.906s\n",
      "  training loss:\t\t0.057630\n",
      "  validation loss:\t\t0.156515\n",
      "  validation accuracy:\t\t96.10 %\n",
      "Epoch 215 of 500 took 1.885s\n",
      "  training loss:\t\t0.057598\n",
      "  validation loss:\t\t0.143021\n",
      "  validation accuracy:\t\t96.30 %\n",
      "Epoch 216 of 500 took 1.931s\n",
      "  training loss:\t\t0.060211\n",
      "  validation loss:\t\t0.143506\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 217 of 500 took 1.887s\n",
      "  training loss:\t\t0.056497\n",
      "  validation loss:\t\t0.153067\n",
      "  validation accuracy:\t\t96.20 %\n",
      "Epoch 218 of 500 took 1.950s\n",
      "  training loss:\t\t0.058049\n",
      "  validation loss:\t\t0.156817\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 219 of 500 took 1.918s\n",
      "  training loss:\t\t0.056773\n",
      "  validation loss:\t\t0.156839\n",
      "  validation accuracy:\t\t96.10 %\n",
      "Epoch 220 of 500 took 1.881s\n",
      "  training loss:\t\t0.056849\n",
      "  validation loss:\t\t0.150978\n",
      "  validation accuracy:\t\t96.30 %\n",
      "Epoch 221 of 500 took 1.948s\n",
      "  training loss:\t\t0.057134\n",
      "  validation loss:\t\t0.155751\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 222 of 500 took 1.884s\n",
      "  training loss:\t\t0.056739\n",
      "  validation loss:\t\t0.147064\n",
      "  validation accuracy:\t\t95.60 %\n",
      "Epoch 223 of 500 took 1.988s\n",
      "  training loss:\t\t0.054835\n",
      "  validation loss:\t\t0.159184\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 224 of 500 took 1.987s\n",
      "  training loss:\t\t0.056823\n",
      "  validation loss:\t\t0.166527\n",
      "  validation accuracy:\t\t95.90 %\n",
      "Epoch 225 of 500 took 1.956s\n",
      "  training loss:\t\t0.058021\n",
      "  validation loss:\t\t0.172529\n",
      "  validation accuracy:\t\t96.10 %\n",
      "Epoch 226 of 500 took 1.924s\n",
      "  training loss:\t\t0.056796\n",
      "  validation loss:\t\t0.164436\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 227 of 500 took 1.957s\n",
      "  training loss:\t\t0.056903\n",
      "  validation loss:\t\t0.148609\n",
      "  validation accuracy:\t\t96.20 %\n",
      "Epoch 228 of 500 took 1.895s\n",
      "  training loss:\t\t0.056787\n",
      "  validation loss:\t\t0.164159\n",
      "  validation accuracy:\t\t95.40 %\n",
      "Epoch 229 of 500 took 1.981s\n",
      "  training loss:\t\t0.056434\n",
      "  validation loss:\t\t0.155935\n",
      "  validation accuracy:\t\t95.60 %\n",
      "Epoch 230 of 500 took 1.865s\n",
      "  training loss:\t\t0.055836\n",
      "  validation loss:\t\t0.162556\n",
      "  validation accuracy:\t\t95.20 %\n",
      "Epoch 231 of 500 took 1.910s\n",
      "  training loss:\t\t0.054671\n",
      "  validation loss:\t\t0.175060\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 232 of 500 took 1.947s\n",
      "  training loss:\t\t0.056484\n",
      "  validation loss:\t\t0.161596\n",
      "  validation accuracy:\t\t95.10 %\n",
      "Epoch 233 of 500 took 2.018s\n",
      "  training loss:\t\t0.055879\n",
      "  validation loss:\t\t0.149483\n",
      "  validation accuracy:\t\t96.10 %\n",
      "Epoch 234 of 500 took 1.975s\n",
      "  training loss:\t\t0.056263\n",
      "  validation loss:\t\t0.178990\n",
      "  validation accuracy:\t\t95.30 %\n",
      "Epoch 235 of 500 took 2.043s\n",
      "  training loss:\t\t0.056102\n",
      "  validation loss:\t\t0.146667\n",
      "  validation accuracy:\t\t95.50 %\n",
      "Epoch 236 of 500 took 1.951s\n",
      "  training loss:\t\t0.052817\n",
      "  validation loss:\t\t0.153937\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 237 of 500 took 2.004s\n",
      "  training loss:\t\t0.051566\n",
      "  validation loss:\t\t0.159036\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 238 of 500 took 1.874s\n",
      "  training loss:\t\t0.055728\n",
      "  validation loss:\t\t0.159183\n",
      "  validation accuracy:\t\t95.40 %\n",
      "Epoch 239 of 500 took 1.972s\n",
      "  training loss:\t\t0.054718\n",
      "  validation loss:\t\t0.156802\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 240 of 500 took 1.880s\n",
      "  training loss:\t\t0.052570\n",
      "  validation loss:\t\t0.147103\n",
      "  validation accuracy:\t\t96.10 %\n",
      "Epoch 241 of 500 took 1.908s\n",
      "  training loss:\t\t0.054628\n",
      "  validation loss:\t\t0.168889\n",
      "  validation accuracy:\t\t95.10 %\n",
      "Epoch 242 of 500 took 1.903s\n",
      "  training loss:\t\t0.054940\n",
      "  validation loss:\t\t0.141280\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 243 of 500 took 1.964s\n",
      "  training loss:\t\t0.054906\n",
      "  validation loss:\t\t0.159784\n",
      "  validation accuracy:\t\t95.40 %\n",
      "Epoch 244 of 500 took 1.914s\n",
      "  training loss:\t\t0.053262\n",
      "  validation loss:\t\t0.147739\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 245 of 500 took 1.925s\n",
      "  training loss:\t\t0.055208\n",
      "  validation loss:\t\t0.151266\n",
      "  validation accuracy:\t\t96.10 %\n",
      "Epoch 246 of 500 took 1.892s\n",
      "  training loss:\t\t0.054353\n",
      "  validation loss:\t\t0.171146\n",
      "  validation accuracy:\t\t95.50 %\n",
      "Epoch 247 of 500 took 1.909s\n",
      "  training loss:\t\t0.055248\n",
      "  validation loss:\t\t0.164177\n",
      "  validation accuracy:\t\t95.60 %\n",
      "Epoch 248 of 500 took 1.907s\n",
      "  training loss:\t\t0.053062\n",
      "  validation loss:\t\t0.173954\n",
      "  validation accuracy:\t\t96.30 %\n",
      "Epoch 249 of 500 took 1.975s\n",
      "  training loss:\t\t0.052482\n",
      "  validation loss:\t\t0.177923\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 250 of 500 took 1.938s\n",
      "  training loss:\t\t0.051275\n",
      "  validation loss:\t\t0.159964\n",
      "  validation accuracy:\t\t96.00 %\n",
      "Epoch 251 of 500 took 2.022s\n",
      "  training loss:\t\t0.054130\n",
      "  validation loss:\t\t0.167983\n",
      "  validation accuracy:\t\t95.50 %\n",
      "Epoch 252 of 500 took 2.002s\n",
      "  training loss:\t\t0.054708\n",
      "  validation loss:\t\t0.164752\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 253 of 500 took 1.916s\n",
      "  training loss:\t\t0.053164\n",
      "  validation loss:\t\t0.176818\n",
      "  validation accuracy:\t\t96.00 %\n",
      "Epoch 254 of 500 took 1.909s\n",
      "  training loss:\t\t0.051874\n",
      "  validation loss:\t\t0.149560\n",
      "  validation accuracy:\t\t96.30 %\n",
      "Epoch 255 of 500 took 1.860s\n",
      "  training loss:\t\t0.053649\n",
      "  validation loss:\t\t0.172458\n",
      "  validation accuracy:\t\t95.60 %\n",
      "Epoch 256 of 500 took 1.966s\n",
      "  training loss:\t\t0.053248\n",
      "  validation loss:\t\t0.167108\n",
      "  validation accuracy:\t\t96.20 %\n",
      "Epoch 257 of 500 took 1.968s\n",
      "  training loss:\t\t0.055592\n",
      "  validation loss:\t\t0.178335\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 258 of 500 took 1.983s\n",
      "  training loss:\t\t0.053062\n",
      "  validation loss:\t\t0.183618\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 259 of 500 took 1.920s\n",
      "  training loss:\t\t0.053932\n",
      "  validation loss:\t\t0.177549\n",
      "  validation accuracy:\t\t96.20 %\n",
      "Epoch 260 of 500 took 1.963s\n",
      "  training loss:\t\t0.054440\n",
      "  validation loss:\t\t0.169575\n",
      "  validation accuracy:\t\t95.40 %\n",
      "Epoch 261 of 500 took 1.877s\n",
      "  training loss:\t\t0.052153\n",
      "  validation loss:\t\t0.183384\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 262 of 500 took 1.900s\n",
      "  training loss:\t\t0.050613\n",
      "  validation loss:\t\t0.171097\n",
      "  validation accuracy:\t\t95.20 %\n",
      "Epoch 263 of 500 took 1.852s\n",
      "  training loss:\t\t0.054539\n",
      "  validation loss:\t\t0.155755\n",
      "  validation accuracy:\t\t96.00 %\n",
      "Epoch 264 of 500 took 2.019s\n",
      "  training loss:\t\t0.054863\n",
      "  validation loss:\t\t0.153192\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 265 of 500 took 1.993s\n",
      "  training loss:\t\t0.052443\n",
      "  validation loss:\t\t0.155292\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 266 of 500 took 2.028s\n",
      "  training loss:\t\t0.051710\n",
      "  validation loss:\t\t0.167949\n",
      "  validation accuracy:\t\t96.10 %\n",
      "Epoch 267 of 500 took 1.912s\n",
      "  training loss:\t\t0.055654\n",
      "  validation loss:\t\t0.170549\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 268 of 500 took 1.942s\n",
      "  training loss:\t\t0.055441\n",
      "  validation loss:\t\t0.167100\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 269 of 500 took 1.914s\n",
      "  training loss:\t\t0.053355\n",
      "  validation loss:\t\t0.166739\n",
      "  validation accuracy:\t\t96.00 %\n",
      "Epoch 270 of 500 took 1.994s\n",
      "  training loss:\t\t0.056662\n",
      "  validation loss:\t\t0.177562\n",
      "  validation accuracy:\t\t95.60 %\n",
      "Epoch 271 of 500 took 1.953s\n",
      "  training loss:\t\t0.051134\n",
      "  validation loss:\t\t0.171105\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 272 of 500 took 1.936s\n",
      "  training loss:\t\t0.052574\n",
      "  validation loss:\t\t0.171084\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 273 of 500 took 1.900s\n",
      "  training loss:\t\t0.051857\n",
      "  validation loss:\t\t0.157268\n",
      "  validation accuracy:\t\t96.30 %\n",
      "Epoch 274 of 500 took 1.922s\n",
      "  training loss:\t\t0.054696\n",
      "  validation loss:\t\t0.161496\n",
      "  validation accuracy:\t\t95.40 %\n",
      "Epoch 275 of 500 took 1.942s\n",
      "  training loss:\t\t0.051645\n",
      "  validation loss:\t\t0.167073\n",
      "  validation accuracy:\t\t95.40 %\n",
      "Epoch 276 of 500 took 1.933s\n",
      "  training loss:\t\t0.054225\n",
      "  validation loss:\t\t0.168388\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 277 of 500 took 1.973s\n",
      "  training loss:\t\t0.053859\n",
      "  validation loss:\t\t0.163335\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 278 of 500 took 1.896s\n",
      "  training loss:\t\t0.054497\n",
      "  validation loss:\t\t0.156904\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 279 of 500 took 2.009s\n",
      "  training loss:\t\t0.053967\n",
      "  validation loss:\t\t0.174754\n",
      "  validation accuracy:\t\t95.90 %\n",
      "Epoch 280 of 500 took 1.937s\n",
      "  training loss:\t\t0.051087\n",
      "  validation loss:\t\t0.169934\n",
      "  validation accuracy:\t\t96.10 %\n",
      "Epoch 281 of 500 took 1.987s\n",
      "  training loss:\t\t0.051893\n",
      "  validation loss:\t\t0.171848\n",
      "  validation accuracy:\t\t95.40 %\n",
      "Epoch 282 of 500 took 1.954s\n",
      "  training loss:\t\t0.051305\n",
      "  validation loss:\t\t0.178997\n",
      "  validation accuracy:\t\t95.40 %\n",
      "Epoch 283 of 500 took 1.945s\n",
      "  training loss:\t\t0.047425\n",
      "  validation loss:\t\t0.184664\n",
      "  validation accuracy:\t\t95.20 %\n",
      "Epoch 284 of 500 took 1.927s\n",
      "  training loss:\t\t0.053926\n",
      "  validation loss:\t\t0.168904\n",
      "  validation accuracy:\t\t95.90 %\n",
      "Epoch 285 of 500 took 1.996s\n",
      "  training loss:\t\t0.054496\n",
      "  validation loss:\t\t0.185537\n",
      "  validation accuracy:\t\t95.30 %\n",
      "Epoch 286 of 500 took 1.906s\n",
      "  training loss:\t\t0.053639\n",
      "  validation loss:\t\t0.177473\n",
      "  validation accuracy:\t\t95.50 %\n",
      "Epoch 287 of 500 took 1.948s\n",
      "  training loss:\t\t0.051539\n",
      "  validation loss:\t\t0.160868\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 288 of 500 took 1.911s\n",
      "  training loss:\t\t0.049630\n",
      "  validation loss:\t\t0.160669\n",
      "  validation accuracy:\t\t95.40 %\n",
      "Epoch 289 of 500 took 1.992s\n",
      "  training loss:\t\t0.052583\n",
      "  validation loss:\t\t0.180810\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 290 of 500 took 1.984s\n",
      "  training loss:\t\t0.053137\n",
      "  validation loss:\t\t0.187518\n",
      "  validation accuracy:\t\t95.00 %\n",
      "Epoch 291 of 500 took 2.011s\n",
      "  training loss:\t\t0.052401\n",
      "  validation loss:\t\t0.171979\n",
      "  validation accuracy:\t\t95.20 %\n",
      "Epoch 292 of 500 took 1.974s\n",
      "  training loss:\t\t0.051903\n",
      "  validation loss:\t\t0.158123\n",
      "  validation accuracy:\t\t96.30 %\n",
      "Epoch 293 of 500 took 1.997s\n",
      "  training loss:\t\t0.054030\n",
      "  validation loss:\t\t0.182282\n",
      "  validation accuracy:\t\t96.10 %\n",
      "Epoch 294 of 500 took 1.881s\n",
      "  training loss:\t\t0.054397\n",
      "  validation loss:\t\t0.166784\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 295 of 500 took 1.917s\n",
      "  training loss:\t\t0.051696\n",
      "  validation loss:\t\t0.178119\n",
      "  validation accuracy:\t\t95.00 %\n",
      "Epoch 296 of 500 took 1.954s\n",
      "  training loss:\t\t0.050171\n",
      "  validation loss:\t\t0.191217\n",
      "  validation accuracy:\t\t95.20 %\n",
      "Epoch 297 of 500 took 2.500s\n",
      "  training loss:\t\t0.052249\n",
      "  validation loss:\t\t0.168695\n",
      "  validation accuracy:\t\t95.90 %\n",
      "Epoch 298 of 500 took 2.244s\n",
      "  training loss:\t\t0.054288\n",
      "  validation loss:\t\t0.167595\n",
      "  validation accuracy:\t\t95.60 %\n",
      "Epoch 299 of 500 took 1.904s\n",
      "  training loss:\t\t0.050750\n",
      "  validation loss:\t\t0.176940\n",
      "  validation accuracy:\t\t95.30 %\n",
      "Epoch 300 of 500 took 1.954s\n",
      "  training loss:\t\t0.053339\n",
      "  validation loss:\t\t0.181511\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 301 of 500 took 1.943s\n",
      "  training loss:\t\t0.049029\n",
      "  validation loss:\t\t0.169956\n",
      "  validation accuracy:\t\t95.50 %\n",
      "Epoch 302 of 500 took 1.883s\n",
      "  training loss:\t\t0.052233\n",
      "  validation loss:\t\t0.190075\n",
      "  validation accuracy:\t\t95.10 %\n",
      "Epoch 303 of 500 took 1.995s\n",
      "  training loss:\t\t0.051119\n",
      "  validation loss:\t\t0.179317\n",
      "  validation accuracy:\t\t96.20 %\n",
      "Epoch 304 of 500 took 1.897s\n",
      "  training loss:\t\t0.052433\n",
      "  validation loss:\t\t0.178390\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 305 of 500 took 2.029s\n",
      "  training loss:\t\t0.050546\n",
      "  validation loss:\t\t0.174720\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 306 of 500 took 1.948s\n",
      "  training loss:\t\t0.052303\n",
      "  validation loss:\t\t0.168050\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 307 of 500 took 2.000s\n",
      "  training loss:\t\t0.051405\n",
      "  validation loss:\t\t0.176834\n",
      "  validation accuracy:\t\t95.40 %\n",
      "Epoch 308 of 500 took 1.940s\n",
      "  training loss:\t\t0.052223\n",
      "  validation loss:\t\t0.181851\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 309 of 500 took 2.078s\n",
      "  training loss:\t\t0.051990\n",
      "  validation loss:\t\t0.186771\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 310 of 500 took 1.950s\n",
      "  training loss:\t\t0.051629\n",
      "  validation loss:\t\t0.163740\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 311 of 500 took 1.915s\n",
      "  training loss:\t\t0.050633\n",
      "  validation loss:\t\t0.163114\n",
      "  validation accuracy:\t\t96.20 %\n",
      "Epoch 312 of 500 took 1.901s\n",
      "  training loss:\t\t0.051535\n",
      "  validation loss:\t\t0.174812\n",
      "  validation accuracy:\t\t96.00 %\n",
      "Epoch 313 of 500 took 1.917s\n",
      "  training loss:\t\t0.049371\n",
      "  validation loss:\t\t0.163881\n",
      "  validation accuracy:\t\t95.90 %\n",
      "Epoch 314 of 500 took 1.892s\n",
      "  training loss:\t\t0.052226\n",
      "  validation loss:\t\t0.198566\n",
      "  validation accuracy:\t\t95.10 %\n",
      "Epoch 315 of 500 took 1.911s\n",
      "  training loss:\t\t0.047974\n",
      "  validation loss:\t\t0.165960\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 316 of 500 took 1.879s\n",
      "  training loss:\t\t0.052765\n",
      "  validation loss:\t\t0.152079\n",
      "  validation accuracy:\t\t95.90 %\n",
      "Epoch 317 of 500 took 1.989s\n",
      "  training loss:\t\t0.050882\n",
      "  validation loss:\t\t0.154909\n",
      "  validation accuracy:\t\t95.90 %\n",
      "Epoch 318 of 500 took 1.886s\n",
      "  training loss:\t\t0.052251\n",
      "  validation loss:\t\t0.164919\n",
      "  validation accuracy:\t\t95.40 %\n",
      "Epoch 319 of 500 took 1.899s\n",
      "  training loss:\t\t0.048336\n",
      "  validation loss:\t\t0.172379\n",
      "  validation accuracy:\t\t95.40 %\n",
      "Epoch 320 of 500 took 1.873s\n",
      "  training loss:\t\t0.051148\n",
      "  validation loss:\t\t0.159136\n",
      "  validation accuracy:\t\t96.20 %\n",
      "Epoch 321 of 500 took 2.213s\n",
      "  training loss:\t\t0.050498\n",
      "  validation loss:\t\t0.160720\n",
      "  validation accuracy:\t\t96.10 %\n",
      "Epoch 322 of 500 took 2.334s\n",
      "  training loss:\t\t0.049841\n",
      "  validation loss:\t\t0.160289\n",
      "  validation accuracy:\t\t96.00 %\n",
      "Epoch 323 of 500 took 3.139s\n",
      "  training loss:\t\t0.051115\n",
      "  validation loss:\t\t0.168775\n",
      "  validation accuracy:\t\t96.00 %\n",
      "Epoch 324 of 500 took 2.125s\n",
      "  training loss:\t\t0.053305\n",
      "  validation loss:\t\t0.172666\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 325 of 500 took 2.070s\n",
      "  training loss:\t\t0.049549\n",
      "  validation loss:\t\t0.166003\n",
      "  validation accuracy:\t\t96.10 %\n",
      "Epoch 326 of 500 took 1.800s\n",
      "  training loss:\t\t0.050908\n",
      "  validation loss:\t\t0.186045\n",
      "  validation accuracy:\t\t95.20 %\n",
      "Epoch 327 of 500 took 1.767s\n",
      "  training loss:\t\t0.048804\n",
      "  validation loss:\t\t0.161441\n",
      "  validation accuracy:\t\t95.60 %\n",
      "Epoch 328 of 500 took 2.042s\n",
      "  training loss:\t\t0.051915\n",
      "  validation loss:\t\t0.162843\n",
      "  validation accuracy:\t\t95.60 %\n",
      "Epoch 329 of 500 took 1.860s\n",
      "  training loss:\t\t0.051084\n",
      "  validation loss:\t\t0.169986\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 330 of 500 took 1.738s\n",
      "  training loss:\t\t0.050188\n",
      "  validation loss:\t\t0.182390\n",
      "  validation accuracy:\t\t95.60 %\n",
      "Epoch 331 of 500 took 1.765s\n",
      "  training loss:\t\t0.048929\n",
      "  validation loss:\t\t0.176438\n",
      "  validation accuracy:\t\t96.00 %\n",
      "Epoch 332 of 500 took 2.365s\n",
      "  training loss:\t\t0.050375\n",
      "  validation loss:\t\t0.158495\n",
      "  validation accuracy:\t\t96.40 %\n",
      "Epoch 333 of 500 took 2.209s\n",
      "  training loss:\t\t0.051078\n",
      "  validation loss:\t\t0.167580\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 334 of 500 took 2.347s\n",
      "  training loss:\t\t0.052448\n",
      "  validation loss:\t\t0.170940\n",
      "  validation accuracy:\t\t95.50 %\n",
      "Epoch 335 of 500 took 2.791s\n",
      "  training loss:\t\t0.050158\n",
      "  validation loss:\t\t0.177005\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 336 of 500 took 3.407s\n",
      "  training loss:\t\t0.050206\n",
      "  validation loss:\t\t0.172159\n",
      "  validation accuracy:\t\t95.60 %\n",
      "Epoch 337 of 500 took 3.021s\n",
      "  training loss:\t\t0.049441\n",
      "  validation loss:\t\t0.154673\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 338 of 500 took 4.610s\n",
      "  training loss:\t\t0.049030\n",
      "  validation loss:\t\t0.171712\n",
      "  validation accuracy:\t\t95.90 %\n",
      "Epoch 339 of 500 took 5.308s\n",
      "  training loss:\t\t0.048616\n",
      "  validation loss:\t\t0.176278\n",
      "  validation accuracy:\t\t95.50 %\n",
      "Epoch 340 of 500 took 3.846s\n",
      "  training loss:\t\t0.049980\n",
      "  validation loss:\t\t0.173340\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 341 of 500 took 2.622s\n",
      "  training loss:\t\t0.049939\n",
      "  validation loss:\t\t0.174698\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 342 of 500 took 1.868s\n",
      "  training loss:\t\t0.051123\n",
      "  validation loss:\t\t0.173994\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 343 of 500 took 1.839s\n",
      "  training loss:\t\t0.049920\n",
      "  validation loss:\t\t0.158965\n",
      "  validation accuracy:\t\t95.90 %\n",
      "Epoch 344 of 500 took 1.886s\n",
      "  training loss:\t\t0.049849\n",
      "  validation loss:\t\t0.182865\n",
      "  validation accuracy:\t\t95.50 %\n",
      "Epoch 345 of 500 took 2.206s\n",
      "  training loss:\t\t0.051880\n",
      "  validation loss:\t\t0.174714\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 346 of 500 took 2.199s\n",
      "  training loss:\t\t0.053502\n",
      "  validation loss:\t\t0.184510\n",
      "  validation accuracy:\t\t95.30 %\n",
      "Epoch 347 of 500 took 1.964s\n",
      "  training loss:\t\t0.046991\n",
      "  validation loss:\t\t0.164418\n",
      "  validation accuracy:\t\t95.50 %\n",
      "Epoch 348 of 500 took 1.966s\n",
      "  training loss:\t\t0.049525\n",
      "  validation loss:\t\t0.157698\n",
      "  validation accuracy:\t\t96.30 %\n",
      "Epoch 349 of 500 took 2.262s\n",
      "  training loss:\t\t0.050660\n",
      "  validation loss:\t\t0.171062\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 350 of 500 took 2.318s\n",
      "  training loss:\t\t0.052266\n",
      "  validation loss:\t\t0.175209\n",
      "  validation accuracy:\t\t95.50 %\n",
      "Epoch 351 of 500 took 1.949s\n",
      "  training loss:\t\t0.050137\n",
      "  validation loss:\t\t0.175407\n",
      "  validation accuracy:\t\t96.00 %\n",
      "Epoch 352 of 500 took 1.995s\n",
      "  training loss:\t\t0.049521\n",
      "  validation loss:\t\t0.177847\n",
      "  validation accuracy:\t\t95.60 %\n",
      "Epoch 353 of 500 took 2.173s\n",
      "  training loss:\t\t0.050553\n",
      "  validation loss:\t\t0.164983\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 354 of 500 took 2.137s\n",
      "  training loss:\t\t0.047695\n",
      "  validation loss:\t\t0.166557\n",
      "  validation accuracy:\t\t95.90 %\n",
      "Epoch 355 of 500 took 2.221s\n",
      "  training loss:\t\t0.046687\n",
      "  validation loss:\t\t0.183517\n",
      "  validation accuracy:\t\t95.60 %\n",
      "Epoch 356 of 500 took 2.330s\n",
      "  training loss:\t\t0.049878\n",
      "  validation loss:\t\t0.198284\n",
      "  validation accuracy:\t\t95.40 %\n",
      "Epoch 357 of 500 took 2.609s\n",
      "  training loss:\t\t0.050927\n",
      "  validation loss:\t\t0.200360\n",
      "  validation accuracy:\t\t95.20 %\n",
      "Epoch 358 of 500 took 2.385s\n",
      "  training loss:\t\t0.048446\n",
      "  validation loss:\t\t0.163198\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 359 of 500 took 2.429s\n",
      "  training loss:\t\t0.049401\n",
      "  validation loss:\t\t0.174950\n",
      "  validation accuracy:\t\t95.10 %\n",
      "Epoch 360 of 500 took 1.979s\n",
      "  training loss:\t\t0.052017\n",
      "  validation loss:\t\t0.185820\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 361 of 500 took 2.319s\n",
      "  training loss:\t\t0.047900\n",
      "  validation loss:\t\t0.181611\n",
      "  validation accuracy:\t\t95.50 %\n",
      "Epoch 362 of 500 took 2.440s\n",
      "  training loss:\t\t0.048126\n",
      "  validation loss:\t\t0.184603\n",
      "  validation accuracy:\t\t95.40 %\n",
      "Epoch 363 of 500 took 2.197s\n",
      "  training loss:\t\t0.053124\n",
      "  validation loss:\t\t0.175577\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 364 of 500 took 2.216s\n",
      "  training loss:\t\t0.050657\n",
      "  validation loss:\t\t0.170681\n",
      "  validation accuracy:\t\t95.10 %\n",
      "Epoch 365 of 500 took 2.068s\n",
      "  training loss:\t\t0.050020\n",
      "  validation loss:\t\t0.179731\n",
      "  validation accuracy:\t\t95.40 %\n",
      "Epoch 366 of 500 took 2.536s\n",
      "  training loss:\t\t0.047680\n",
      "  validation loss:\t\t0.159055\n",
      "  validation accuracy:\t\t96.00 %\n",
      "Epoch 367 of 500 took 2.953s\n",
      "  training loss:\t\t0.051598\n",
      "  validation loss:\t\t0.172959\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 368 of 500 took 2.541s\n",
      "  training loss:\t\t0.051284\n",
      "  validation loss:\t\t0.181633\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 369 of 500 took 2.222s\n",
      "  training loss:\t\t0.048384\n",
      "  validation loss:\t\t0.171040\n",
      "  validation accuracy:\t\t95.50 %\n",
      "Epoch 370 of 500 took 2.291s\n",
      "  training loss:\t\t0.049437\n",
      "  validation loss:\t\t0.190307\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 371 of 500 took 3.134s\n",
      "  training loss:\t\t0.046585\n",
      "  validation loss:\t\t0.166430\n",
      "  validation accuracy:\t\t95.90 %\n",
      "Epoch 372 of 500 took 3.634s\n",
      "  training loss:\t\t0.047628\n",
      "  validation loss:\t\t0.151268\n",
      "  validation accuracy:\t\t96.20 %\n",
      "Epoch 373 of 500 took 2.497s\n",
      "  training loss:\t\t0.050958\n",
      "  validation loss:\t\t0.167295\n",
      "  validation accuracy:\t\t95.90 %\n",
      "Epoch 374 of 500 took 2.533s\n",
      "  training loss:\t\t0.049765\n",
      "  validation loss:\t\t0.160670\n",
      "  validation accuracy:\t\t96.10 %\n",
      "Epoch 375 of 500 took 1.779s\n",
      "  training loss:\t\t0.051350\n",
      "  validation loss:\t\t0.188944\n",
      "  validation accuracy:\t\t96.20 %\n",
      "Epoch 376 of 500 took 2.216s\n",
      "  training loss:\t\t0.049957\n",
      "  validation loss:\t\t0.179480\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 377 of 500 took 1.889s\n",
      "  training loss:\t\t0.048732\n",
      "  validation loss:\t\t0.171135\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 378 of 500 took 1.807s\n",
      "  training loss:\t\t0.050377\n",
      "  validation loss:\t\t0.166061\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 379 of 500 took 1.862s\n",
      "  training loss:\t\t0.045323\n",
      "  validation loss:\t\t0.199186\n",
      "  validation accuracy:\t\t95.20 %\n",
      "Epoch 380 of 500 took 1.767s\n",
      "  training loss:\t\t0.048925\n",
      "  validation loss:\t\t0.181255\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 381 of 500 took 1.948s\n",
      "  training loss:\t\t0.047581\n",
      "  validation loss:\t\t0.182802\n",
      "  validation accuracy:\t\t95.60 %\n",
      "Epoch 382 of 500 took 2.260s\n",
      "  training loss:\t\t0.047343\n",
      "  validation loss:\t\t0.191190\n",
      "  validation accuracy:\t\t95.90 %\n",
      "Epoch 383 of 500 took 1.889s\n",
      "  training loss:\t\t0.051603\n",
      "  validation loss:\t\t0.171687\n",
      "  validation accuracy:\t\t95.60 %\n",
      "Epoch 384 of 500 took 2.228s\n",
      "  training loss:\t\t0.050787\n",
      "  validation loss:\t\t0.179268\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 385 of 500 took 1.917s\n",
      "  training loss:\t\t0.046787\n",
      "  validation loss:\t\t0.181707\n",
      "  validation accuracy:\t\t95.90 %\n",
      "Epoch 386 of 500 took 1.905s\n",
      "  training loss:\t\t0.050552\n",
      "  validation loss:\t\t0.176524\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 387 of 500 took 1.727s\n",
      "  training loss:\t\t0.050190\n",
      "  validation loss:\t\t0.188370\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 388 of 500 took 1.730s\n",
      "  training loss:\t\t0.049484\n",
      "  validation loss:\t\t0.169959\n",
      "  validation accuracy:\t\t95.50 %\n",
      "Epoch 389 of 500 took 1.726s\n",
      "  training loss:\t\t0.048449\n",
      "  validation loss:\t\t0.167845\n",
      "  validation accuracy:\t\t95.90 %\n",
      "Epoch 390 of 500 took 1.781s\n",
      "  training loss:\t\t0.048416\n",
      "  validation loss:\t\t0.186720\n",
      "  validation accuracy:\t\t95.40 %\n",
      "Epoch 391 of 500 took 1.841s\n",
      "  training loss:\t\t0.044324\n",
      "  validation loss:\t\t0.177147\n",
      "  validation accuracy:\t\t96.00 %\n",
      "Epoch 392 of 500 took 1.685s\n",
      "  training loss:\t\t0.047875\n",
      "  validation loss:\t\t0.172514\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 393 of 500 took 1.670s\n",
      "  training loss:\t\t0.047655\n",
      "  validation loss:\t\t0.161522\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 394 of 500 took 1.653s\n",
      "  training loss:\t\t0.047414\n",
      "  validation loss:\t\t0.190699\n",
      "  validation accuracy:\t\t95.60 %\n",
      "Epoch 395 of 500 took 1.679s\n",
      "  training loss:\t\t0.046573\n",
      "  validation loss:\t\t0.190358\n",
      "  validation accuracy:\t\t95.90 %\n",
      "Epoch 396 of 500 took 1.789s\n",
      "  training loss:\t\t0.051811\n",
      "  validation loss:\t\t0.183999\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 397 of 500 took 1.660s\n",
      "  training loss:\t\t0.048222\n",
      "  validation loss:\t\t0.205881\n",
      "  validation accuracy:\t\t95.20 %\n",
      "Epoch 398 of 500 took 1.715s\n",
      "  training loss:\t\t0.049528\n",
      "  validation loss:\t\t0.184390\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 399 of 500 took 1.853s\n",
      "  training loss:\t\t0.046394\n",
      "  validation loss:\t\t0.184613\n",
      "  validation accuracy:\t\t95.20 %\n",
      "Epoch 400 of 500 took 1.812s\n",
      "  training loss:\t\t0.048757\n",
      "  validation loss:\t\t0.188677\n",
      "  validation accuracy:\t\t95.50 %\n",
      "Epoch 401 of 500 took 1.692s\n",
      "  training loss:\t\t0.048553\n",
      "  validation loss:\t\t0.196889\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 402 of 500 took 1.754s\n",
      "  training loss:\t\t0.047111\n",
      "  validation loss:\t\t0.186445\n",
      "  validation accuracy:\t\t96.00 %\n",
      "Epoch 403 of 500 took 1.716s\n",
      "  training loss:\t\t0.047273\n",
      "  validation loss:\t\t0.182747\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 404 of 500 took 1.812s\n",
      "  training loss:\t\t0.047419\n",
      "  validation loss:\t\t0.184105\n",
      "  validation accuracy:\t\t95.90 %\n",
      "Epoch 405 of 500 took 1.704s\n",
      "  training loss:\t\t0.048997\n",
      "  validation loss:\t\t0.178958\n",
      "  validation accuracy:\t\t95.30 %\n",
      "Epoch 406 of 500 took 1.674s\n",
      "  training loss:\t\t0.047456\n",
      "  validation loss:\t\t0.206500\n",
      "  validation accuracy:\t\t95.00 %\n",
      "Epoch 407 of 500 took 1.661s\n",
      "  training loss:\t\t0.048066\n",
      "  validation loss:\t\t0.203424\n",
      "  validation accuracy:\t\t95.00 %\n",
      "Epoch 408 of 500 took 1.719s\n",
      "  training loss:\t\t0.047932\n",
      "  validation loss:\t\t0.181682\n",
      "  validation accuracy:\t\t95.20 %\n",
      "Epoch 409 of 500 took 1.889s\n",
      "  training loss:\t\t0.048013\n",
      "  validation loss:\t\t0.193009\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 410 of 500 took 1.867s\n",
      "  training loss:\t\t0.046795\n",
      "  validation loss:\t\t0.190309\n",
      "  validation accuracy:\t\t95.50 %\n",
      "Epoch 411 of 500 took 2.082s\n",
      "  training loss:\t\t0.045506\n",
      "  validation loss:\t\t0.197929\n",
      "  validation accuracy:\t\t95.20 %\n",
      "Epoch 412 of 500 took 2.105s\n",
      "  training loss:\t\t0.049033\n",
      "  validation loss:\t\t0.184615\n",
      "  validation accuracy:\t\t96.40 %\n",
      "Epoch 413 of 500 took 2.006s\n",
      "  training loss:\t\t0.046261\n",
      "  validation loss:\t\t0.193392\n",
      "  validation accuracy:\t\t95.50 %\n",
      "Epoch 414 of 500 took 2.452s\n",
      "  training loss:\t\t0.045398\n",
      "  validation loss:\t\t0.197288\n",
      "  validation accuracy:\t\t95.50 %\n",
      "Epoch 415 of 500 took 2.709s\n",
      "  training loss:\t\t0.046696\n",
      "  validation loss:\t\t0.188965\n",
      "  validation accuracy:\t\t95.60 %\n",
      "Epoch 416 of 500 took 2.458s\n",
      "  training loss:\t\t0.046795\n",
      "  validation loss:\t\t0.182198\n",
      "  validation accuracy:\t\t96.00 %\n",
      "Epoch 417 of 500 took 2.190s\n",
      "  training loss:\t\t0.047311\n",
      "  validation loss:\t\t0.211866\n",
      "  validation accuracy:\t\t95.30 %\n",
      "Epoch 418 of 500 took 2.414s\n",
      "  training loss:\t\t0.046897\n",
      "  validation loss:\t\t0.193581\n",
      "  validation accuracy:\t\t95.40 %\n",
      "Epoch 419 of 500 took 1.851s\n",
      "  training loss:\t\t0.045738\n",
      "  validation loss:\t\t0.192093\n",
      "  validation accuracy:\t\t95.50 %\n",
      "Epoch 420 of 500 took 1.672s\n",
      "  training loss:\t\t0.046232\n",
      "  validation loss:\t\t0.189124\n",
      "  validation accuracy:\t\t95.50 %\n",
      "Epoch 421 of 500 took 1.807s\n",
      "  training loss:\t\t0.047888\n",
      "  validation loss:\t\t0.171707\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 422 of 500 took 1.683s\n",
      "  training loss:\t\t0.047496\n",
      "  validation loss:\t\t0.187376\n",
      "  validation accuracy:\t\t95.60 %\n",
      "Epoch 423 of 500 took 1.801s\n",
      "  training loss:\t\t0.046979\n",
      "  validation loss:\t\t0.173676\n",
      "  validation accuracy:\t\t95.60 %\n",
      "Epoch 424 of 500 took 1.707s\n",
      "  training loss:\t\t0.047705\n",
      "  validation loss:\t\t0.165515\n",
      "  validation accuracy:\t\t95.90 %\n",
      "Epoch 425 of 500 took 1.725s\n",
      "  training loss:\t\t0.047130\n",
      "  validation loss:\t\t0.177905\n",
      "  validation accuracy:\t\t95.60 %\n",
      "Epoch 426 of 500 took 1.735s\n",
      "  training loss:\t\t0.045063\n",
      "  validation loss:\t\t0.175173\n",
      "  validation accuracy:\t\t95.00 %\n",
      "Epoch 427 of 500 took 1.675s\n",
      "  training loss:\t\t0.049739\n",
      "  validation loss:\t\t0.188649\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 428 of 500 took 1.662s\n",
      "  training loss:\t\t0.049929\n",
      "  validation loss:\t\t0.180956\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 429 of 500 took 1.649s\n",
      "  training loss:\t\t0.048030\n",
      "  validation loss:\t\t0.174039\n",
      "  validation accuracy:\t\t96.20 %\n",
      "Epoch 430 of 500 took 1.660s\n",
      "  training loss:\t\t0.044160\n",
      "  validation loss:\t\t0.182948\n",
      "  validation accuracy:\t\t95.60 %\n",
      "Epoch 431 of 500 took 1.666s\n",
      "  training loss:\t\t0.051693\n",
      "  validation loss:\t\t0.186226\n",
      "  validation accuracy:\t\t95.40 %\n",
      "Epoch 432 of 500 took 1.662s\n",
      "  training loss:\t\t0.046162\n",
      "  validation loss:\t\t0.188300\n",
      "  validation accuracy:\t\t95.60 %\n",
      "Epoch 433 of 500 took 1.646s\n",
      "  training loss:\t\t0.049238\n",
      "  validation loss:\t\t0.172472\n",
      "  validation accuracy:\t\t96.10 %\n",
      "Epoch 434 of 500 took 1.686s\n",
      "  training loss:\t\t0.047108\n",
      "  validation loss:\t\t0.189616\n",
      "  validation accuracy:\t\t95.50 %\n",
      "Epoch 435 of 500 took 1.648s\n",
      "  training loss:\t\t0.045869\n",
      "  validation loss:\t\t0.186657\n",
      "  validation accuracy:\t\t96.00 %\n",
      "Epoch 436 of 500 took 1.662s\n",
      "  training loss:\t\t0.047949\n",
      "  validation loss:\t\t0.186889\n",
      "  validation accuracy:\t\t95.50 %\n",
      "Epoch 437 of 500 took 1.678s\n",
      "  training loss:\t\t0.051782\n",
      "  validation loss:\t\t0.181094\n",
      "  validation accuracy:\t\t95.60 %\n",
      "Epoch 438 of 500 took 1.643s\n",
      "  training loss:\t\t0.045031\n",
      "  validation loss:\t\t0.190085\n",
      "  validation accuracy:\t\t95.60 %\n",
      "Epoch 439 of 500 took 1.675s\n",
      "  training loss:\t\t0.043911\n",
      "  validation loss:\t\t0.180989\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 440 of 500 took 1.654s\n",
      "  training loss:\t\t0.043752\n",
      "  validation loss:\t\t0.173101\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 441 of 500 took 1.654s\n",
      "  training loss:\t\t0.048597\n",
      "  validation loss:\t\t0.188165\n",
      "  validation accuracy:\t\t96.00 %\n",
      "Epoch 442 of 500 took 1.653s\n",
      "  training loss:\t\t0.044499\n",
      "  validation loss:\t\t0.186994\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 443 of 500 took 1.652s\n",
      "  training loss:\t\t0.044145\n",
      "  validation loss:\t\t0.189426\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 444 of 500 took 1.658s\n",
      "  training loss:\t\t0.048056\n",
      "  validation loss:\t\t0.174661\n",
      "  validation accuracy:\t\t96.10 %\n",
      "Epoch 445 of 500 took 1.659s\n",
      "  training loss:\t\t0.048925\n",
      "  validation loss:\t\t0.175728\n",
      "  validation accuracy:\t\t96.30 %\n",
      "Epoch 446 of 500 took 1.666s\n",
      "  training loss:\t\t0.043514\n",
      "  validation loss:\t\t0.181334\n",
      "  validation accuracy:\t\t96.10 %\n",
      "Epoch 447 of 500 took 1.661s\n",
      "  training loss:\t\t0.048535\n",
      "  validation loss:\t\t0.178012\n",
      "  validation accuracy:\t\t96.20 %\n",
      "Epoch 448 of 500 took 1.673s\n",
      "  training loss:\t\t0.045026\n",
      "  validation loss:\t\t0.180718\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 449 of 500 took 1.676s\n",
      "  training loss:\t\t0.043189\n",
      "  validation loss:\t\t0.194647\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 450 of 500 took 1.652s\n",
      "  training loss:\t\t0.046179\n",
      "  validation loss:\t\t0.193016\n",
      "  validation accuracy:\t\t96.10 %\n",
      "Epoch 451 of 500 took 1.925s\n",
      "  training loss:\t\t0.042141\n",
      "  validation loss:\t\t0.191825\n",
      "  validation accuracy:\t\t96.40 %\n",
      "Epoch 452 of 500 took 1.743s\n",
      "  training loss:\t\t0.045126\n",
      "  validation loss:\t\t0.205667\n",
      "  validation accuracy:\t\t95.60 %\n",
      "Epoch 453 of 500 took 2.092s\n",
      "  training loss:\t\t0.046962\n",
      "  validation loss:\t\t0.192354\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 454 of 500 took 2.047s\n",
      "  training loss:\t\t0.047633\n",
      "  validation loss:\t\t0.204423\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 455 of 500 took 2.081s\n",
      "  training loss:\t\t0.046164\n",
      "  validation loss:\t\t0.190931\n",
      "  validation accuracy:\t\t96.10 %\n",
      "Epoch 456 of 500 took 1.804s\n",
      "  training loss:\t\t0.044618\n",
      "  validation loss:\t\t0.184224\n",
      "  validation accuracy:\t\t96.00 %\n",
      "Epoch 457 of 500 took 1.946s\n",
      "  training loss:\t\t0.046626\n",
      "  validation loss:\t\t0.195623\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 458 of 500 took 2.250s\n",
      "  training loss:\t\t0.044048\n",
      "  validation loss:\t\t0.187134\n",
      "  validation accuracy:\t\t96.00 %\n",
      "Epoch 459 of 500 took 1.977s\n",
      "  training loss:\t\t0.049438\n",
      "  validation loss:\t\t0.181851\n",
      "  validation accuracy:\t\t95.50 %\n",
      "Epoch 460 of 500 took 1.699s\n",
      "  training loss:\t\t0.046440\n",
      "  validation loss:\t\t0.196708\n",
      "  validation accuracy:\t\t95.20 %\n",
      "Epoch 461 of 500 took 1.658s\n",
      "  training loss:\t\t0.045971\n",
      "  validation loss:\t\t0.184535\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 462 of 500 took 1.649s\n",
      "  training loss:\t\t0.048526\n",
      "  validation loss:\t\t0.186537\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 463 of 500 took 1.658s\n",
      "  training loss:\t\t0.046722\n",
      "  validation loss:\t\t0.182667\n",
      "  validation accuracy:\t\t95.30 %\n",
      "Epoch 464 of 500 took 1.658s\n",
      "  training loss:\t\t0.044699\n",
      "  validation loss:\t\t0.177190\n",
      "  validation accuracy:\t\t96.20 %\n",
      "Epoch 465 of 500 took 1.646s\n",
      "  training loss:\t\t0.043765\n",
      "  validation loss:\t\t0.192092\n",
      "  validation accuracy:\t\t95.50 %\n",
      "Epoch 466 of 500 took 1.652s\n",
      "  training loss:\t\t0.045700\n",
      "  validation loss:\t\t0.178295\n",
      "  validation accuracy:\t\t96.00 %\n",
      "Epoch 467 of 500 took 1.648s\n",
      "  training loss:\t\t0.046855\n",
      "  validation loss:\t\t0.196243\n",
      "  validation accuracy:\t\t95.50 %\n",
      "Epoch 468 of 500 took 1.659s\n",
      "  training loss:\t\t0.046842\n",
      "  validation loss:\t\t0.184141\n",
      "  validation accuracy:\t\t95.50 %\n",
      "Epoch 469 of 500 took 1.659s\n",
      "  training loss:\t\t0.044578\n",
      "  validation loss:\t\t0.176671\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 470 of 500 took 1.667s\n",
      "  training loss:\t\t0.044753\n",
      "  validation loss:\t\t0.202491\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 471 of 500 took 1.649s\n",
      "  training loss:\t\t0.046751\n",
      "  validation loss:\t\t0.179818\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 472 of 500 took 1.652s\n",
      "  training loss:\t\t0.050228\n",
      "  validation loss:\t\t0.185643\n",
      "  validation accuracy:\t\t95.50 %\n",
      "Epoch 473 of 500 took 1.672s\n",
      "  training loss:\t\t0.046316\n",
      "  validation loss:\t\t0.181664\n",
      "  validation accuracy:\t\t95.60 %\n",
      "Epoch 474 of 500 took 1.660s\n",
      "  training loss:\t\t0.046509\n",
      "  validation loss:\t\t0.176774\n",
      "  validation accuracy:\t\t96.10 %\n",
      "Epoch 475 of 500 took 1.685s\n",
      "  training loss:\t\t0.046522\n",
      "  validation loss:\t\t0.204294\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 476 of 500 took 1.664s\n",
      "  training loss:\t\t0.045305\n",
      "  validation loss:\t\t0.181459\n",
      "  validation accuracy:\t\t95.50 %\n",
      "Epoch 477 of 500 took 1.659s\n",
      "  training loss:\t\t0.046630\n",
      "  validation loss:\t\t0.179217\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 478 of 500 took 1.666s\n",
      "  training loss:\t\t0.045587\n",
      "  validation loss:\t\t0.181425\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 479 of 500 took 1.690s\n",
      "  training loss:\t\t0.044521\n",
      "  validation loss:\t\t0.182317\n",
      "  validation accuracy:\t\t95.60 %\n",
      "Epoch 480 of 500 took 1.650s\n",
      "  training loss:\t\t0.048155\n",
      "  validation loss:\t\t0.186040\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 481 of 500 took 1.682s\n",
      "  training loss:\t\t0.045913\n",
      "  validation loss:\t\t0.189583\n",
      "  validation accuracy:\t\t95.40 %\n",
      "Epoch 482 of 500 took 1.679s\n",
      "  training loss:\t\t0.044466\n",
      "  validation loss:\t\t0.186776\n",
      "  validation accuracy:\t\t95.60 %\n",
      "Epoch 483 of 500 took 1.668s\n",
      "  training loss:\t\t0.040879\n",
      "  validation loss:\t\t0.206950\n",
      "  validation accuracy:\t\t95.50 %\n",
      "Epoch 484 of 500 took 1.667s\n",
      "  training loss:\t\t0.046446\n",
      "  validation loss:\t\t0.186538\n",
      "  validation accuracy:\t\t95.50 %\n",
      "Epoch 485 of 500 took 1.648s\n",
      "  training loss:\t\t0.044280\n",
      "  validation loss:\t\t0.193712\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 486 of 500 took 1.675s\n",
      "  training loss:\t\t0.048258\n",
      "  validation loss:\t\t0.187238\n",
      "  validation accuracy:\t\t95.50 %\n",
      "Epoch 487 of 500 took 1.656s\n",
      "  training loss:\t\t0.047336\n",
      "  validation loss:\t\t0.195315\n",
      "  validation accuracy:\t\t95.40 %\n",
      "Epoch 488 of 500 took 1.701s\n",
      "  training loss:\t\t0.044968\n",
      "  validation loss:\t\t0.186575\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 489 of 500 took 1.744s\n",
      "  training loss:\t\t0.046382\n",
      "  validation loss:\t\t0.176114\n",
      "  validation accuracy:\t\t96.00 %\n",
      "Epoch 490 of 500 took 1.671s\n",
      "  training loss:\t\t0.043923\n",
      "  validation loss:\t\t0.201614\n",
      "  validation accuracy:\t\t95.40 %\n",
      "Epoch 491 of 500 took 1.677s\n",
      "  training loss:\t\t0.045759\n",
      "  validation loss:\t\t0.184962\n",
      "  validation accuracy:\t\t95.50 %\n",
      "Epoch 492 of 500 took 1.654s\n",
      "  training loss:\t\t0.046495\n",
      "  validation loss:\t\t0.171132\n",
      "  validation accuracy:\t\t95.90 %\n",
      "Epoch 493 of 500 took 1.667s\n",
      "  training loss:\t\t0.044483\n",
      "  validation loss:\t\t0.186275\n",
      "  validation accuracy:\t\t95.60 %\n",
      "Epoch 494 of 500 took 1.674s\n",
      "  training loss:\t\t0.044949\n",
      "  validation loss:\t\t0.189685\n",
      "  validation accuracy:\t\t95.40 %\n",
      "Epoch 495 of 500 took 1.663s\n",
      "  training loss:\t\t0.044409\n",
      "  validation loss:\t\t0.190412\n",
      "  validation accuracy:\t\t95.60 %\n",
      "Epoch 496 of 500 took 1.651s\n",
      "  training loss:\t\t0.043133\n",
      "  validation loss:\t\t0.181454\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 497 of 500 took 1.657s\n",
      "  training loss:\t\t0.045179\n",
      "  validation loss:\t\t0.208811\n",
      "  validation accuracy:\t\t94.90 %\n",
      "Epoch 498 of 500 took 1.660s\n",
      "  training loss:\t\t0.041745\n",
      "  validation loss:\t\t0.190595\n",
      "  validation accuracy:\t\t95.40 %\n",
      "Epoch 499 of 500 took 1.668s\n",
      "  training loss:\t\t0.047711\n",
      "  validation loss:\t\t0.204972\n",
      "  validation accuracy:\t\t95.00 %\n",
      "Epoch 500 of 500 took 1.670s\n",
      "  training loss:\t\t0.044378\n",
      "  validation loss:\t\t0.193229\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Final results:\n",
      "  test loss:\t\t\t19.781387\n",
      "  test accuracy:\t\t18.98 %\n",
      "Percentages average:\t\t\t0.95650\n",
      "\u0007\n"
     ]
    }
   ],
   "source": [
    "train_nnet(num_epochs=500,depth = 2, width = 250, d_i = 0.0, d_h = 0.3, l_r = 0.4, momentum = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x111c19b10>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMKqY/x+IbtOuVev80gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113ebab10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scatter(arange(len(percs)),percs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  3.62611374e-06,   6.21268733e-13,   1.33535923e-12,\n",
       "          8.37693973e-08,   9.99996290e-01],\n",
       "       [  3.66462163e-08,   8.91376032e-10,   3.92889751e-06,\n",
       "          9.99994007e-01,   2.02695301e-06],\n",
       "       [  3.14984670e-17,   1.05778112e-17,   8.53266119e-10,\n",
       "          9.99998433e-01,   1.56663816e-06],\n",
       "       ..., \n",
       "       [  1.00000000e+00,   1.09937620e-36,   5.09375161e-28,\n",
       "          5.38054065e-37,   4.14001682e-44],\n",
       "       [  9.99998660e-01,   2.26114856e-12,   3.02439884e-12,\n",
       "          1.04537335e-09,   1.33865977e-06],\n",
       "       [  1.00601106e-08,   1.19021461e-07,   9.91759863e-01,\n",
       "          3.40355022e-07,   8.23966723e-03]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final = result.eval()\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(final[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_output = list([])\n",
    "for i in arange(len(final)):\n",
    "    my_list = final[i,:]\n",
    "    max_index = my_list.argmax()\n",
    "    y_output.append(max_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8137"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " ...]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('result2.csv', 'wb') as fp:\n",
    "    a = csv.writer(fp, delimiter=',')\n",
    "    a.writerow(('Id','y'))\n",
    "    a.writerows(zip(ids,y_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8137, 1, 10, 10)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_c = X_train.reshape(len(X_train),1,10,10)\n",
    "X_val_c = X_val.reshape(len(X_val),1,10,10)\n",
    "X_test_c = X_test.reshape(len(X_test),1,10,10) \n",
    "X_test_c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_cnn(input_var=None):\n",
    "    # As a third model, we'll create a CNN of two convolution + pooling stages\n",
    "    # and a fully-connected hidden layer in front of the output layer.\n",
    "\n",
    "    # Input layer, as usual:\n",
    "    network = lasagne.layers.InputLayer(shape=(None, 1, 10, 10),\n",
    "                                        input_var=input_var)\n",
    "    # This time we do not apply input dropout, as it tends to work less well\n",
    "    # for convolutional layers.\n",
    "\n",
    "    # Convolutional layer with 32 kernels of size 5x5. Strided and padded\n",
    "    # convolutions are supported as well; see the docstring.\n",
    "    network = lasagne.layers.Conv2DLayer(\n",
    "            network, num_filters=32, filter_size=(3, 3),\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform())\n",
    "    # Expert note: Lasagne provides alternative convolutional layers that\n",
    "    # override Theano's choice of which implementation to use; for details\n",
    "    # please see http://lasagne.readthedocs.org/en/latest/user/tutorial.html.\n",
    "\n",
    "    # Max-pooling layer of factor 2 in both dimensions:\n",
    "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2))\n",
    "\n",
    "    # Another convolution with 32 5x5 kernels, and another 2x2 pooling:\n",
    "    network = lasagne.layers.Conv2DLayer(\n",
    "            network, num_filters=32, filter_size=(3, 3),\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2))\n",
    "\n",
    "    # A fully-connected layer of 256 units with 50% dropout on its inputs:\n",
    "    network = lasagne.layers.DenseLayer(\n",
    "            lasagne.layers.dropout(network, p=.4),\n",
    "            num_units=256,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "\n",
    "    # And, finally, the 10-unit output layer with 50% dropout on its inputs:\n",
    "    network = lasagne.layers.DenseLayer(\n",
    "            lasagne.layers.dropout(network, p=.4),\n",
    "            num_units=5,\n",
    "            nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_cnn(num_epochs=500):\n",
    "    # Load the dataset\n",
    "    # print(\"Loading data...\")\n",
    "    # X_train, y_train, X_val, y_val, X_test, y_test = load_dataset()\n",
    "    \n",
    "    # theano.config.optimizer='fast_compile'\n",
    "    # theano.config.exception_verbosity='high'\n",
    "\n",
    "    # Prepare Theano variables for inputs and targets\n",
    "    input_var = T.tensor4('inputs')\n",
    "    target_var = T.ivector('targets')\n",
    "\n",
    "    # Create neural network model (depending on first command line parameter)\n",
    "    print(\"Building model and compiling functions...\")\n",
    "    \n",
    "    network = build_cnn(input_var=input_var)\n",
    "\n",
    "    # Create a loss expression for training, i.e., a scalar objective we want\n",
    "    # to minimize (for our multi-class problem, it is the cross-entropy loss):\n",
    "    prediction = lasagne.layers.get_output(network)\n",
    "    loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "    loss = loss.mean()\n",
    "    # We could add some weight decay as well here, see lasagne.regularization.\n",
    "\n",
    "    # Create update expressions for training, i.e., how to modify the\n",
    "    # parameters at each training step. Here, we'll use Stochastic Gradient\n",
    "    # Descent (SGD) with Nesterov momentum, but Lasagne offers plenty more.\n",
    "    params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "    updates = lasagne.updates.nesterov_momentum(loss, params, learning_rate=0.006, momentum=0.8)\n",
    "\n",
    "    # Create a loss expression for validation/testing. The crucial difference\n",
    "    # here is that we do a deterministic forward pass through the network,\n",
    "    # disabling dropout layers.\n",
    "    test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "    test_loss = lasagne.objectives.categorical_crossentropy(test_prediction,target_var)\n",
    "    test_loss = test_loss.mean()\n",
    "    \n",
    "    # As a bonus, also create an expression for the classification accuracy:\n",
    "    test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),dtype=theano.config.floatX)\n",
    "\n",
    "    # Compile a function performing a training step on a mini-batch (by giving\n",
    "    # the updates dictionary) and returning the corresponding training loss:\n",
    "    train_fn = theano.function([input_var, target_var], loss, updates=updates, allow_input_downcast=True)\n",
    "\n",
    "    # Compile a second function computing the validation loss and accuracy:\n",
    "    val_fn = theano.function([input_var, target_var], [test_loss, test_acc], allow_input_downcast=True)\n",
    "\n",
    "    # Finally, launch the training loop.\n",
    "    print(\"Starting training...\")\n",
    "    # We iterate over epochs:\n",
    "    global percs2\n",
    "    percs2 = array([])\n",
    "    for epoch in range(num_epochs):\n",
    "        # In each epoch, we do a full pass over the training data:\n",
    "        train_err = 0\n",
    "        train_batches = 0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for batch in iterate_minibatches(X_train_c, y_train, 500, shuffle=True):\n",
    "            inputs, targets = batch\n",
    "            train_err += train_fn(inputs, targets)\n",
    "            train_batches += 1\n",
    "\n",
    "        # And a full pass over the validation data:\n",
    "        val_err = 0\n",
    "        val_acc = 0\n",
    "        val_batches = 0\n",
    "        for batch in iterate_minibatches(X_val_c, y_val, 500, shuffle=False):\n",
    "            inputs, targets = batch\n",
    "            err, acc = val_fn(inputs, targets)\n",
    "            val_err += err\n",
    "            val_acc += acc\n",
    "            val_batches += 1\n",
    "\n",
    "        # Then we print the results for this epoch:\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "            epoch + 1, num_epochs, time.time() - start_time))\n",
    "        print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "        print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "        perc = val_acc/val_batches\n",
    "        percs2 = append(percs2,perc)\n",
    "        print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "            perc * 100))\n",
    "\n",
    "    # After training, we compute and print the test error:\n",
    "    test_err = 0\n",
    "    test_acc = 0\n",
    "    test_batches = 0\n",
    "    for batch in iterate_minibatches(X_test_c, zeros(len(X_test_c)), 500, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        err, acc = val_fn(inputs, targets)\n",
    "        test_err += err\n",
    "        test_acc += acc\n",
    "        test_batches += 1\n",
    "    print(\"Final results:\")\n",
    "    print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "    print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "        test_acc / test_batches * 100))\n",
    "    global result2\n",
    "    result2 = lasagne.layers.get_output(network, X_test_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model and compiling functions...\n",
      "Starting training...\n",
      "Epoch 1 of 200 took 8.104s\n",
      "  training loss:\t\t1.605329\n",
      "  validation loss:\t\t1.595370\n",
      "  validation accuracy:\t\t35.10 %\n",
      "Epoch 2 of 200 took 9.128s\n",
      "  training loss:\t\t1.594809\n",
      "  validation loss:\t\t1.580480\n",
      "  validation accuracy:\t\t37.00 %\n",
      "Epoch 3 of 200 took 8.630s\n",
      "  training loss:\t\t1.579551\n",
      "  validation loss:\t\t1.556551\n",
      "  validation accuracy:\t\t43.30 %\n",
      "Epoch 4 of 200 took 8.000s\n",
      "  training loss:\t\t1.555958\n",
      "  validation loss:\t\t1.517005\n",
      "  validation accuracy:\t\t48.60 %\n",
      "Epoch 5 of 200 took 8.021s\n",
      "  training loss:\t\t1.515565\n",
      "  validation loss:\t\t1.453244\n",
      "  validation accuracy:\t\t54.20 %\n",
      "Epoch 6 of 200 took 8.042s\n",
      "  training loss:\t\t1.454881\n",
      "  validation loss:\t\t1.359869\n",
      "  validation accuracy:\t\t55.50 %\n",
      "Epoch 7 of 200 took 8.086s\n",
      "  training loss:\t\t1.373174\n",
      "  validation loss:\t\t1.246869\n",
      "  validation accuracy:\t\t60.30 %\n",
      "Epoch 8 of 200 took 8.039s\n",
      "  training loss:\t\t1.280525\n",
      "  validation loss:\t\t1.128536\n",
      "  validation accuracy:\t\t64.50 %\n",
      "Epoch 9 of 200 took 8.032s\n",
      "  training loss:\t\t1.189100\n",
      "  validation loss:\t\t1.020004\n",
      "  validation accuracy:\t\t67.20 %\n",
      "Epoch 10 of 200 took 8.050s\n",
      "  training loss:\t\t1.108413\n",
      "  validation loss:\t\t0.928469\n",
      "  validation accuracy:\t\t69.40 %\n",
      "Epoch 11 of 200 took 7.975s\n",
      "  training loss:\t\t1.036004\n",
      "  validation loss:\t\t0.850083\n",
      "  validation accuracy:\t\t70.30 %\n",
      "Epoch 12 of 200 took 7.941s\n",
      "  training loss:\t\t0.978064\n",
      "  validation loss:\t\t0.793585\n",
      "  validation accuracy:\t\t73.30 %\n",
      "Epoch 13 of 200 took 8.010s\n",
      "  training loss:\t\t0.926503\n",
      "  validation loss:\t\t0.746796\n",
      "  validation accuracy:\t\t75.30 %\n",
      "Epoch 14 of 200 took 7.921s\n",
      "  training loss:\t\t0.884514\n",
      "  validation loss:\t\t0.706123\n",
      "  validation accuracy:\t\t76.40 %\n",
      "Epoch 15 of 200 took 7.937s\n",
      "  training loss:\t\t0.847506\n",
      "  validation loss:\t\t0.669086\n",
      "  validation accuracy:\t\t77.10 %\n",
      "Epoch 16 of 200 took 8.336s\n",
      "  training loss:\t\t0.817789\n",
      "  validation loss:\t\t0.637556\n",
      "  validation accuracy:\t\t78.00 %\n",
      "Epoch 17 of 200 took 8.127s\n",
      "  training loss:\t\t0.792047\n",
      "  validation loss:\t\t0.616793\n",
      "  validation accuracy:\t\t78.50 %\n",
      "Epoch 18 of 200 took 7.950s\n",
      "  training loss:\t\t0.765281\n",
      "  validation loss:\t\t0.597154\n",
      "  validation accuracy:\t\t79.70 %\n",
      "Epoch 19 of 200 took 8.078s\n",
      "  training loss:\t\t0.743829\n",
      "  validation loss:\t\t0.574206\n",
      "  validation accuracy:\t\t80.30 %\n",
      "Epoch 20 of 200 took 7.995s\n",
      "  training loss:\t\t0.725031\n",
      "  validation loss:\t\t0.555656\n",
      "  validation accuracy:\t\t80.70 %\n",
      "Epoch 21 of 200 took 8.018s\n",
      "  training loss:\t\t0.710565\n",
      "  validation loss:\t\t0.545193\n",
      "  validation accuracy:\t\t80.70 %\n",
      "Epoch 22 of 200 took 8.038s\n",
      "  training loss:\t\t0.693923\n",
      "  validation loss:\t\t0.525192\n",
      "  validation accuracy:\t\t82.00 %\n",
      "Epoch 23 of 200 took 8.002s\n",
      "  training loss:\t\t0.682002\n",
      "  validation loss:\t\t0.514015\n",
      "  validation accuracy:\t\t82.80 %\n",
      "Epoch 24 of 200 took 8.184s\n",
      "  training loss:\t\t0.668003\n",
      "  validation loss:\t\t0.496519\n",
      "  validation accuracy:\t\t83.30 %\n",
      "Epoch 25 of 200 took 8.036s\n",
      "  training loss:\t\t0.653778\n",
      "  validation loss:\t\t0.490878\n",
      "  validation accuracy:\t\t83.30 %\n",
      "Epoch 26 of 200 took 7.977s\n",
      "  training loss:\t\t0.645650\n",
      "  validation loss:\t\t0.483085\n",
      "  validation accuracy:\t\t83.70 %\n",
      "Epoch 27 of 200 took 7.955s\n",
      "  training loss:\t\t0.631375\n",
      "  validation loss:\t\t0.472321\n",
      "  validation accuracy:\t\t83.50 %\n",
      "Epoch 28 of 200 took 8.017s\n",
      "  training loss:\t\t0.624419\n",
      "  validation loss:\t\t0.459791\n",
      "  validation accuracy:\t\t84.20 %\n",
      "Epoch 29 of 200 took 8.039s\n",
      "  training loss:\t\t0.614381\n",
      "  validation loss:\t\t0.445606\n",
      "  validation accuracy:\t\t84.40 %\n",
      "Epoch 30 of 200 took 7.956s\n",
      "  training loss:\t\t0.606311\n",
      "  validation loss:\t\t0.448835\n",
      "  validation accuracy:\t\t84.50 %\n",
      "Epoch 31 of 200 took 7.943s\n",
      "  training loss:\t\t0.598357\n",
      "  validation loss:\t\t0.437211\n",
      "  validation accuracy:\t\t84.70 %\n",
      "Epoch 32 of 200 took 7.997s\n",
      "  training loss:\t\t0.589148\n",
      "  validation loss:\t\t0.425980\n",
      "  validation accuracy:\t\t85.40 %\n",
      "Epoch 33 of 200 took 7.959s\n",
      "  training loss:\t\t0.583763\n",
      "  validation loss:\t\t0.423346\n",
      "  validation accuracy:\t\t85.00 %\n",
      "Epoch 34 of 200 took 7.957s\n",
      "  training loss:\t\t0.579154\n",
      "  validation loss:\t\t0.425024\n",
      "  validation accuracy:\t\t85.40 %\n",
      "Epoch 35 of 200 took 7.934s\n",
      "  training loss:\t\t0.569498\n",
      "  validation loss:\t\t0.408434\n",
      "  validation accuracy:\t\t86.10 %\n",
      "Epoch 36 of 200 took 7.938s\n",
      "  training loss:\t\t0.560156\n",
      "  validation loss:\t\t0.409150\n",
      "  validation accuracy:\t\t86.30 %\n",
      "Epoch 37 of 200 took 7.918s\n",
      "  training loss:\t\t0.558497\n",
      "  validation loss:\t\t0.406031\n",
      "  validation accuracy:\t\t85.80 %\n",
      "Epoch 38 of 200 took 7.966s\n",
      "  training loss:\t\t0.555204\n",
      "  validation loss:\t\t0.397139\n",
      "  validation accuracy:\t\t86.40 %\n",
      "Epoch 39 of 200 took 7.991s\n",
      "  training loss:\t\t0.547951\n",
      "  validation loss:\t\t0.390187\n",
      "  validation accuracy:\t\t86.80 %\n",
      "Epoch 40 of 200 took 7.951s\n",
      "  training loss:\t\t0.542175\n",
      "  validation loss:\t\t0.385944\n",
      "  validation accuracy:\t\t86.40 %\n",
      "Epoch 41 of 200 took 7.924s\n",
      "  training loss:\t\t0.540718\n",
      "  validation loss:\t\t0.387813\n",
      "  validation accuracy:\t\t86.70 %\n",
      "Epoch 42 of 200 took 7.951s\n",
      "  training loss:\t\t0.533506\n",
      "  validation loss:\t\t0.378829\n",
      "  validation accuracy:\t\t87.00 %\n",
      "Epoch 43 of 200 took 7.897s\n",
      "  training loss:\t\t0.530009\n",
      "  validation loss:\t\t0.370366\n",
      "  validation accuracy:\t\t87.50 %\n",
      "Epoch 44 of 200 took 7.952s\n",
      "  training loss:\t\t0.528555\n",
      "  validation loss:\t\t0.375201\n",
      "  validation accuracy:\t\t87.10 %\n",
      "Epoch 45 of 200 took 7.939s\n",
      "  training loss:\t\t0.519818\n",
      "  validation loss:\t\t0.365400\n",
      "  validation accuracy:\t\t87.00 %\n",
      "Epoch 46 of 200 took 7.907s\n",
      "  training loss:\t\t0.515669\n",
      "  validation loss:\t\t0.369088\n",
      "  validation accuracy:\t\t87.90 %\n",
      "Epoch 47 of 200 took 7.969s\n",
      "  training loss:\t\t0.513160\n",
      "  validation loss:\t\t0.364433\n",
      "  validation accuracy:\t\t87.80 %\n",
      "Epoch 48 of 200 took 8.010s\n",
      "  training loss:\t\t0.515571\n",
      "  validation loss:\t\t0.358857\n",
      "  validation accuracy:\t\t88.30 %\n",
      "Epoch 49 of 200 took 7.958s\n",
      "  training loss:\t\t0.506227\n",
      "  validation loss:\t\t0.356973\n",
      "  validation accuracy:\t\t87.60 %\n",
      "Epoch 50 of 200 took 8.028s\n",
      "  training loss:\t\t0.504404\n",
      "  validation loss:\t\t0.356055\n",
      "  validation accuracy:\t\t87.80 %\n",
      "Epoch 51 of 200 took 8.618s\n",
      "  training loss:\t\t0.499874\n",
      "  validation loss:\t\t0.349953\n",
      "  validation accuracy:\t\t88.40 %\n",
      "Epoch 52 of 200 took 8.004s\n",
      "  training loss:\t\t0.494682\n",
      "  validation loss:\t\t0.345837\n",
      "  validation accuracy:\t\t88.80 %\n",
      "Epoch 53 of 200 took 7.863s\n",
      "  training loss:\t\t0.493410\n",
      "  validation loss:\t\t0.341899\n",
      "  validation accuracy:\t\t88.50 %\n",
      "Epoch 54 of 200 took 8.019s\n",
      "  training loss:\t\t0.488507\n",
      "  validation loss:\t\t0.348550\n",
      "  validation accuracy:\t\t88.30 %\n",
      "Epoch 55 of 200 took 7.960s\n",
      "  training loss:\t\t0.489131\n",
      "  validation loss:\t\t0.345452\n",
      "  validation accuracy:\t\t88.20 %\n",
      "Epoch 56 of 200 took 10.684s\n",
      "  training loss:\t\t0.481225\n",
      "  validation loss:\t\t0.336836\n",
      "  validation accuracy:\t\t88.80 %\n",
      "Epoch 57 of 200 took 8.746s\n",
      "  training loss:\t\t0.484926\n",
      "  validation loss:\t\t0.345319\n",
      "  validation accuracy:\t\t88.60 %\n",
      "Epoch 58 of 200 took 7.845s\n",
      "  training loss:\t\t0.481040\n",
      "  validation loss:\t\t0.335683\n",
      "  validation accuracy:\t\t89.20 %\n",
      "Epoch 59 of 200 took 8.102s\n",
      "  training loss:\t\t0.476712\n",
      "  validation loss:\t\t0.332169\n",
      "  validation accuracy:\t\t88.90 %\n",
      "Epoch 60 of 200 took 8.064s\n",
      "  training loss:\t\t0.471323\n",
      "  validation loss:\t\t0.332087\n",
      "  validation accuracy:\t\t89.00 %\n",
      "Epoch 61 of 200 took 8.069s\n",
      "  training loss:\t\t0.471573\n",
      "  validation loss:\t\t0.335707\n",
      "  validation accuracy:\t\t89.00 %\n",
      "Epoch 62 of 200 took 7.898s\n",
      "  training loss:\t\t0.473301\n",
      "  validation loss:\t\t0.325137\n",
      "  validation accuracy:\t\t89.30 %\n",
      "Epoch 63 of 200 took 7.912s\n",
      "  training loss:\t\t0.464989\n",
      "  validation loss:\t\t0.327623\n",
      "  validation accuracy:\t\t89.10 %\n",
      "Epoch 64 of 200 took 8.869s\n",
      "  training loss:\t\t0.469914\n",
      "  validation loss:\t\t0.323339\n",
      "  validation accuracy:\t\t88.90 %\n",
      "Epoch 65 of 200 took 8.485s\n",
      "  training loss:\t\t0.459673\n",
      "  validation loss:\t\t0.323972\n",
      "  validation accuracy:\t\t89.10 %\n",
      "Epoch 66 of 200 took 8.137s\n",
      "  training loss:\t\t0.454176\n",
      "  validation loss:\t\t0.326826\n",
      "  validation accuracy:\t\t89.50 %\n",
      "Epoch 67 of 200 took 7.930s\n",
      "  training loss:\t\t0.458821\n",
      "  validation loss:\t\t0.318497\n",
      "  validation accuracy:\t\t88.60 %\n",
      "Epoch 68 of 200 took 7.906s\n",
      "  training loss:\t\t0.456204\n",
      "  validation loss:\t\t0.317079\n",
      "  validation accuracy:\t\t89.70 %\n",
      "Epoch 69 of 200 took 7.898s\n",
      "  training loss:\t\t0.456166\n",
      "  validation loss:\t\t0.318658\n",
      "  validation accuracy:\t\t89.60 %\n",
      "Epoch 70 of 200 took 7.840s\n",
      "  training loss:\t\t0.452490\n",
      "  validation loss:\t\t0.315842\n",
      "  validation accuracy:\t\t89.90 %\n",
      "Epoch 71 of 200 took 7.880s\n",
      "  training loss:\t\t0.450356\n",
      "  validation loss:\t\t0.318749\n",
      "  validation accuracy:\t\t88.90 %\n",
      "Epoch 72 of 200 took 7.925s\n",
      "  training loss:\t\t0.449792\n",
      "  validation loss:\t\t0.313747\n",
      "  validation accuracy:\t\t89.60 %\n",
      "Epoch 73 of 200 took 7.929s\n",
      "  training loss:\t\t0.446290\n",
      "  validation loss:\t\t0.307391\n",
      "  validation accuracy:\t\t90.20 %\n",
      "Epoch 74 of 200 took 8.073s\n",
      "  training loss:\t\t0.444232\n",
      "  validation loss:\t\t0.308841\n",
      "  validation accuracy:\t\t90.40 %\n",
      "Epoch 75 of 200 took 8.673s\n",
      "  training loss:\t\t0.444730\n",
      "  validation loss:\t\t0.309573\n",
      "  validation accuracy:\t\t89.20 %\n",
      "Epoch 76 of 200 took 8.645s\n",
      "  training loss:\t\t0.443216\n",
      "  validation loss:\t\t0.311829\n",
      "  validation accuracy:\t\t89.50 %\n",
      "Epoch 77 of 200 took 8.070s\n",
      "  training loss:\t\t0.442740\n",
      "  validation loss:\t\t0.306490\n",
      "  validation accuracy:\t\t90.10 %\n",
      "Epoch 78 of 200 took 7.958s\n",
      "  training loss:\t\t0.435878\n",
      "  validation loss:\t\t0.310933\n",
      "  validation accuracy:\t\t90.10 %\n",
      "Epoch 79 of 200 took 7.929s\n",
      "  training loss:\t\t0.436114\n",
      "  validation loss:\t\t0.307907\n",
      "  validation accuracy:\t\t89.60 %\n",
      "Epoch 80 of 200 took 7.962s\n",
      "  training loss:\t\t0.435992\n",
      "  validation loss:\t\t0.311693\n",
      "  validation accuracy:\t\t89.30 %\n",
      "Epoch 81 of 200 took 7.985s\n",
      "  training loss:\t\t0.432056\n",
      "  validation loss:\t\t0.302602\n",
      "  validation accuracy:\t\t89.70 %\n",
      "Epoch 82 of 200 took 7.975s\n",
      "  training loss:\t\t0.435950\n",
      "  validation loss:\t\t0.312273\n",
      "  validation accuracy:\t\t89.90 %\n",
      "Epoch 83 of 200 took 8.034s\n",
      "  training loss:\t\t0.428631\n",
      "  validation loss:\t\t0.308991\n",
      "  validation accuracy:\t\t89.50 %\n",
      "Epoch 84 of 200 took 7.977s\n",
      "  training loss:\t\t0.430015\n",
      "  validation loss:\t\t0.302660\n",
      "  validation accuracy:\t\t89.80 %\n",
      "Epoch 85 of 200 took 10.256s\n",
      "  training loss:\t\t0.423393\n",
      "  validation loss:\t\t0.304090\n",
      "  validation accuracy:\t\t89.70 %\n",
      "Epoch 86 of 200 took 9.775s\n",
      "  training loss:\t\t0.423529\n",
      "  validation loss:\t\t0.297681\n",
      "  validation accuracy:\t\t89.60 %\n",
      "Epoch 87 of 200 took 13.052s\n",
      "  training loss:\t\t0.423361\n",
      "  validation loss:\t\t0.297215\n",
      "  validation accuracy:\t\t90.00 %\n",
      "Epoch 88 of 200 took 8.678s\n",
      "  training loss:\t\t0.422578\n",
      "  validation loss:\t\t0.305163\n",
      "  validation accuracy:\t\t89.30 %\n",
      "Epoch 89 of 200 took 9.103s\n",
      "  training loss:\t\t0.420160\n",
      "  validation loss:\t\t0.301024\n",
      "  validation accuracy:\t\t89.90 %\n",
      "Epoch 90 of 200 took 9.044s\n",
      "  training loss:\t\t0.420225\n",
      "  validation loss:\t\t0.295890\n",
      "  validation accuracy:\t\t89.80 %\n",
      "Epoch 91 of 200 took 8.975s\n",
      "  training loss:\t\t0.418703\n",
      "  validation loss:\t\t0.292425\n",
      "  validation accuracy:\t\t90.30 %\n",
      "Epoch 92 of 200 took 7.897s\n",
      "  training loss:\t\t0.415662\n",
      "  validation loss:\t\t0.299264\n",
      "  validation accuracy:\t\t89.80 %\n",
      "Epoch 93 of 200 took 7.883s\n",
      "  training loss:\t\t0.414389\n",
      "  validation loss:\t\t0.288420\n",
      "  validation accuracy:\t\t90.60 %\n",
      "Epoch 94 of 200 took 7.934s\n",
      "  training loss:\t\t0.413478\n",
      "  validation loss:\t\t0.291816\n",
      "  validation accuracy:\t\t90.30 %\n",
      "Epoch 95 of 200 took 7.985s\n",
      "  training loss:\t\t0.414027\n",
      "  validation loss:\t\t0.291261\n",
      "  validation accuracy:\t\t90.70 %\n",
      "Epoch 96 of 200 took 8.235s\n",
      "  training loss:\t\t0.414219\n",
      "  validation loss:\t\t0.291490\n",
      "  validation accuracy:\t\t89.90 %\n",
      "Epoch 97 of 200 took 8.989s\n",
      "  training loss:\t\t0.410072\n",
      "  validation loss:\t\t0.289019\n",
      "  validation accuracy:\t\t90.10 %\n",
      "Epoch 98 of 200 took 9.308s\n",
      "  training loss:\t\t0.413978\n",
      "  validation loss:\t\t0.291146\n",
      "  validation accuracy:\t\t90.40 %\n",
      "Epoch 99 of 200 took 8.325s\n",
      "  training loss:\t\t0.409727\n",
      "  validation loss:\t\t0.285397\n",
      "  validation accuracy:\t\t90.30 %\n",
      "Epoch 100 of 200 took 7.774s\n",
      "  training loss:\t\t0.410346\n",
      "  validation loss:\t\t0.285334\n",
      "  validation accuracy:\t\t90.10 %\n",
      "Epoch 101 of 200 took 7.830s\n",
      "  training loss:\t\t0.402234\n",
      "  validation loss:\t\t0.289184\n",
      "  validation accuracy:\t\t90.80 %\n",
      "Epoch 102 of 200 took 7.844s\n",
      "  training loss:\t\t0.399734\n",
      "  validation loss:\t\t0.290944\n",
      "  validation accuracy:\t\t90.50 %\n",
      "Epoch 103 of 200 took 7.824s\n",
      "  training loss:\t\t0.405024\n",
      "  validation loss:\t\t0.287338\n",
      "  validation accuracy:\t\t90.20 %\n",
      "Epoch 104 of 200 took 7.781s\n",
      "  training loss:\t\t0.402059\n",
      "  validation loss:\t\t0.280184\n",
      "  validation accuracy:\t\t90.50 %\n",
      "Epoch 105 of 200 took 7.797s\n",
      "  training loss:\t\t0.400204\n",
      "  validation loss:\t\t0.286330\n",
      "  validation accuracy:\t\t90.40 %\n",
      "Epoch 106 of 200 took 7.853s\n",
      "  training loss:\t\t0.399984\n",
      "  validation loss:\t\t0.280068\n",
      "  validation accuracy:\t\t90.20 %\n",
      "Epoch 107 of 200 took 7.879s\n",
      "  training loss:\t\t0.400507\n",
      "  validation loss:\t\t0.287612\n",
      "  validation accuracy:\t\t89.80 %\n",
      "Epoch 108 of 200 took 7.944s\n",
      "  training loss:\t\t0.400491\n",
      "  validation loss:\t\t0.285328\n",
      "  validation accuracy:\t\t90.40 %\n",
      "Epoch 109 of 200 took 7.792s\n",
      "  training loss:\t\t0.397056\n",
      "  validation loss:\t\t0.283951\n",
      "  validation accuracy:\t\t90.70 %\n",
      "Epoch 110 of 200 took 7.842s\n",
      "  training loss:\t\t0.399696\n",
      "  validation loss:\t\t0.277493\n",
      "  validation accuracy:\t\t90.70 %\n",
      "Epoch 111 of 200 took 7.868s\n",
      "  training loss:\t\t0.396731\n",
      "  validation loss:\t\t0.280307\n",
      "  validation accuracy:\t\t90.30 %\n",
      "Epoch 112 of 200 took 7.860s\n",
      "  training loss:\t\t0.398557\n",
      "  validation loss:\t\t0.285331\n",
      "  validation accuracy:\t\t90.40 %\n",
      "Epoch 113 of 200 took 7.838s\n",
      "  training loss:\t\t0.393951\n",
      "  validation loss:\t\t0.277628\n",
      "  validation accuracy:\t\t90.80 %\n",
      "Epoch 114 of 200 took 8.198s\n",
      "  training loss:\t\t0.391325\n",
      "  validation loss:\t\t0.288298\n",
      "  validation accuracy:\t\t90.40 %\n",
      "Epoch 115 of 200 took 7.816s\n",
      "  training loss:\t\t0.392734\n",
      "  validation loss:\t\t0.281465\n",
      "  validation accuracy:\t\t90.50 %\n",
      "Epoch 116 of 200 took 7.836s\n",
      "  training loss:\t\t0.390734\n",
      "  validation loss:\t\t0.284017\n",
      "  validation accuracy:\t\t90.60 %\n",
      "Epoch 117 of 200 took 7.861s\n",
      "  training loss:\t\t0.389699\n",
      "  validation loss:\t\t0.290350\n",
      "  validation accuracy:\t\t90.00 %\n",
      "Epoch 118 of 200 took 9.099s\n",
      "  training loss:\t\t0.385452\n",
      "  validation loss:\t\t0.280311\n",
      "  validation accuracy:\t\t90.70 %\n",
      "Epoch 119 of 200 took 7.844s\n",
      "  training loss:\t\t0.390298\n",
      "  validation loss:\t\t0.276604\n",
      "  validation accuracy:\t\t90.90 %\n",
      "Epoch 120 of 200 took 7.820s\n",
      "  training loss:\t\t0.385493\n",
      "  validation loss:\t\t0.289027\n",
      "  validation accuracy:\t\t90.40 %\n",
      "Epoch 121 of 200 took 7.842s\n",
      "  training loss:\t\t0.389603\n",
      "  validation loss:\t\t0.279520\n",
      "  validation accuracy:\t\t90.80 %\n",
      "Epoch 122 of 200 took 7.883s\n",
      "  training loss:\t\t0.386605\n",
      "  validation loss:\t\t0.285030\n",
      "  validation accuracy:\t\t90.90 %\n",
      "Epoch 123 of 200 took 7.843s\n",
      "  training loss:\t\t0.384422\n",
      "  validation loss:\t\t0.282714\n",
      "  validation accuracy:\t\t90.60 %\n",
      "Epoch 124 of 200 took 7.571s\n",
      "  training loss:\t\t0.387431\n",
      "  validation loss:\t\t0.283115\n",
      "  validation accuracy:\t\t90.50 %\n",
      "Epoch 125 of 200 took 7.510s\n",
      "  training loss:\t\t0.384152\n",
      "  validation loss:\t\t0.278530\n",
      "  validation accuracy:\t\t90.60 %\n",
      "Epoch 126 of 200 took 7.580s\n",
      "  training loss:\t\t0.384076\n",
      "  validation loss:\t\t0.274406\n",
      "  validation accuracy:\t\t90.90 %\n",
      "Epoch 127 of 200 took 7.206s\n",
      "  training loss:\t\t0.383213\n",
      "  validation loss:\t\t0.273568\n",
      "  validation accuracy:\t\t90.80 %\n",
      "Epoch 128 of 200 took 7.168s\n",
      "  training loss:\t\t0.375583\n",
      "  validation loss:\t\t0.273834\n",
      "  validation accuracy:\t\t90.70 %\n",
      "Epoch 129 of 200 took 7.385s\n",
      "  training loss:\t\t0.381077\n",
      "  validation loss:\t\t0.280789\n",
      "  validation accuracy:\t\t90.50 %\n",
      "Epoch 130 of 200 took 7.468s\n",
      "  training loss:\t\t0.379621\n",
      "  validation loss:\t\t0.274906\n",
      "  validation accuracy:\t\t90.70 %\n",
      "Epoch 131 of 200 took 7.489s\n",
      "  training loss:\t\t0.378704\n",
      "  validation loss:\t\t0.270656\n",
      "  validation accuracy:\t\t91.60 %\n",
      "Epoch 132 of 200 took 7.485s\n",
      "  training loss:\t\t0.375595\n",
      "  validation loss:\t\t0.279986\n",
      "  validation accuracy:\t\t90.00 %\n",
      "Epoch 133 of 200 took 7.427s\n",
      "  training loss:\t\t0.378003\n",
      "  validation loss:\t\t0.273198\n",
      "  validation accuracy:\t\t91.40 %\n",
      "Epoch 134 of 200 took 7.464s\n",
      "  training loss:\t\t0.375492\n",
      "  validation loss:\t\t0.274035\n",
      "  validation accuracy:\t\t91.40 %\n",
      "Epoch 135 of 200 took 7.446s\n",
      "  training loss:\t\t0.376623\n",
      "  validation loss:\t\t0.274406\n",
      "  validation accuracy:\t\t91.00 %\n",
      "Epoch 136 of 200 took 7.485s\n",
      "  training loss:\t\t0.377876\n",
      "  validation loss:\t\t0.269615\n",
      "  validation accuracy:\t\t91.60 %\n",
      "Epoch 137 of 200 took 7.433s\n",
      "  training loss:\t\t0.376604\n",
      "  validation loss:\t\t0.278827\n",
      "  validation accuracy:\t\t90.60 %\n",
      "Epoch 138 of 200 took 7.457s\n",
      "  training loss:\t\t0.371555\n",
      "  validation loss:\t\t0.269520\n",
      "  validation accuracy:\t\t91.00 %\n",
      "Epoch 139 of 200 took 7.536s\n",
      "  training loss:\t\t0.372177\n",
      "  validation loss:\t\t0.268478\n",
      "  validation accuracy:\t\t91.10 %\n",
      "Epoch 140 of 200 took 7.580s\n",
      "  training loss:\t\t0.370374\n",
      "  validation loss:\t\t0.271552\n",
      "  validation accuracy:\t\t90.50 %\n",
      "Epoch 141 of 200 took 7.530s\n",
      "  training loss:\t\t0.367346\n",
      "  validation loss:\t\t0.267723\n",
      "  validation accuracy:\t\t91.00 %\n",
      "Epoch 142 of 200 took 7.596s\n",
      "  training loss:\t\t0.371943\n",
      "  validation loss:\t\t0.268235\n",
      "  validation accuracy:\t\t91.20 %\n",
      "Epoch 143 of 200 took 7.544s\n",
      "  training loss:\t\t0.372114\n",
      "  validation loss:\t\t0.271652\n",
      "  validation accuracy:\t\t90.70 %\n",
      "Epoch 144 of 200 took 7.578s\n",
      "  training loss:\t\t0.368002\n",
      "  validation loss:\t\t0.268095\n",
      "  validation accuracy:\t\t91.00 %\n",
      "Epoch 145 of 200 took 7.549s\n",
      "  training loss:\t\t0.370098\n",
      "  validation loss:\t\t0.269779\n",
      "  validation accuracy:\t\t90.80 %\n",
      "Epoch 146 of 200 took 7.505s\n",
      "  training loss:\t\t0.370526\n",
      "  validation loss:\t\t0.277641\n",
      "  validation accuracy:\t\t90.70 %\n",
      "Epoch 147 of 200 took 7.539s\n",
      "  training loss:\t\t0.370919\n",
      "  validation loss:\t\t0.271221\n",
      "  validation accuracy:\t\t90.80 %\n",
      "Epoch 148 of 200 took 7.503s\n",
      "  training loss:\t\t0.365335\n",
      "  validation loss:\t\t0.267682\n",
      "  validation accuracy:\t\t90.90 %\n",
      "Epoch 149 of 200 took 7.526s\n",
      "  training loss:\t\t0.363172\n",
      "  validation loss:\t\t0.269066\n",
      "  validation accuracy:\t\t91.10 %\n",
      "Epoch 150 of 200 took 7.551s\n",
      "  training loss:\t\t0.361934\n",
      "  validation loss:\t\t0.272134\n",
      "  validation accuracy:\t\t91.20 %\n",
      "Epoch 151 of 200 took 7.658s\n",
      "  training loss:\t\t0.366020\n",
      "  validation loss:\t\t0.269808\n",
      "  validation accuracy:\t\t90.80 %\n",
      "Epoch 152 of 200 took 7.675s\n",
      "  training loss:\t\t0.364365\n",
      "  validation loss:\t\t0.268318\n",
      "  validation accuracy:\t\t90.70 %\n",
      "Epoch 153 of 200 took 8.065s\n",
      "  training loss:\t\t0.365281\n",
      "  validation loss:\t\t0.260499\n",
      "  validation accuracy:\t\t91.30 %\n",
      "Epoch 154 of 200 took 8.847s\n",
      "  training loss:\t\t0.364552\n",
      "  validation loss:\t\t0.261566\n",
      "  validation accuracy:\t\t91.60 %\n",
      "Epoch 155 of 200 took 8.667s\n",
      "  training loss:\t\t0.364993\n",
      "  validation loss:\t\t0.264978\n",
      "  validation accuracy:\t\t91.50 %\n",
      "Epoch 156 of 200 took 8.255s\n",
      "  training loss:\t\t0.364009\n",
      "  validation loss:\t\t0.270239\n",
      "  validation accuracy:\t\t91.10 %\n",
      "Epoch 157 of 200 took 8.779s\n",
      "  training loss:\t\t0.365922\n",
      "  validation loss:\t\t0.275842\n",
      "  validation accuracy:\t\t90.60 %\n",
      "Epoch 158 of 200 took 8.708s\n",
      "  training loss:\t\t0.362660\n",
      "  validation loss:\t\t0.264397\n",
      "  validation accuracy:\t\t90.70 %\n",
      "Epoch 159 of 200 took 8.426s\n",
      "  training loss:\t\t0.359023\n",
      "  validation loss:\t\t0.262035\n",
      "  validation accuracy:\t\t91.40 %\n",
      "Epoch 160 of 200 took 8.199s\n",
      "  training loss:\t\t0.358491\n",
      "  validation loss:\t\t0.262696\n",
      "  validation accuracy:\t\t91.40 %\n",
      "Epoch 161 of 200 took 7.898s\n",
      "  training loss:\t\t0.358341\n",
      "  validation loss:\t\t0.263661\n",
      "  validation accuracy:\t\t90.80 %\n",
      "Epoch 162 of 200 took 8.438s\n",
      "  training loss:\t\t0.360890\n",
      "  validation loss:\t\t0.261573\n",
      "  validation accuracy:\t\t90.70 %\n",
      "Epoch 163 of 200 took 8.934s\n",
      "  training loss:\t\t0.362095\n",
      "  validation loss:\t\t0.278539\n",
      "  validation accuracy:\t\t90.50 %\n",
      "Epoch 164 of 200 took 8.127s\n",
      "  training loss:\t\t0.352311\n",
      "  validation loss:\t\t0.261367\n",
      "  validation accuracy:\t\t90.90 %\n",
      "Epoch 165 of 200 took 7.903s\n",
      "  training loss:\t\t0.359130\n",
      "  validation loss:\t\t0.258608\n",
      "  validation accuracy:\t\t91.70 %\n",
      "Epoch 166 of 200 took 7.794s\n",
      "  training loss:\t\t0.357826\n",
      "  validation loss:\t\t0.260801\n",
      "  validation accuracy:\t\t90.90 %\n",
      "Epoch 167 of 200 took 7.727s\n",
      "  training loss:\t\t0.359260\n",
      "  validation loss:\t\t0.258613\n",
      "  validation accuracy:\t\t91.10 %\n",
      "Epoch 168 of 200 took 7.768s\n",
      "  training loss:\t\t0.359229\n",
      "  validation loss:\t\t0.257153\n",
      "  validation accuracy:\t\t91.30 %\n",
      "Epoch 169 of 200 took 7.920s\n",
      "  training loss:\t\t0.358510\n",
      "  validation loss:\t\t0.265267\n",
      "  validation accuracy:\t\t90.80 %\n",
      "Epoch 170 of 200 took 7.748s\n",
      "  training loss:\t\t0.354825\n",
      "  validation loss:\t\t0.261816\n",
      "  validation accuracy:\t\t90.60 %\n",
      "Epoch 171 of 200 took 7.835s\n",
      "  training loss:\t\t0.354028\n",
      "  validation loss:\t\t0.254512\n",
      "  validation accuracy:\t\t91.20 %\n",
      "Epoch 172 of 200 took 7.883s\n",
      "  training loss:\t\t0.356429\n",
      "  validation loss:\t\t0.258156\n",
      "  validation accuracy:\t\t91.10 %\n",
      "Epoch 173 of 200 took 7.790s\n",
      "  training loss:\t\t0.357418\n",
      "  validation loss:\t\t0.263976\n",
      "  validation accuracy:\t\t90.80 %\n",
      "Epoch 174 of 200 took 7.837s\n",
      "  training loss:\t\t0.354270\n",
      "  validation loss:\t\t0.268439\n",
      "  validation accuracy:\t\t90.50 %\n",
      "Epoch 175 of 200 took 7.975s\n",
      "  training loss:\t\t0.356840\n",
      "  validation loss:\t\t0.258394\n",
      "  validation accuracy:\t\t91.50 %\n",
      "Epoch 176 of 200 took 7.817s\n",
      "  training loss:\t\t0.354581\n",
      "  validation loss:\t\t0.266672\n",
      "  validation accuracy:\t\t91.30 %\n",
      "Epoch 177 of 200 took 7.849s\n",
      "  training loss:\t\t0.358252\n",
      "  validation loss:\t\t0.258606\n",
      "  validation accuracy:\t\t91.40 %\n",
      "Epoch 178 of 200 took 7.843s\n",
      "  training loss:\t\t0.356251\n",
      "  validation loss:\t\t0.258998\n",
      "  validation accuracy:\t\t91.50 %\n",
      "Epoch 179 of 200 took 7.817s\n",
      "  training loss:\t\t0.351007\n",
      "  validation loss:\t\t0.254148\n",
      "  validation accuracy:\t\t91.60 %\n",
      "Epoch 180 of 200 took 7.897s\n",
      "  training loss:\t\t0.348100\n",
      "  validation loss:\t\t0.258240\n",
      "  validation accuracy:\t\t91.10 %\n",
      "Epoch 181 of 200 took 7.823s\n",
      "  training loss:\t\t0.352854\n",
      "  validation loss:\t\t0.252768\n",
      "  validation accuracy:\t\t91.30 %\n",
      "Epoch 182 of 200 took 7.794s\n",
      "  training loss:\t\t0.350328\n",
      "  validation loss:\t\t0.254068\n",
      "  validation accuracy:\t\t91.70 %\n",
      "Epoch 183 of 200 took 7.818s\n",
      "  training loss:\t\t0.347259\n",
      "  validation loss:\t\t0.255473\n",
      "  validation accuracy:\t\t91.40 %\n",
      "Epoch 184 of 200 took 7.813s\n",
      "  training loss:\t\t0.351603\n",
      "  validation loss:\t\t0.261176\n",
      "  validation accuracy:\t\t90.80 %\n",
      "Epoch 185 of 200 took 8.430s\n",
      "  training loss:\t\t0.350030\n",
      "  validation loss:\t\t0.259364\n",
      "  validation accuracy:\t\t91.10 %\n",
      "Epoch 186 of 200 took 7.852s\n",
      "  training loss:\t\t0.350498\n",
      "  validation loss:\t\t0.250818\n",
      "  validation accuracy:\t\t91.30 %\n",
      "Epoch 187 of 200 took 7.818s\n",
      "  training loss:\t\t0.344846\n",
      "  validation loss:\t\t0.258339\n",
      "  validation accuracy:\t\t91.40 %\n",
      "Epoch 188 of 200 took 7.850s\n",
      "  training loss:\t\t0.343325\n",
      "  validation loss:\t\t0.253755\n",
      "  validation accuracy:\t\t91.50 %\n",
      "Epoch 189 of 200 took 7.921s\n",
      "  training loss:\t\t0.344295\n",
      "  validation loss:\t\t0.257776\n",
      "  validation accuracy:\t\t91.20 %\n",
      "Epoch 190 of 200 took 7.834s\n",
      "  training loss:\t\t0.344876\n",
      "  validation loss:\t\t0.252243\n",
      "  validation accuracy:\t\t91.30 %\n",
      "Epoch 191 of 200 took 7.857s\n",
      "  training loss:\t\t0.345022\n",
      "  validation loss:\t\t0.258270\n",
      "  validation accuracy:\t\t91.40 %\n",
      "Epoch 192 of 200 took 7.874s\n",
      "  training loss:\t\t0.349056\n",
      "  validation loss:\t\t0.253664\n",
      "  validation accuracy:\t\t91.10 %\n",
      "Epoch 193 of 200 took 7.840s\n",
      "  training loss:\t\t0.346610\n",
      "  validation loss:\t\t0.250787\n",
      "  validation accuracy:\t\t91.10 %\n",
      "Epoch 194 of 200 took 7.818s\n",
      "  training loss:\t\t0.343192\n",
      "  validation loss:\t\t0.255021\n",
      "  validation accuracy:\t\t91.10 %\n",
      "Epoch 195 of 200 took 7.804s\n",
      "  training loss:\t\t0.338817\n",
      "  validation loss:\t\t0.256543\n",
      "  validation accuracy:\t\t91.40 %\n",
      "Epoch 196 of 200 took 7.852s\n",
      "  training loss:\t\t0.339574\n",
      "  validation loss:\t\t0.251150\n",
      "  validation accuracy:\t\t91.40 %\n",
      "Epoch 197 of 200 took 7.787s\n",
      "  training loss:\t\t0.341325\n",
      "  validation loss:\t\t0.245383\n",
      "  validation accuracy:\t\t91.50 %\n",
      "Epoch 198 of 200 took 7.785s\n",
      "  training loss:\t\t0.341320\n",
      "  validation loss:\t\t0.255006\n",
      "  validation accuracy:\t\t91.50 %\n",
      "Epoch 199 of 200 took 7.785s\n",
      "  training loss:\t\t0.346188\n",
      "  validation loss:\t\t0.253418\n",
      "  validation accuracy:\t\t91.70 %\n",
      "Epoch 200 of 200 took 7.782s\n",
      "  training loss:\t\t0.341096\n",
      "  validation loss:\t\t0.243866\n",
      "  validation accuracy:\t\t92.10 %\n",
      "Final results:\n",
      "  test loss:\t\t\t6.683797\n",
      "  test accuracy:\t\t19.25 %\n"
     ]
    }
   ],
   "source": [
    "train_cnn(num_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x113d3ef50>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEACAYAAABI5zaHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113d62c10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scatter(arange(len(percs2)),percs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
