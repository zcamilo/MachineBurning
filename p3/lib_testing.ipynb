{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_train = h5py.File('train.h5','r')\n",
    "f_test = h5py.File('test.h5','r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'train']\n",
      "[u'test']\n"
     ]
    }
   ],
   "source": [
    "print(f_train.keys())\n",
    "print(f_test.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a_train = f_train['train']\n",
    "a_test = f_test['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'axis0', u'axis1', u'block0_items', u'block0_values', u'block1_items', u'block1_values']\n",
      "[u'axis0', u'axis1', u'block0_items', u'block0_values']\n"
     ]
    }
   ],
   "source": [
    "print(a_train.keys())\n",
    "print(a_test.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b = a_test['block0_values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8137, 100)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# axis0 -> Labels [shape (101,)]\n",
    "# axis1 -> id column [shape (45324,)]\n",
    "# block0_items -> traits labels [shape (100,)]\n",
    "# block0_values -> traits values [shape (45324,100)]\n",
    "# block1_items -> 'y' label\n",
    "# block1_values -> y column (without label) [shape (45324,1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = a_train['block0_values'][()]\n",
    "y_train = a_train['block1_values'][()]\n",
    "y_train = y_train[:,0]\n",
    "X_test = a_test['block0_values'][()]\n",
    "X_train, X_val = X_train[:-8324], X_train[-8324:]\n",
    "y_train, y_val = y_train[:-8324], y_train[-8324:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37000, 100)\n",
      "<type 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(type(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x119e63590>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAEACAYAAABWLgY0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x119e63390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scatter(arange(100), y_train[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = ''\n",
    "z = 0\n",
    "for i in arange(100):\n",
    "    for j in arange(100):\n",
    "        if b[i,j]==0.:\n",
    "            z=z+1\n",
    "        s += str(b[i,j]) + ' '\n",
    "    s += '/n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78.1% percentage of zeros in chunk of matrix\n"
     ]
    }
   ],
   "source": [
    "print(str(z/100.) + '% percentage of zeros in chunk of matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from theano import *\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.0\n"
     ]
    }
   ],
   "source": [
    "x = T.dvector('x') # declare variable\n",
    "y = T.dvector('y')\n",
    "out = y.dot(y) + x.dot(x)         # build symbolic expression\n",
    "fun = theano.function([x,y], out)   # compile function\n",
    "print(fun([2,1],[1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Neural Network: Lasagne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Camilo/anaconda2/lib/python2.7/site-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import time\n",
    "import lasagne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_custom_mlp(input_var=None, depth=2, width=800, drop_input=.2,\n",
    "                     drop_hidden=.5):\n",
    "    # By default, this creates the same network as `build_mlp`, but it can be\n",
    "    # customized with respect to the number and size of hidden layers. This\n",
    "    # mostly showcases how creating a network in Python code can be a lot more\n",
    "    # flexible than a configuration file. Note that to make the code easier,\n",
    "    # all the layers are just called `network` -- there is no need to give them\n",
    "    # different names if all we return is the last one we created anyway; we\n",
    "    # just used different names above for clarity.\n",
    "\n",
    "    # Input layer and dropout (with shortcut `dropout` for `DropoutLayer`):\n",
    "    network = lasagne.layers.InputLayer(shape=(None, 100),\n",
    "                                        input_var=input_var)\n",
    "    if drop_input:\n",
    "        network = lasagne.layers.dropout(network, p=drop_input)\n",
    "    # Hidden layers and dropout:\n",
    "    nonlin = lasagne.nonlinearities.rectify\n",
    "    for _ in range(depth):\n",
    "        network = lasagne.layers.DenseLayer(network, width, nonlinearity=nonlin)\n",
    "        if drop_hidden:\n",
    "            network = lasagne.layers.dropout(network, p=drop_hidden)\n",
    "    # Output layer:\n",
    "    softmax = lasagne.nonlinearities.softmax\n",
    "    network = lasagne.layers.DenseLayer(network, 5, nonlinearity=softmax)\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(num_epochs=500):\n",
    "    # Load the dataset\n",
    "    # print(\"Loading data...\")\n",
    "    # X_train, y_train, X_val, y_val, X_test, y_test = load_dataset()\n",
    "    \n",
    "    # theano.config.optimizer='fast_compile'\n",
    "    # theano.config.exception_verbosity='high'\n",
    "\n",
    "    # Prepare Theano variables for inputs and targets\n",
    "    input_var = T.matrix('inputs')\n",
    "    target_var = T.ivector('targets')\n",
    "\n",
    "    # Create neural network model (depending on first command line parameter)\n",
    "    print(\"Building model and compiling functions...\")\n",
    "    \n",
    "    network = build_custom_mlp(input_var=input_var)\n",
    "\n",
    "    # Create a loss expression for training, i.e., a scalar objective we want\n",
    "    # to minimize (for our multi-class problem, it is the cross-entropy loss):\n",
    "    global prediction\n",
    "    prediction = lasagne.layers.get_output(network)\n",
    "    loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "    loss = loss.mean()\n",
    "    # We could add some weight decay as well here, see lasagne.regularization.\n",
    "\n",
    "    # Create update expressions for training, i.e., how to modify the\n",
    "    # parameters at each training step. Here, we'll use Stochastic Gradient\n",
    "    # Descent (SGD) with Nesterov momentum, but Lasagne offers plenty more.\n",
    "    global params\n",
    "    params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "    updates = lasagne.updates.nesterov_momentum(loss, params, learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "    # Create a loss expression for validation/testing. The crucial difference\n",
    "    # here is that we do a deterministic forward pass through the network,\n",
    "    # disabling dropout layers.\n",
    "    global test_prediction\n",
    "    test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "    test_loss = lasagne.objectives.categorical_crossentropy(test_prediction,target_var)\n",
    "    test_loss = test_loss.mean()\n",
    "    \n",
    "    # As a bonus, also create an expression for the classification accuracy:\n",
    "    test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),dtype=theano.config.floatX)\n",
    "\n",
    "    # Compile a function performing a training step on a mini-batch (by giving\n",
    "    # the updates dictionary) and returning the corresponding training loss:\n",
    "    train_fn = theano.function([input_var, target_var], loss, updates=updates, allow_input_downcast=True)\n",
    "\n",
    "    # Compile a second function computing the validation loss and accuracy:\n",
    "    val_fn = theano.function([input_var, target_var], [test_loss, test_acc], allow_input_downcast=True)\n",
    "\n",
    "    # Finally, launch the training loop.\n",
    "    print(\"Starting training...\")\n",
    "    # We iterate over epochs:\n",
    "    for epoch in range(num_epochs):\n",
    "        # In each epoch, we do a full pass over the training data:\n",
    "        train_err = 0\n",
    "        train_batches = 0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for batch in iterate_minibatches(X_train, y_train, 500, shuffle=True):\n",
    "            inputs, targets = batch\n",
    "            train_err += train_fn(inputs, targets)\n",
    "            train_batches += 1\n",
    "\n",
    "        # And a full pass over the validation data:\n",
    "        val_err = 0\n",
    "        val_acc = 0\n",
    "        val_batches = 0\n",
    "        for batch in iterate_minibatches(X_val, y_val, 500, shuffle=False):\n",
    "            inputs, targets = batch\n",
    "            err, acc = val_fn(inputs, targets)\n",
    "            val_err += err\n",
    "            val_acc += acc\n",
    "            val_batches += 1\n",
    "\n",
    "        # Then we print the results for this epoch:\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "            epoch + 1, num_epochs, time.time() - start_time))\n",
    "        print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "        print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "        print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "            val_acc / val_batches * 100))\n",
    "\n",
    "    # After training, we compute and print the test error:\n",
    "    test_err = 0\n",
    "    test_acc = 0\n",
    "    test_batches = 0\n",
    "    for batch in iterate_minibatches(X_test, zeros(len(X_test)), 500, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        err, acc = val_fn(inputs, targets)\n",
    "        test_err += err\n",
    "        test_acc += acc\n",
    "        test_batches += 1\n",
    "    print(\"Final results:\")\n",
    "    print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "    print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "        test_acc / test_batches * 100))\n",
    "    global result\n",
    "    result = lasagne.layers.get_output(network, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model and compiling functions...\n",
      "Starting training...\n",
      "Epoch 1 of 10 took 7.961s\n",
      "  training loss:\t\t1.271109\n",
      "  validation loss:\t\t0.800002\n",
      "  validation accuracy:\t\t73.99 %\n",
      "Epoch 2 of 10 took 7.829s\n",
      "  training loss:\t\t0.862463\n",
      "  validation loss:\t\t0.626833\n",
      "  validation accuracy:\t\t78.47 %\n",
      "Epoch 3 of 10 took 7.847s\n",
      "  training loss:\t\t0.769628\n",
      "  validation loss:\t\t0.559716\n",
      "  validation accuracy:\t\t80.61 %\n",
      "Epoch 4 of 10 took 7.993s\n",
      "  training loss:\t\t0.708806\n",
      "  validation loss:\t\t0.514491\n",
      "  validation accuracy:\t\t82.69 %\n",
      "Epoch 5 of 10 took 7.867s\n",
      "  training loss:\t\t0.669824\n",
      "  validation loss:\t\t0.479372\n",
      "  validation accuracy:\t\t84.19 %\n",
      "Epoch 6 of 10 took 7.838s\n",
      "  training loss:\t\t0.629248\n",
      "  validation loss:\t\t0.444861\n",
      "  validation accuracy:\t\t85.52 %\n",
      "Epoch 7 of 10 took 7.865s\n",
      "  training loss:\t\t0.602473\n",
      "  validation loss:\t\t0.423070\n",
      "  validation accuracy:\t\t86.40 %\n",
      "Epoch 8 of 10 took 7.893s\n",
      "  training loss:\t\t0.578431\n",
      "  validation loss:\t\t0.400502\n",
      "  validation accuracy:\t\t87.02 %\n",
      "Epoch 9 of 10 took 8.485s\n",
      "  training loss:\t\t0.560682\n",
      "  validation loss:\t\t0.383109\n",
      "  validation accuracy:\t\t87.86 %\n",
      "Epoch 10 of 10 took 7.827s\n",
      "  training loss:\t\t0.539651\n",
      "  validation loss:\t\t0.368446\n",
      "  validation accuracy:\t\t88.14 %\n",
      "Final results:\n",
      "  test loss:\t\t\t4.002048\n",
      "  test accuracy:\t\t19.46 %\n"
     ]
    }
   ],
   "source": [
    "train(num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  5.50124294e-02,   4.87319617e-02,   3.62583785e-02,\n",
       "          1.60178457e-01,   6.99818774e-01],\n",
       "       [  1.61181905e-02,   1.40533871e-03,   4.23213505e-01,\n",
       "          2.55160790e-01,   3.04102176e-01],\n",
       "       [  3.23989469e-01,   6.07254414e-02,   1.56731097e-02,\n",
       "          1.81347875e-01,   4.18264105e-01],\n",
       "       ..., \n",
       "       [  9.87987196e-01,   2.24993996e-03,   3.06034903e-03,\n",
       "          6.38861710e-03,   3.13898410e-04],\n",
       "       [  9.34480076e-01,   2.02446452e-03,   2.01833231e-02,\n",
       "          2.53259230e-02,   1.79862138e-02],\n",
       "       [  2.99844557e-02,   6.36540826e-03,   7.92438084e-01,\n",
       "          4.85413042e-02,   1.22670748e-01]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final = result.eval()\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99999999999999989"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(final[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_output = list([])\n",
    "for i in arange(len(final)):\n",
    "    my_list = final[i,:]\n",
    "    max_index = my_list.argmax()\n",
    "    y_output.append(max_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8137"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " ...]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
