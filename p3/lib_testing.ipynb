{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_train = h5py.File('train.h5','r')\n",
    "f_test = h5py.File('test.h5','r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'train']\n",
      "[u'test']\n"
     ]
    }
   ],
   "source": [
    "print(f_train.keys())\n",
    "print(f_test.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a_train = f_train['train']\n",
    "a_test = f_test['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'axis0', u'axis1', u'block0_items', u'block0_values', u'block1_items', u'block1_values']\n",
      "[u'axis0', u'axis1', u'block0_items', u'block0_values']\n"
     ]
    }
   ],
   "source": [
    "print(a_train.keys())\n",
    "print(a_test.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b = a_test['axis1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8137,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# axis0 -> Labels [shape (101,)]\n",
    "# axis1 -> id column [shape (45324,)]\n",
    "# block0_items -> traits labels [shape (100,)]\n",
    "# block0_values -> traits values [shape (45324,100)]\n",
    "# block1_items -> 'y' label\n",
    "# block1_values -> y column (without label) [shape (45324,1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = a_train['block0_values'][()]\n",
    "y_train = a_train['block1_values'][()]\n",
    "y_train = y_train[:,0]\n",
    "X_test = a_test['block0_values'][()]\n",
    "X_train, X_val = X_train[:-1000], X_train[-1000:]\n",
    "y_train, y_val = y_train[:-1000], y_train[-1000:]\n",
    "ids = a_test['axis1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44324, 100)\n",
      "<type 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(type(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x10de6cc10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAEACAYAAABWLgY0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x109aec550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scatter(arange(100), y_train[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = ''\n",
    "z = 0\n",
    "for i in arange(100):\n",
    "    for j in arange(100):\n",
    "        if b[i,j]==0.:\n",
    "            z=z+1\n",
    "        s += str(b[i,j]) + ' '\n",
    "    s += '/n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78.1% percentage of zeros in chunk of matrix\n"
     ]
    }
   ],
   "source": [
    "print(str(z/100.) + '% percentage of zeros in chunk of matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from theano import *\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.0\n"
     ]
    }
   ],
   "source": [
    "x = T.dvector('x') # declare variable\n",
    "y = T.dvector('y')\n",
    "out = y.dot(y) + x.dot(x)         # build symbolic expression\n",
    "fun = theano.function([x,y], out)   # compile function\n",
    "print(fun([2,1],[1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Neural Network: Lasagne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Camilo/anaconda2/lib/python2.7/site-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import time\n",
    "import lasagne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_custom_mlp(input_var=None, depth=2, width=800, drop_input=.2,\n",
    "                     drop_hidden=.5):\n",
    "    # By default, this creates the same network as `build_mlp`, but it can be\n",
    "    # customized with respect to the number and size of hidden layers. This\n",
    "    # mostly showcases how creating a network in Python code can be a lot more\n",
    "    # flexible than a configuration file. Note that to make the code easier,\n",
    "    # all the layers are just called `network` -- there is no need to give them\n",
    "    # different names if all we return is the last one we created anyway; we\n",
    "    # just used different names above for clarity.\n",
    "\n",
    "    # Input layer and dropout (with shortcut `dropout` for `DropoutLayer`):\n",
    "    network = lasagne.layers.InputLayer(shape=(None, 100),\n",
    "                                        input_var=input_var)\n",
    "    if drop_input:\n",
    "        network = lasagne.layers.dropout(network, p=drop_input)\n",
    "    # Hidden layers and dropout:\n",
    "    nonlin = lasagne.nonlinearities.rectify\n",
    "    for _ in range(depth):\n",
    "        network = lasagne.layers.DenseLayer(network, width, nonlinearity=nonlin)\n",
    "        if drop_hidden:\n",
    "            network = lasagne.layers.dropout(network, p=drop_hidden)\n",
    "    # Output layer:\n",
    "    softmax = lasagne.nonlinearities.softmax\n",
    "    network = lasagne.layers.DenseLayer(network, 5, nonlinearity=softmax)\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_nnet(num_epochs=500):\n",
    "    # Load the dataset\n",
    "    # print(\"Loading data...\")\n",
    "    # X_train, y_train, X_val, y_val, X_test, y_test = load_dataset()\n",
    "    \n",
    "    # theano.config.optimizer='fast_compile'\n",
    "    # theano.config.exception_verbosity='high'\n",
    "\n",
    "    # Prepare Theano variables for inputs and targets\n",
    "    input_var = T.matrix('inputs')\n",
    "    target_var = T.ivector('targets')\n",
    "\n",
    "    # Create neural network model (depending on first command line parameter)\n",
    "    print(\"Building model and compiling functions...\")\n",
    "    \n",
    "    network = build_custom_mlp(input_var=input_var,depth=2,width=1000,drop_input=.3,drop_hidden=.5)\n",
    "\n",
    "    # Create a loss expression for training, i.e., a scalar objective we want\n",
    "    # to minimize (for our multi-class problem, it is the cross-entropy loss):\n",
    "    prediction = lasagne.layers.get_output(network)\n",
    "    loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "    loss = loss.mean()\n",
    "    # We could add some weight decay as well here, see lasagne.regularization.\n",
    "\n",
    "    # Create update expressions for training, i.e., how to modify the\n",
    "    # parameters at each training step. Here, we'll use Stochastic Gradient\n",
    "    # Descent (SGD) with Nesterov momentum, but Lasagne offers plenty more.\n",
    "    params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "    updates = lasagne.updates.nesterov_momentum(loss, params, learning_rate=0.1, momentum=0.8)\n",
    "\n",
    "    # Create a loss expression for validation/testing. The crucial difference\n",
    "    # here is that we do a deterministic forward pass through the network,\n",
    "    # disabling dropout layers.\n",
    "    test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "    test_loss = lasagne.objectives.categorical_crossentropy(test_prediction,target_var)\n",
    "    test_loss = test_loss.mean()\n",
    "    \n",
    "    # As a bonus, also create an expression for the classification accuracy:\n",
    "    test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),dtype=theano.config.floatX)\n",
    "\n",
    "    # Compile a function performing a training step on a mini-batch (by giving\n",
    "    # the updates dictionary) and returning the corresponding training loss:\n",
    "    train_fn = theano.function([input_var, target_var], loss, updates=updates, allow_input_downcast=True)\n",
    "\n",
    "    # Compile a second function computing the validation loss and accuracy:\n",
    "    val_fn = theano.function([input_var, target_var], [test_loss, test_acc], allow_input_downcast=True)\n",
    "\n",
    "    # Finally, launch the training loop.\n",
    "    print(\"Starting training...\")\n",
    "    # We iterate over epochs:\n",
    "    global percs\n",
    "    percs = array([])\n",
    "    for epoch in range(num_epochs):\n",
    "        # In each epoch, we do a full pass over the training data:\n",
    "        train_err = 0\n",
    "        train_batches = 0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for batch in iterate_minibatches(X_train, y_train, 500, shuffle=True):\n",
    "            inputs, targets = batch\n",
    "            train_err += train_fn(inputs, targets)\n",
    "            train_batches += 1\n",
    "\n",
    "        # And a full pass over the validation data:\n",
    "        val_err = 0\n",
    "        val_acc = 0\n",
    "        val_batches = 0\n",
    "        for batch in iterate_minibatches(X_val, y_val, 500, shuffle=False):\n",
    "            inputs, targets = batch\n",
    "            err, acc = val_fn(inputs, targets)\n",
    "            val_err += err\n",
    "            val_acc += acc\n",
    "            val_batches += 1\n",
    "\n",
    "        # Then we print the results for this epoch:\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "            epoch + 1, num_epochs, time.time() - start_time))\n",
    "        print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "        print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "        perc = val_acc / val_batches\n",
    "        percs = append(percs, perc)\n",
    "        print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "            perc * 100))\n",
    "\n",
    "    # After training, we compute and print the test error:\n",
    "    test_err = 0\n",
    "    test_acc = 0\n",
    "    test_batches = 0\n",
    "    for batch in iterate_minibatches(X_test, zeros(len(X_test)), 500, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        err, acc = val_fn(inputs, targets)\n",
    "        test_err += err\n",
    "        test_acc += acc\n",
    "        test_batches += 1\n",
    "    print(\"Final results:\")\n",
    "    print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "    print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "        test_acc / test_batches * 100))\n",
    "    global result\n",
    "    result = lasagne.layers.get_output(network, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model and compiling functions...\n",
      "Starting training...\n",
      "Epoch 1 of 20 took 17.201s\n",
      "  training loss:\t\t0.910129\n",
      "  validation loss:\t\t0.473334\n",
      "  validation accuracy:\t\t85.10 %\n",
      "Epoch 2 of 20 took 16.000s\n",
      "  training loss:\t\t0.651152\n",
      "  validation loss:\t\t0.371750\n",
      "  validation accuracy:\t\t88.10 %\n",
      "Epoch 3 of 20 took 15.549s\n",
      "  training loss:\t\t0.577208\n",
      "  validation loss:\t\t0.332465\n",
      "  validation accuracy:\t\t89.40 %\n",
      "Epoch 4 of 20 took 15.444s\n",
      "  training loss:\t\t0.531923\n",
      "  validation loss:\t\t0.298115\n",
      "  validation accuracy:\t\t90.20 %\n",
      "Epoch 5 of 20 took 15.675s\n",
      "  training loss:\t\t0.503946\n",
      "  validation loss:\t\t0.278973\n",
      "  validation accuracy:\t\t91.40 %\n",
      "Epoch 6 of 20 took 15.650s\n",
      "  training loss:\t\t0.486745\n",
      "  validation loss:\t\t0.277478\n",
      "  validation accuracy:\t\t91.40 %\n",
      "Epoch 7 of 20 took 15.657s\n",
      "  training loss:\t\t0.476122\n",
      "  validation loss:\t\t0.255260\n",
      "  validation accuracy:\t\t92.50 %\n",
      "Epoch 8 of 20 took 15.401s\n",
      "  training loss:\t\t0.454511\n",
      "  validation loss:\t\t0.243681\n",
      "  validation accuracy:\t\t92.40 %\n",
      "Epoch 9 of 20 took 15.814s\n",
      "  training loss:\t\t0.451709\n",
      "  validation loss:\t\t0.242789\n",
      "  validation accuracy:\t\t92.10 %\n",
      "Epoch 10 of 20 took 15.513s\n",
      "  training loss:\t\t0.446095\n",
      "  validation loss:\t\t0.238795\n",
      "  validation accuracy:\t\t92.60 %\n",
      "Epoch 11 of 20 took 15.394s\n",
      "  training loss:\t\t0.435917\n",
      "  validation loss:\t\t0.237293\n",
      "  validation accuracy:\t\t93.00 %\n",
      "Epoch 12 of 20 took 15.465s\n",
      "  training loss:\t\t0.427234\n",
      "  validation loss:\t\t0.228632\n",
      "  validation accuracy:\t\t93.80 %\n",
      "Epoch 13 of 20 took 15.655s\n",
      "  training loss:\t\t0.425562\n",
      "  validation loss:\t\t0.229006\n",
      "  validation accuracy:\t\t92.60 %\n",
      "Epoch 14 of 20 took 15.559s\n",
      "  training loss:\t\t0.416665\n",
      "  validation loss:\t\t0.222721\n",
      "  validation accuracy:\t\t92.90 %\n",
      "Epoch 15 of 20 took 15.702s\n",
      "  training loss:\t\t0.417196\n",
      "  validation loss:\t\t0.222981\n",
      "  validation accuracy:\t\t92.70 %\n",
      "Epoch 16 of 20 took 16.188s\n",
      "  training loss:\t\t0.403741\n",
      "  validation loss:\t\t0.216373\n",
      "  validation accuracy:\t\t93.20 %\n",
      "Epoch 17 of 20 took 18.727s\n",
      "  training loss:\t\t0.408647\n",
      "  validation loss:\t\t0.218418\n",
      "  validation accuracy:\t\t92.90 %\n",
      "Epoch 18 of 20 took 18.576s\n",
      "  training loss:\t\t0.408588\n",
      "  validation loss:\t\t0.208839\n",
      "  validation accuracy:\t\t93.70 %\n",
      "Epoch 19 of 20 took 17.798s\n",
      "  training loss:\t\t0.403999\n",
      "  validation loss:\t\t0.218946\n",
      "  validation accuracy:\t\t93.00 %\n",
      "Epoch 20 of 20 took 15.270s\n",
      "  training loss:\t\t0.397485\n",
      "  validation loss:\t\t0.211312\n",
      "  validation accuracy:\t\t93.00 %\n",
      "Final results:\n",
      "  test loss:\t\t\t4.872640\n",
      "  test accuracy:\t\t19.00 %\n"
     ]
    }
   ],
   "source": [
    "train_nnet(num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x114411a50>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZglx6JuZJeT/AWppi0/y11bgAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114428e50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scatter(arange(len(percs)),percs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  3.18216351e-03,   2.23474139e-02,   4.50536699e-02,\n",
       "          2.95644512e-03,   9.26460308e-01],\n",
       "       [  1.21483342e-01,   8.51775788e-03,   1.29975228e-01,\n",
       "          3.72959250e-01,   3.67064422e-01],\n",
       "       [  3.95705044e-01,   4.60606726e-01,   2.13350668e-02,\n",
       "          6.33180164e-02,   5.90351466e-02],\n",
       "       ..., \n",
       "       [  9.99949351e-01,   3.44554606e-05,   2.93492469e-06,\n",
       "          1.26010328e-05,   6.57124163e-07],\n",
       "       [  9.95113945e-01,   4.40167075e-05,   4.56171035e-04,\n",
       "          7.07840554e-04,   3.67802660e-03],\n",
       "       [  1.37977142e-02,   8.00849838e-04,   9.74504435e-01,\n",
       "          8.21589474e-03,   2.68110659e-03]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final = result.eval()\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(final[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_output = list([])\n",
    "for i in arange(len(final)):\n",
    "    my_list = final[i,:]\n",
    "    max_index = my_list.argmax()\n",
    "    y_output.append(max_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8137"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " ...]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('result1.csv', 'wb') as fp:\n",
    "    a = csv.writer(fp, delimiter=',')\n",
    "    a.writerow(('Id','y'))\n",
    "    a.writerows(zip(ids,y_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8137, 1, 10, 10)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_c = X_train.reshape(len(X_train),1,10,10)\n",
    "X_val_c = X_val.reshape(len(X_val),1,10,10)\n",
    "X_test_c = X_test.reshape(len(X_test),1,10,10) \n",
    "X_test_c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_cnn(input_var=None):\n",
    "    # As a third model, we'll create a CNN of two convolution + pooling stages\n",
    "    # and a fully-connected hidden layer in front of the output layer.\n",
    "\n",
    "    # Input layer, as usual:\n",
    "    network = lasagne.layers.InputLayer(shape=(None, 1, 10, 10),\n",
    "                                        input_var=input_var)\n",
    "    # This time we do not apply input dropout, as it tends to work less well\n",
    "    # for convolutional layers.\n",
    "\n",
    "    # Convolutional layer with 32 kernels of size 5x5. Strided and padded\n",
    "    # convolutions are supported as well; see the docstring.\n",
    "    network = lasagne.layers.Conv2DLayer(\n",
    "            network, num_filters=32, filter_size=(3, 3),\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform())\n",
    "    # Expert note: Lasagne provides alternative convolutional layers that\n",
    "    # override Theano's choice of which implementation to use; for details\n",
    "    # please see http://lasagne.readthedocs.org/en/latest/user/tutorial.html.\n",
    "\n",
    "    # Max-pooling layer of factor 2 in both dimensions:\n",
    "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2))\n",
    "\n",
    "    # Another convolution with 32 5x5 kernels, and another 2x2 pooling:\n",
    "    network = lasagne.layers.Conv2DLayer(\n",
    "            network, num_filters=32, filter_size=(3, 3),\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2))\n",
    "\n",
    "    # A fully-connected layer of 256 units with 50% dropout on its inputs:\n",
    "    network = lasagne.layers.DenseLayer(\n",
    "            lasagne.layers.dropout(network, p=.4),\n",
    "            num_units=256,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "\n",
    "    # And, finally, the 10-unit output layer with 50% dropout on its inputs:\n",
    "    network = lasagne.layers.DenseLayer(\n",
    "            lasagne.layers.dropout(network, p=.4),\n",
    "            num_units=5,\n",
    "            nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_cnn(num_epochs=500):\n",
    "    # Load the dataset\n",
    "    # print(\"Loading data...\")\n",
    "    # X_train, y_train, X_val, y_val, X_test, y_test = load_dataset()\n",
    "    \n",
    "    # theano.config.optimizer='fast_compile'\n",
    "    # theano.config.exception_verbosity='high'\n",
    "\n",
    "    # Prepare Theano variables for inputs and targets\n",
    "    input_var = T.tensor4('inputs')\n",
    "    target_var = T.ivector('targets')\n",
    "\n",
    "    # Create neural network model (depending on first command line parameter)\n",
    "    print(\"Building model and compiling functions...\")\n",
    "    \n",
    "    network = build_cnn(input_var=input_var)\n",
    "\n",
    "    # Create a loss expression for training, i.e., a scalar objective we want\n",
    "    # to minimize (for our multi-class problem, it is the cross-entropy loss):\n",
    "    prediction = lasagne.layers.get_output(network)\n",
    "    loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "    loss = loss.mean()\n",
    "    # We could add some weight decay as well here, see lasagne.regularization.\n",
    "\n",
    "    # Create update expressions for training, i.e., how to modify the\n",
    "    # parameters at each training step. Here, we'll use Stochastic Gradient\n",
    "    # Descent (SGD) with Nesterov momentum, but Lasagne offers plenty more.\n",
    "    params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "    updates = lasagne.updates.nesterov_momentum(loss, params, learning_rate=0.006, momentum=0.8)\n",
    "\n",
    "    # Create a loss expression for validation/testing. The crucial difference\n",
    "    # here is that we do a deterministic forward pass through the network,\n",
    "    # disabling dropout layers.\n",
    "    test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "    test_loss = lasagne.objectives.categorical_crossentropy(test_prediction,target_var)\n",
    "    test_loss = test_loss.mean()\n",
    "    \n",
    "    # As a bonus, also create an expression for the classification accuracy:\n",
    "    test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),dtype=theano.config.floatX)\n",
    "\n",
    "    # Compile a function performing a training step on a mini-batch (by giving\n",
    "    # the updates dictionary) and returning the corresponding training loss:\n",
    "    train_fn = theano.function([input_var, target_var], loss, updates=updates, allow_input_downcast=True)\n",
    "\n",
    "    # Compile a second function computing the validation loss and accuracy:\n",
    "    val_fn = theano.function([input_var, target_var], [test_loss, test_acc], allow_input_downcast=True)\n",
    "\n",
    "    # Finally, launch the training loop.\n",
    "    print(\"Starting training...\")\n",
    "    # We iterate over epochs:\n",
    "    global percs2\n",
    "    percs2 = array([])\n",
    "    for epoch in range(num_epochs):\n",
    "        # In each epoch, we do a full pass over the training data:\n",
    "        train_err = 0\n",
    "        train_batches = 0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for batch in iterate_minibatches(X_train_c, y_train, 500, shuffle=True):\n",
    "            inputs, targets = batch\n",
    "            train_err += train_fn(inputs, targets)\n",
    "            train_batches += 1\n",
    "\n",
    "        # And a full pass over the validation data:\n",
    "        val_err = 0\n",
    "        val_acc = 0\n",
    "        val_batches = 0\n",
    "        for batch in iterate_minibatches(X_val_c, y_val, 500, shuffle=False):\n",
    "            inputs, targets = batch\n",
    "            err, acc = val_fn(inputs, targets)\n",
    "            val_err += err\n",
    "            val_acc += acc\n",
    "            val_batches += 1\n",
    "\n",
    "        # Then we print the results for this epoch:\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "            epoch + 1, num_epochs, time.time() - start_time))\n",
    "        print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "        print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "        perc = val_acc/val_batches\n",
    "        percs2 = append(percs2,perc)\n",
    "        print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "            perc * 100))\n",
    "\n",
    "    # After training, we compute and print the test error:\n",
    "    test_err = 0\n",
    "    test_acc = 0\n",
    "    test_batches = 0\n",
    "    for batch in iterate_minibatches(X_test_c, zeros(len(X_test_c)), 500, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        err, acc = val_fn(inputs, targets)\n",
    "        test_err += err\n",
    "        test_acc += acc\n",
    "        test_batches += 1\n",
    "    print(\"Final results:\")\n",
    "    print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "    print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "        test_acc / test_batches * 100))\n",
    "    global result2\n",
    "    result2 = lasagne.layers.get_output(network, X_test_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model and compiling functions...\n",
      "Starting training...\n",
      "Epoch 1 of 200 took 8.104s\n",
      "  training loss:\t\t1.605329\n",
      "  validation loss:\t\t1.595370\n",
      "  validation accuracy:\t\t35.10 %\n",
      "Epoch 2 of 200 took 9.128s\n",
      "  training loss:\t\t1.594809\n",
      "  validation loss:\t\t1.580480\n",
      "  validation accuracy:\t\t37.00 %\n",
      "Epoch 3 of 200 took 8.630s\n",
      "  training loss:\t\t1.579551\n",
      "  validation loss:\t\t1.556551\n",
      "  validation accuracy:\t\t43.30 %\n",
      "Epoch 4 of 200 took 8.000s\n",
      "  training loss:\t\t1.555958\n",
      "  validation loss:\t\t1.517005\n",
      "  validation accuracy:\t\t48.60 %\n",
      "Epoch 5 of 200 took 8.021s\n",
      "  training loss:\t\t1.515565\n",
      "  validation loss:\t\t1.453244\n",
      "  validation accuracy:\t\t54.20 %\n",
      "Epoch 6 of 200 took 8.042s\n",
      "  training loss:\t\t1.454881\n",
      "  validation loss:\t\t1.359869\n",
      "  validation accuracy:\t\t55.50 %\n",
      "Epoch 7 of 200 took 8.086s\n",
      "  training loss:\t\t1.373174\n",
      "  validation loss:\t\t1.246869\n",
      "  validation accuracy:\t\t60.30 %\n",
      "Epoch 8 of 200 took 8.039s\n",
      "  training loss:\t\t1.280525\n",
      "  validation loss:\t\t1.128536\n",
      "  validation accuracy:\t\t64.50 %\n",
      "Epoch 9 of 200 took 8.032s\n",
      "  training loss:\t\t1.189100\n",
      "  validation loss:\t\t1.020004\n",
      "  validation accuracy:\t\t67.20 %\n",
      "Epoch 10 of 200 took 8.050s\n",
      "  training loss:\t\t1.108413\n",
      "  validation loss:\t\t0.928469\n",
      "  validation accuracy:\t\t69.40 %\n",
      "Epoch 11 of 200 took 7.975s\n",
      "  training loss:\t\t1.036004\n",
      "  validation loss:\t\t0.850083\n",
      "  validation accuracy:\t\t70.30 %\n",
      "Epoch 12 of 200 took 7.941s\n",
      "  training loss:\t\t0.978064\n",
      "  validation loss:\t\t0.793585\n",
      "  validation accuracy:\t\t73.30 %\n",
      "Epoch 13 of 200 took 8.010s\n",
      "  training loss:\t\t0.926503\n",
      "  validation loss:\t\t0.746796\n",
      "  validation accuracy:\t\t75.30 %\n",
      "Epoch 14 of 200 took 7.921s\n",
      "  training loss:\t\t0.884514\n",
      "  validation loss:\t\t0.706123\n",
      "  validation accuracy:\t\t76.40 %\n",
      "Epoch 15 of 200 took 7.937s\n",
      "  training loss:\t\t0.847506\n",
      "  validation loss:\t\t0.669086\n",
      "  validation accuracy:\t\t77.10 %\n",
      "Epoch 16 of 200 took 8.336s\n",
      "  training loss:\t\t0.817789\n",
      "  validation loss:\t\t0.637556\n",
      "  validation accuracy:\t\t78.00 %\n",
      "Epoch 17 of 200 took 8.127s\n",
      "  training loss:\t\t0.792047\n",
      "  validation loss:\t\t0.616793\n",
      "  validation accuracy:\t\t78.50 %\n",
      "Epoch 18 of 200 took 7.950s\n",
      "  training loss:\t\t0.765281\n",
      "  validation loss:\t\t0.597154\n",
      "  validation accuracy:\t\t79.70 %\n",
      "Epoch 19 of 200 took 8.078s\n",
      "  training loss:\t\t0.743829\n",
      "  validation loss:\t\t0.574206\n",
      "  validation accuracy:\t\t80.30 %\n",
      "Epoch 20 of 200 took 7.995s\n",
      "  training loss:\t\t0.725031\n",
      "  validation loss:\t\t0.555656\n",
      "  validation accuracy:\t\t80.70 %\n",
      "Epoch 21 of 200 took 8.018s\n",
      "  training loss:\t\t0.710565\n",
      "  validation loss:\t\t0.545193\n",
      "  validation accuracy:\t\t80.70 %\n",
      "Epoch 22 of 200 took 8.038s\n",
      "  training loss:\t\t0.693923\n",
      "  validation loss:\t\t0.525192\n",
      "  validation accuracy:\t\t82.00 %\n",
      "Epoch 23 of 200 took 8.002s\n",
      "  training loss:\t\t0.682002\n",
      "  validation loss:\t\t0.514015\n",
      "  validation accuracy:\t\t82.80 %\n",
      "Epoch 24 of 200 took 8.184s\n",
      "  training loss:\t\t0.668003\n",
      "  validation loss:\t\t0.496519\n",
      "  validation accuracy:\t\t83.30 %\n",
      "Epoch 25 of 200 took 8.036s\n",
      "  training loss:\t\t0.653778\n",
      "  validation loss:\t\t0.490878\n",
      "  validation accuracy:\t\t83.30 %\n",
      "Epoch 26 of 200 took 7.977s\n",
      "  training loss:\t\t0.645650\n",
      "  validation loss:\t\t0.483085\n",
      "  validation accuracy:\t\t83.70 %\n",
      "Epoch 27 of 200 took 7.955s\n",
      "  training loss:\t\t0.631375\n",
      "  validation loss:\t\t0.472321\n",
      "  validation accuracy:\t\t83.50 %\n",
      "Epoch 28 of 200 took 8.017s\n",
      "  training loss:\t\t0.624419\n",
      "  validation loss:\t\t0.459791\n",
      "  validation accuracy:\t\t84.20 %\n",
      "Epoch 29 of 200 took 8.039s\n",
      "  training loss:\t\t0.614381\n",
      "  validation loss:\t\t0.445606\n",
      "  validation accuracy:\t\t84.40 %\n",
      "Epoch 30 of 200 took 7.956s\n",
      "  training loss:\t\t0.606311\n",
      "  validation loss:\t\t0.448835\n",
      "  validation accuracy:\t\t84.50 %\n",
      "Epoch 31 of 200 took 7.943s\n",
      "  training loss:\t\t0.598357\n",
      "  validation loss:\t\t0.437211\n",
      "  validation accuracy:\t\t84.70 %\n",
      "Epoch 32 of 200 took 7.997s\n",
      "  training loss:\t\t0.589148\n",
      "  validation loss:\t\t0.425980\n",
      "  validation accuracy:\t\t85.40 %\n",
      "Epoch 33 of 200 took 7.959s\n",
      "  training loss:\t\t0.583763\n",
      "  validation loss:\t\t0.423346\n",
      "  validation accuracy:\t\t85.00 %\n",
      "Epoch 34 of 200 took 7.957s\n",
      "  training loss:\t\t0.579154\n",
      "  validation loss:\t\t0.425024\n",
      "  validation accuracy:\t\t85.40 %\n",
      "Epoch 35 of 200 took 7.934s\n",
      "  training loss:\t\t0.569498\n",
      "  validation loss:\t\t0.408434\n",
      "  validation accuracy:\t\t86.10 %\n",
      "Epoch 36 of 200 took 7.938s\n",
      "  training loss:\t\t0.560156\n",
      "  validation loss:\t\t0.409150\n",
      "  validation accuracy:\t\t86.30 %\n",
      "Epoch 37 of 200 took 7.918s\n",
      "  training loss:\t\t0.558497\n",
      "  validation loss:\t\t0.406031\n",
      "  validation accuracy:\t\t85.80 %\n",
      "Epoch 38 of 200 took 7.966s\n",
      "  training loss:\t\t0.555204\n",
      "  validation loss:\t\t0.397139\n",
      "  validation accuracy:\t\t86.40 %\n",
      "Epoch 39 of 200 took 7.991s\n",
      "  training loss:\t\t0.547951\n",
      "  validation loss:\t\t0.390187\n",
      "  validation accuracy:\t\t86.80 %\n",
      "Epoch 40 of 200 took 7.951s\n",
      "  training loss:\t\t0.542175\n",
      "  validation loss:\t\t0.385944\n",
      "  validation accuracy:\t\t86.40 %\n",
      "Epoch 41 of 200 took 7.924s\n",
      "  training loss:\t\t0.540718\n",
      "  validation loss:\t\t0.387813\n",
      "  validation accuracy:\t\t86.70 %\n",
      "Epoch 42 of 200 took 7.951s\n",
      "  training loss:\t\t0.533506\n",
      "  validation loss:\t\t0.378829\n",
      "  validation accuracy:\t\t87.00 %\n",
      "Epoch 43 of 200 took 7.897s\n",
      "  training loss:\t\t0.530009\n",
      "  validation loss:\t\t0.370366\n",
      "  validation accuracy:\t\t87.50 %\n",
      "Epoch 44 of 200 took 7.952s\n",
      "  training loss:\t\t0.528555\n",
      "  validation loss:\t\t0.375201\n",
      "  validation accuracy:\t\t87.10 %\n",
      "Epoch 45 of 200 took 7.939s\n",
      "  training loss:\t\t0.519818\n",
      "  validation loss:\t\t0.365400\n",
      "  validation accuracy:\t\t87.00 %\n",
      "Epoch 46 of 200 took 7.907s\n",
      "  training loss:\t\t0.515669\n",
      "  validation loss:\t\t0.369088\n",
      "  validation accuracy:\t\t87.90 %\n",
      "Epoch 47 of 200 took 7.969s\n",
      "  training loss:\t\t0.513160\n",
      "  validation loss:\t\t0.364433\n",
      "  validation accuracy:\t\t87.80 %\n",
      "Epoch 48 of 200 took 8.010s\n",
      "  training loss:\t\t0.515571\n",
      "  validation loss:\t\t0.358857\n",
      "  validation accuracy:\t\t88.30 %\n",
      "Epoch 49 of 200 took 7.958s\n",
      "  training loss:\t\t0.506227\n",
      "  validation loss:\t\t0.356973\n",
      "  validation accuracy:\t\t87.60 %\n",
      "Epoch 50 of 200 took 8.028s\n",
      "  training loss:\t\t0.504404\n",
      "  validation loss:\t\t0.356055\n",
      "  validation accuracy:\t\t87.80 %\n",
      "Epoch 51 of 200 took 8.618s\n",
      "  training loss:\t\t0.499874\n",
      "  validation loss:\t\t0.349953\n",
      "  validation accuracy:\t\t88.40 %\n",
      "Epoch 52 of 200 took 8.004s\n",
      "  training loss:\t\t0.494682\n",
      "  validation loss:\t\t0.345837\n",
      "  validation accuracy:\t\t88.80 %\n",
      "Epoch 53 of 200 took 7.863s\n",
      "  training loss:\t\t0.493410\n",
      "  validation loss:\t\t0.341899\n",
      "  validation accuracy:\t\t88.50 %\n",
      "Epoch 54 of 200 took 8.019s\n",
      "  training loss:\t\t0.488507\n",
      "  validation loss:\t\t0.348550\n",
      "  validation accuracy:\t\t88.30 %\n",
      "Epoch 55 of 200 took 7.960s\n",
      "  training loss:\t\t0.489131\n",
      "  validation loss:\t\t0.345452\n",
      "  validation accuracy:\t\t88.20 %\n",
      "Epoch 56 of 200 took 10.684s\n",
      "  training loss:\t\t0.481225\n",
      "  validation loss:\t\t0.336836\n",
      "  validation accuracy:\t\t88.80 %\n",
      "Epoch 57 of 200 took 8.746s\n",
      "  training loss:\t\t0.484926\n",
      "  validation loss:\t\t0.345319\n",
      "  validation accuracy:\t\t88.60 %\n",
      "Epoch 58 of 200 took 7.845s\n",
      "  training loss:\t\t0.481040\n",
      "  validation loss:\t\t0.335683\n",
      "  validation accuracy:\t\t89.20 %\n",
      "Epoch 59 of 200 took 8.102s\n",
      "  training loss:\t\t0.476712\n",
      "  validation loss:\t\t0.332169\n",
      "  validation accuracy:\t\t88.90 %\n",
      "Epoch 60 of 200 took 8.064s\n",
      "  training loss:\t\t0.471323\n",
      "  validation loss:\t\t0.332087\n",
      "  validation accuracy:\t\t89.00 %\n",
      "Epoch 61 of 200 took 8.069s\n",
      "  training loss:\t\t0.471573\n",
      "  validation loss:\t\t0.335707\n",
      "  validation accuracy:\t\t89.00 %\n",
      "Epoch 62 of 200 took 7.898s\n",
      "  training loss:\t\t0.473301\n",
      "  validation loss:\t\t0.325137\n",
      "  validation accuracy:\t\t89.30 %\n",
      "Epoch 63 of 200 took 7.912s\n",
      "  training loss:\t\t0.464989\n",
      "  validation loss:\t\t0.327623\n",
      "  validation accuracy:\t\t89.10 %\n",
      "Epoch 64 of 200 took 8.869s\n",
      "  training loss:\t\t0.469914\n",
      "  validation loss:\t\t0.323339\n",
      "  validation accuracy:\t\t88.90 %\n",
      "Epoch 65 of 200 took 8.485s\n",
      "  training loss:\t\t0.459673\n",
      "  validation loss:\t\t0.323972\n",
      "  validation accuracy:\t\t89.10 %\n",
      "Epoch 66 of 200 took 8.137s\n",
      "  training loss:\t\t0.454176\n",
      "  validation loss:\t\t0.326826\n",
      "  validation accuracy:\t\t89.50 %\n",
      "Epoch 67 of 200 took 7.930s\n",
      "  training loss:\t\t0.458821\n",
      "  validation loss:\t\t0.318497\n",
      "  validation accuracy:\t\t88.60 %\n",
      "Epoch 68 of 200 took 7.906s\n",
      "  training loss:\t\t0.456204\n",
      "  validation loss:\t\t0.317079\n",
      "  validation accuracy:\t\t89.70 %\n",
      "Epoch 69 of 200 took 7.898s\n",
      "  training loss:\t\t0.456166\n",
      "  validation loss:\t\t0.318658\n",
      "  validation accuracy:\t\t89.60 %\n",
      "Epoch 70 of 200 took 7.840s\n",
      "  training loss:\t\t0.452490\n",
      "  validation loss:\t\t0.315842\n",
      "  validation accuracy:\t\t89.90 %\n",
      "Epoch 71 of 200 took 7.880s\n",
      "  training loss:\t\t0.450356\n",
      "  validation loss:\t\t0.318749\n",
      "  validation accuracy:\t\t88.90 %\n",
      "Epoch 72 of 200 took 7.925s\n",
      "  training loss:\t\t0.449792\n",
      "  validation loss:\t\t0.313747\n",
      "  validation accuracy:\t\t89.60 %\n",
      "Epoch 73 of 200 took 7.929s\n",
      "  training loss:\t\t0.446290\n",
      "  validation loss:\t\t0.307391\n",
      "  validation accuracy:\t\t90.20 %\n",
      "Epoch 74 of 200 took 8.073s\n",
      "  training loss:\t\t0.444232\n",
      "  validation loss:\t\t0.308841\n",
      "  validation accuracy:\t\t90.40 %\n",
      "Epoch 75 of 200 took 8.673s\n",
      "  training loss:\t\t0.444730\n",
      "  validation loss:\t\t0.309573\n",
      "  validation accuracy:\t\t89.20 %\n",
      "Epoch 76 of 200 took 8.645s\n",
      "  training loss:\t\t0.443216\n",
      "  validation loss:\t\t0.311829\n",
      "  validation accuracy:\t\t89.50 %\n",
      "Epoch 77 of 200 took 8.070s\n",
      "  training loss:\t\t0.442740\n",
      "  validation loss:\t\t0.306490\n",
      "  validation accuracy:\t\t90.10 %\n",
      "Epoch 78 of 200 took 7.958s\n",
      "  training loss:\t\t0.435878\n",
      "  validation loss:\t\t0.310933\n",
      "  validation accuracy:\t\t90.10 %\n",
      "Epoch 79 of 200 took 7.929s\n",
      "  training loss:\t\t0.436114\n",
      "  validation loss:\t\t0.307907\n",
      "  validation accuracy:\t\t89.60 %\n",
      "Epoch 80 of 200 took 7.962s\n",
      "  training loss:\t\t0.435992\n",
      "  validation loss:\t\t0.311693\n",
      "  validation accuracy:\t\t89.30 %\n",
      "Epoch 81 of 200 took 7.985s\n",
      "  training loss:\t\t0.432056\n",
      "  validation loss:\t\t0.302602\n",
      "  validation accuracy:\t\t89.70 %\n",
      "Epoch 82 of 200 took 7.975s\n",
      "  training loss:\t\t0.435950\n",
      "  validation loss:\t\t0.312273\n",
      "  validation accuracy:\t\t89.90 %\n",
      "Epoch 83 of 200 took 8.034s\n",
      "  training loss:\t\t0.428631\n",
      "  validation loss:\t\t0.308991\n",
      "  validation accuracy:\t\t89.50 %\n",
      "Epoch 84 of 200 took 7.977s\n",
      "  training loss:\t\t0.430015\n",
      "  validation loss:\t\t0.302660\n",
      "  validation accuracy:\t\t89.80 %\n",
      "Epoch 85 of 200 took 10.256s\n",
      "  training loss:\t\t0.423393\n",
      "  validation loss:\t\t0.304090\n",
      "  validation accuracy:\t\t89.70 %\n",
      "Epoch 86 of 200 took 9.775s\n",
      "  training loss:\t\t0.423529\n",
      "  validation loss:\t\t0.297681\n",
      "  validation accuracy:\t\t89.60 %\n",
      "Epoch 87 of 200 took 13.052s\n",
      "  training loss:\t\t0.423361\n",
      "  validation loss:\t\t0.297215\n",
      "  validation accuracy:\t\t90.00 %\n",
      "Epoch 88 of 200 took 8.678s\n",
      "  training loss:\t\t0.422578\n",
      "  validation loss:\t\t0.305163\n",
      "  validation accuracy:\t\t89.30 %\n",
      "Epoch 89 of 200 took 9.103s\n",
      "  training loss:\t\t0.420160\n",
      "  validation loss:\t\t0.301024\n",
      "  validation accuracy:\t\t89.90 %\n",
      "Epoch 90 of 200 took 9.044s\n",
      "  training loss:\t\t0.420225\n",
      "  validation loss:\t\t0.295890\n",
      "  validation accuracy:\t\t89.80 %\n",
      "Epoch 91 of 200 took 8.975s\n",
      "  training loss:\t\t0.418703\n",
      "  validation loss:\t\t0.292425\n",
      "  validation accuracy:\t\t90.30 %\n",
      "Epoch 92 of 200 took 7.897s\n",
      "  training loss:\t\t0.415662\n",
      "  validation loss:\t\t0.299264\n",
      "  validation accuracy:\t\t89.80 %\n",
      "Epoch 93 of 200 took 7.883s\n",
      "  training loss:\t\t0.414389\n",
      "  validation loss:\t\t0.288420\n",
      "  validation accuracy:\t\t90.60 %\n",
      "Epoch 94 of 200 took 7.934s\n",
      "  training loss:\t\t0.413478\n",
      "  validation loss:\t\t0.291816\n",
      "  validation accuracy:\t\t90.30 %\n",
      "Epoch 95 of 200 took 7.985s\n",
      "  training loss:\t\t0.414027\n",
      "  validation loss:\t\t0.291261\n",
      "  validation accuracy:\t\t90.70 %\n",
      "Epoch 96 of 200 took 8.235s\n",
      "  training loss:\t\t0.414219\n",
      "  validation loss:\t\t0.291490\n",
      "  validation accuracy:\t\t89.90 %\n",
      "Epoch 97 of 200 took 8.989s\n",
      "  training loss:\t\t0.410072\n",
      "  validation loss:\t\t0.289019\n",
      "  validation accuracy:\t\t90.10 %\n",
      "Epoch 98 of 200 took 9.308s\n",
      "  training loss:\t\t0.413978\n",
      "  validation loss:\t\t0.291146\n",
      "  validation accuracy:\t\t90.40 %\n",
      "Epoch 99 of 200 took 8.325s\n",
      "  training loss:\t\t0.409727\n",
      "  validation loss:\t\t0.285397\n",
      "  validation accuracy:\t\t90.30 %\n",
      "Epoch 100 of 200 took 7.774s\n",
      "  training loss:\t\t0.410346\n",
      "  validation loss:\t\t0.285334\n",
      "  validation accuracy:\t\t90.10 %\n",
      "Epoch 101 of 200 took 7.830s\n",
      "  training loss:\t\t0.402234\n",
      "  validation loss:\t\t0.289184\n",
      "  validation accuracy:\t\t90.80 %\n",
      "Epoch 102 of 200 took 7.844s\n",
      "  training loss:\t\t0.399734\n",
      "  validation loss:\t\t0.290944\n",
      "  validation accuracy:\t\t90.50 %\n",
      "Epoch 103 of 200 took 7.824s\n",
      "  training loss:\t\t0.405024\n",
      "  validation loss:\t\t0.287338\n",
      "  validation accuracy:\t\t90.20 %\n",
      "Epoch 104 of 200 took 7.781s\n",
      "  training loss:\t\t0.402059\n",
      "  validation loss:\t\t0.280184\n",
      "  validation accuracy:\t\t90.50 %\n",
      "Epoch 105 of 200 took 7.797s\n",
      "  training loss:\t\t0.400204\n",
      "  validation loss:\t\t0.286330\n",
      "  validation accuracy:\t\t90.40 %\n",
      "Epoch 106 of 200 took 7.853s\n",
      "  training loss:\t\t0.399984\n",
      "  validation loss:\t\t0.280068\n",
      "  validation accuracy:\t\t90.20 %\n",
      "Epoch 107 of 200 took 7.879s\n",
      "  training loss:\t\t0.400507\n",
      "  validation loss:\t\t0.287612\n",
      "  validation accuracy:\t\t89.80 %\n",
      "Epoch 108 of 200 took 7.944s\n",
      "  training loss:\t\t0.400491\n",
      "  validation loss:\t\t0.285328\n",
      "  validation accuracy:\t\t90.40 %\n",
      "Epoch 109 of 200 took 7.792s\n",
      "  training loss:\t\t0.397056\n",
      "  validation loss:\t\t0.283951\n",
      "  validation accuracy:\t\t90.70 %\n",
      "Epoch 110 of 200 took 7.842s\n",
      "  training loss:\t\t0.399696\n",
      "  validation loss:\t\t0.277493\n",
      "  validation accuracy:\t\t90.70 %\n",
      "Epoch 111 of 200 took 7.868s\n",
      "  training loss:\t\t0.396731\n",
      "  validation loss:\t\t0.280307\n",
      "  validation accuracy:\t\t90.30 %\n",
      "Epoch 112 of 200 took 7.860s\n",
      "  training loss:\t\t0.398557\n",
      "  validation loss:\t\t0.285331\n",
      "  validation accuracy:\t\t90.40 %\n",
      "Epoch 113 of 200 took 7.838s\n",
      "  training loss:\t\t0.393951\n",
      "  validation loss:\t\t0.277628\n",
      "  validation accuracy:\t\t90.80 %\n",
      "Epoch 114 of 200 took 8.198s\n",
      "  training loss:\t\t0.391325\n",
      "  validation loss:\t\t0.288298\n",
      "  validation accuracy:\t\t90.40 %\n",
      "Epoch 115 of 200 took 7.816s\n",
      "  training loss:\t\t0.392734\n",
      "  validation loss:\t\t0.281465\n",
      "  validation accuracy:\t\t90.50 %\n",
      "Epoch 116 of 200 took 7.836s\n",
      "  training loss:\t\t0.390734\n",
      "  validation loss:\t\t0.284017\n",
      "  validation accuracy:\t\t90.60 %\n",
      "Epoch 117 of 200 took 7.861s\n",
      "  training loss:\t\t0.389699\n",
      "  validation loss:\t\t0.290350\n",
      "  validation accuracy:\t\t90.00 %\n",
      "Epoch 118 of 200 took 9.099s\n",
      "  training loss:\t\t0.385452\n",
      "  validation loss:\t\t0.280311\n",
      "  validation accuracy:\t\t90.70 %\n",
      "Epoch 119 of 200 took 7.844s\n",
      "  training loss:\t\t0.390298\n",
      "  validation loss:\t\t0.276604\n",
      "  validation accuracy:\t\t90.90 %\n",
      "Epoch 120 of 200 took 7.820s\n",
      "  training loss:\t\t0.385493\n",
      "  validation loss:\t\t0.289027\n",
      "  validation accuracy:\t\t90.40 %\n",
      "Epoch 121 of 200 took 7.842s\n",
      "  training loss:\t\t0.389603\n",
      "  validation loss:\t\t0.279520\n",
      "  validation accuracy:\t\t90.80 %\n",
      "Epoch 122 of 200 took 7.883s\n",
      "  training loss:\t\t0.386605\n",
      "  validation loss:\t\t0.285030\n",
      "  validation accuracy:\t\t90.90 %\n",
      "Epoch 123 of 200 took 7.843s\n",
      "  training loss:\t\t0.384422\n",
      "  validation loss:\t\t0.282714\n",
      "  validation accuracy:\t\t90.60 %\n",
      "Epoch 124 of 200 took 7.571s\n",
      "  training loss:\t\t0.387431\n",
      "  validation loss:\t\t0.283115\n",
      "  validation accuracy:\t\t90.50 %\n",
      "Epoch 125 of 200 took 7.510s\n",
      "  training loss:\t\t0.384152\n",
      "  validation loss:\t\t0.278530\n",
      "  validation accuracy:\t\t90.60 %\n",
      "Epoch 126 of 200 took 7.580s\n",
      "  training loss:\t\t0.384076\n",
      "  validation loss:\t\t0.274406\n",
      "  validation accuracy:\t\t90.90 %\n",
      "Epoch 127 of 200 took 7.206s\n",
      "  training loss:\t\t0.383213\n",
      "  validation loss:\t\t0.273568\n",
      "  validation accuracy:\t\t90.80 %\n",
      "Epoch 128 of 200 took 7.168s\n",
      "  training loss:\t\t0.375583\n",
      "  validation loss:\t\t0.273834\n",
      "  validation accuracy:\t\t90.70 %\n",
      "Epoch 129 of 200 took 7.385s\n",
      "  training loss:\t\t0.381077\n",
      "  validation loss:\t\t0.280789\n",
      "  validation accuracy:\t\t90.50 %\n",
      "Epoch 130 of 200 took 7.468s\n",
      "  training loss:\t\t0.379621\n",
      "  validation loss:\t\t0.274906\n",
      "  validation accuracy:\t\t90.70 %\n",
      "Epoch 131 of 200 took 7.489s\n",
      "  training loss:\t\t0.378704\n",
      "  validation loss:\t\t0.270656\n",
      "  validation accuracy:\t\t91.60 %\n",
      "Epoch 132 of 200 took 7.485s\n",
      "  training loss:\t\t0.375595\n",
      "  validation loss:\t\t0.279986\n",
      "  validation accuracy:\t\t90.00 %\n",
      "Epoch 133 of 200 took 7.427s\n",
      "  training loss:\t\t0.378003\n",
      "  validation loss:\t\t0.273198\n",
      "  validation accuracy:\t\t91.40 %\n",
      "Epoch 134 of 200 took 7.464s\n",
      "  training loss:\t\t0.375492\n",
      "  validation loss:\t\t0.274035\n",
      "  validation accuracy:\t\t91.40 %\n",
      "Epoch 135 of 200 took 7.446s\n",
      "  training loss:\t\t0.376623\n",
      "  validation loss:\t\t0.274406\n",
      "  validation accuracy:\t\t91.00 %\n",
      "Epoch 136 of 200 took 7.485s\n",
      "  training loss:\t\t0.377876\n",
      "  validation loss:\t\t0.269615\n",
      "  validation accuracy:\t\t91.60 %\n",
      "Epoch 137 of 200 took 7.433s\n",
      "  training loss:\t\t0.376604\n",
      "  validation loss:\t\t0.278827\n",
      "  validation accuracy:\t\t90.60 %\n",
      "Epoch 138 of 200 took 7.457s\n",
      "  training loss:\t\t0.371555\n",
      "  validation loss:\t\t0.269520\n",
      "  validation accuracy:\t\t91.00 %\n",
      "Epoch 139 of 200 took 7.536s\n",
      "  training loss:\t\t0.372177\n",
      "  validation loss:\t\t0.268478\n",
      "  validation accuracy:\t\t91.10 %\n",
      "Epoch 140 of 200 took 7.580s\n",
      "  training loss:\t\t0.370374\n",
      "  validation loss:\t\t0.271552\n",
      "  validation accuracy:\t\t90.50 %\n",
      "Epoch 141 of 200 took 7.530s\n",
      "  training loss:\t\t0.367346\n",
      "  validation loss:\t\t0.267723\n",
      "  validation accuracy:\t\t91.00 %\n",
      "Epoch 142 of 200 took 7.596s\n",
      "  training loss:\t\t0.371943\n",
      "  validation loss:\t\t0.268235\n",
      "  validation accuracy:\t\t91.20 %\n",
      "Epoch 143 of 200 took 7.544s\n",
      "  training loss:\t\t0.372114\n",
      "  validation loss:\t\t0.271652\n",
      "  validation accuracy:\t\t90.70 %\n",
      "Epoch 144 of 200 took 7.578s\n",
      "  training loss:\t\t0.368002\n",
      "  validation loss:\t\t0.268095\n",
      "  validation accuracy:\t\t91.00 %\n",
      "Epoch 145 of 200 took 7.549s\n",
      "  training loss:\t\t0.370098\n",
      "  validation loss:\t\t0.269779\n",
      "  validation accuracy:\t\t90.80 %\n",
      "Epoch 146 of 200 took 7.505s\n",
      "  training loss:\t\t0.370526\n",
      "  validation loss:\t\t0.277641\n",
      "  validation accuracy:\t\t90.70 %\n",
      "Epoch 147 of 200 took 7.539s\n",
      "  training loss:\t\t0.370919\n",
      "  validation loss:\t\t0.271221\n",
      "  validation accuracy:\t\t90.80 %\n",
      "Epoch 148 of 200 took 7.503s\n",
      "  training loss:\t\t0.365335\n",
      "  validation loss:\t\t0.267682\n",
      "  validation accuracy:\t\t90.90 %\n",
      "Epoch 149 of 200 took 7.526s\n",
      "  training loss:\t\t0.363172\n",
      "  validation loss:\t\t0.269066\n",
      "  validation accuracy:\t\t91.10 %\n",
      "Epoch 150 of 200 took 7.551s\n",
      "  training loss:\t\t0.361934\n",
      "  validation loss:\t\t0.272134\n",
      "  validation accuracy:\t\t91.20 %\n",
      "Epoch 151 of 200 took 7.658s\n",
      "  training loss:\t\t0.366020\n",
      "  validation loss:\t\t0.269808\n",
      "  validation accuracy:\t\t90.80 %\n",
      "Epoch 152 of 200 took 7.675s\n",
      "  training loss:\t\t0.364365\n",
      "  validation loss:\t\t0.268318\n",
      "  validation accuracy:\t\t90.70 %\n",
      "Epoch 153 of 200 took 8.065s\n",
      "  training loss:\t\t0.365281\n",
      "  validation loss:\t\t0.260499\n",
      "  validation accuracy:\t\t91.30 %\n",
      "Epoch 154 of 200 took 8.847s\n",
      "  training loss:\t\t0.364552\n",
      "  validation loss:\t\t0.261566\n",
      "  validation accuracy:\t\t91.60 %\n",
      "Epoch 155 of 200 took 8.667s\n",
      "  training loss:\t\t0.364993\n",
      "  validation loss:\t\t0.264978\n",
      "  validation accuracy:\t\t91.50 %\n",
      "Epoch 156 of 200 took 8.255s\n",
      "  training loss:\t\t0.364009\n",
      "  validation loss:\t\t0.270239\n",
      "  validation accuracy:\t\t91.10 %\n",
      "Epoch 157 of 200 took 8.779s\n",
      "  training loss:\t\t0.365922\n",
      "  validation loss:\t\t0.275842\n",
      "  validation accuracy:\t\t90.60 %\n",
      "Epoch 158 of 200 took 8.708s\n",
      "  training loss:\t\t0.362660\n",
      "  validation loss:\t\t0.264397\n",
      "  validation accuracy:\t\t90.70 %\n",
      "Epoch 159 of 200 took 8.426s\n",
      "  training loss:\t\t0.359023\n",
      "  validation loss:\t\t0.262035\n",
      "  validation accuracy:\t\t91.40 %\n",
      "Epoch 160 of 200 took 8.199s\n",
      "  training loss:\t\t0.358491\n",
      "  validation loss:\t\t0.262696\n",
      "  validation accuracy:\t\t91.40 %\n",
      "Epoch 161 of 200 took 7.898s\n",
      "  training loss:\t\t0.358341\n",
      "  validation loss:\t\t0.263661\n",
      "  validation accuracy:\t\t90.80 %\n",
      "Epoch 162 of 200 took 8.438s\n",
      "  training loss:\t\t0.360890\n",
      "  validation loss:\t\t0.261573\n",
      "  validation accuracy:\t\t90.70 %\n",
      "Epoch 163 of 200 took 8.934s\n",
      "  training loss:\t\t0.362095\n",
      "  validation loss:\t\t0.278539\n",
      "  validation accuracy:\t\t90.50 %\n",
      "Epoch 164 of 200 took 8.127s\n",
      "  training loss:\t\t0.352311\n",
      "  validation loss:\t\t0.261367\n",
      "  validation accuracy:\t\t90.90 %\n",
      "Epoch 165 of 200 took 7.903s\n",
      "  training loss:\t\t0.359130\n",
      "  validation loss:\t\t0.258608\n",
      "  validation accuracy:\t\t91.70 %\n",
      "Epoch 166 of 200 took 7.794s\n",
      "  training loss:\t\t0.357826\n",
      "  validation loss:\t\t0.260801\n",
      "  validation accuracy:\t\t90.90 %\n",
      "Epoch 167 of 200 took 7.727s\n",
      "  training loss:\t\t0.359260\n",
      "  validation loss:\t\t0.258613\n",
      "  validation accuracy:\t\t91.10 %\n",
      "Epoch 168 of 200 took 7.768s\n",
      "  training loss:\t\t0.359229\n",
      "  validation loss:\t\t0.257153\n",
      "  validation accuracy:\t\t91.30 %\n",
      "Epoch 169 of 200 took 7.920s\n",
      "  training loss:\t\t0.358510\n",
      "  validation loss:\t\t0.265267\n",
      "  validation accuracy:\t\t90.80 %\n",
      "Epoch 170 of 200 took 7.748s\n",
      "  training loss:\t\t0.354825\n",
      "  validation loss:\t\t0.261816\n",
      "  validation accuracy:\t\t90.60 %\n",
      "Epoch 171 of 200 took 7.835s\n",
      "  training loss:\t\t0.354028\n",
      "  validation loss:\t\t0.254512\n",
      "  validation accuracy:\t\t91.20 %\n",
      "Epoch 172 of 200 took 7.883s\n",
      "  training loss:\t\t0.356429\n",
      "  validation loss:\t\t0.258156\n",
      "  validation accuracy:\t\t91.10 %\n",
      "Epoch 173 of 200 took 7.790s\n",
      "  training loss:\t\t0.357418\n",
      "  validation loss:\t\t0.263976\n",
      "  validation accuracy:\t\t90.80 %\n",
      "Epoch 174 of 200 took 7.837s\n",
      "  training loss:\t\t0.354270\n",
      "  validation loss:\t\t0.268439\n",
      "  validation accuracy:\t\t90.50 %\n",
      "Epoch 175 of 200 took 7.975s\n",
      "  training loss:\t\t0.356840\n",
      "  validation loss:\t\t0.258394\n",
      "  validation accuracy:\t\t91.50 %\n",
      "Epoch 176 of 200 took 7.817s\n",
      "  training loss:\t\t0.354581\n",
      "  validation loss:\t\t0.266672\n",
      "  validation accuracy:\t\t91.30 %\n",
      "Epoch 177 of 200 took 7.849s\n",
      "  training loss:\t\t0.358252\n",
      "  validation loss:\t\t0.258606\n",
      "  validation accuracy:\t\t91.40 %\n",
      "Epoch 178 of 200 took 7.843s\n",
      "  training loss:\t\t0.356251\n",
      "  validation loss:\t\t0.258998\n",
      "  validation accuracy:\t\t91.50 %\n",
      "Epoch 179 of 200 took 7.817s\n",
      "  training loss:\t\t0.351007\n",
      "  validation loss:\t\t0.254148\n",
      "  validation accuracy:\t\t91.60 %\n",
      "Epoch 180 of 200 took 7.897s\n",
      "  training loss:\t\t0.348100\n",
      "  validation loss:\t\t0.258240\n",
      "  validation accuracy:\t\t91.10 %\n",
      "Epoch 181 of 200 took 7.823s\n",
      "  training loss:\t\t0.352854\n",
      "  validation loss:\t\t0.252768\n",
      "  validation accuracy:\t\t91.30 %\n",
      "Epoch 182 of 200 took 7.794s\n",
      "  training loss:\t\t0.350328\n",
      "  validation loss:\t\t0.254068\n",
      "  validation accuracy:\t\t91.70 %\n",
      "Epoch 183 of 200 took 7.818s\n",
      "  training loss:\t\t0.347259\n",
      "  validation loss:\t\t0.255473\n",
      "  validation accuracy:\t\t91.40 %\n",
      "Epoch 184 of 200 took 7.813s\n",
      "  training loss:\t\t0.351603\n",
      "  validation loss:\t\t0.261176\n",
      "  validation accuracy:\t\t90.80 %\n",
      "Epoch 185 of 200 took 8.430s\n",
      "  training loss:\t\t0.350030\n",
      "  validation loss:\t\t0.259364\n",
      "  validation accuracy:\t\t91.10 %\n",
      "Epoch 186 of 200 took 7.852s\n",
      "  training loss:\t\t0.350498\n",
      "  validation loss:\t\t0.250818\n",
      "  validation accuracy:\t\t91.30 %\n",
      "Epoch 187 of 200 took 7.818s\n",
      "  training loss:\t\t0.344846\n",
      "  validation loss:\t\t0.258339\n",
      "  validation accuracy:\t\t91.40 %\n",
      "Epoch 188 of 200 took 7.850s\n",
      "  training loss:\t\t0.343325\n",
      "  validation loss:\t\t0.253755\n",
      "  validation accuracy:\t\t91.50 %\n",
      "Epoch 189 of 200 took 7.921s\n",
      "  training loss:\t\t0.344295\n",
      "  validation loss:\t\t0.257776\n",
      "  validation accuracy:\t\t91.20 %\n",
      "Epoch 190 of 200 took 7.834s\n",
      "  training loss:\t\t0.344876\n",
      "  validation loss:\t\t0.252243\n",
      "  validation accuracy:\t\t91.30 %\n",
      "Epoch 191 of 200 took 7.857s\n",
      "  training loss:\t\t0.345022\n",
      "  validation loss:\t\t0.258270\n",
      "  validation accuracy:\t\t91.40 %\n",
      "Epoch 192 of 200 took 7.874s\n",
      "  training loss:\t\t0.349056\n",
      "  validation loss:\t\t0.253664\n",
      "  validation accuracy:\t\t91.10 %\n",
      "Epoch 193 of 200 took 7.840s\n",
      "  training loss:\t\t0.346610\n",
      "  validation loss:\t\t0.250787\n",
      "  validation accuracy:\t\t91.10 %\n",
      "Epoch 194 of 200 took 7.818s\n",
      "  training loss:\t\t0.343192\n",
      "  validation loss:\t\t0.255021\n",
      "  validation accuracy:\t\t91.10 %\n",
      "Epoch 195 of 200 took 7.804s\n",
      "  training loss:\t\t0.338817\n",
      "  validation loss:\t\t0.256543\n",
      "  validation accuracy:\t\t91.40 %\n",
      "Epoch 196 of 200 took 7.852s\n",
      "  training loss:\t\t0.339574\n",
      "  validation loss:\t\t0.251150\n",
      "  validation accuracy:\t\t91.40 %\n",
      "Epoch 197 of 200 took 7.787s\n",
      "  training loss:\t\t0.341325\n",
      "  validation loss:\t\t0.245383\n",
      "  validation accuracy:\t\t91.50 %\n",
      "Epoch 198 of 200 took 7.785s\n",
      "  training loss:\t\t0.341320\n",
      "  validation loss:\t\t0.255006\n",
      "  validation accuracy:\t\t91.50 %\n",
      "Epoch 199 of 200 took 7.785s\n",
      "  training loss:\t\t0.346188\n",
      "  validation loss:\t\t0.253418\n",
      "  validation accuracy:\t\t91.70 %\n",
      "Epoch 200 of 200 took 7.782s\n",
      "  training loss:\t\t0.341096\n",
      "  validation loss:\t\t0.243866\n",
      "  validation accuracy:\t\t92.10 %\n",
      "Final results:\n",
      "  test loss:\t\t\t6.683797\n",
      "  test accuracy:\t\t19.25 %\n"
     ]
    }
   ],
   "source": [
    "train_cnn(num_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x113d3ef50>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEACAYAAABI5zaHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113d62c10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scatter(arange(len(percs2)),percs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
