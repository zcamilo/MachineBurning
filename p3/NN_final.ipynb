{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'train']\n",
      "[u'test']\n"
     ]
    }
   ],
   "source": [
    "f_train = h5py.File('train.h5','r')\n",
    "f_test = h5py.File('test.h5','r')\n",
    "print(f_train.keys())\n",
    "print(f_test.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'axis0', u'axis1', u'block0_items', u'block0_values', u'block1_items', u'block1_values']\n",
      "[u'axis0', u'axis1', u'block0_items', u'block0_values']\n"
     ]
    }
   ],
   "source": [
    "a_train = f_train['train']\n",
    "a_test = f_test['test']\n",
    "print(a_train.keys())\n",
    "print(a_test.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45324, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a_train['block1_values']\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# axis0 -> Labels [shape (101,)]\n",
    "# axis1 -> id column [shape (45324,)]\n",
    "# block0_items -> traits labels [shape (100,)]\n",
    "# block0_values -> traits values [shape (45324,100)]\n",
    "# block1_items -> 'y' label\n",
    "# block1_values -> y column (without label) [shape (45324,1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_tr = a_train['block0_values'][()]\n",
    "y_train = a_train['block1_values'][()]\n",
    "y_train = y_train[:,0]\n",
    "X_te = a_test['block0_values'][()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Preprocessing\n",
    "#Shuffle\n",
    "c = zip(y_train, X_tr)\n",
    "random.shuffle(c)\n",
    "y_train, X_tr = zip(*c)\n",
    "y_train = asarray(y_train)\n",
    "X_tr = asarray(X_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_tr, X_va = X_tr[:-5000], X_tr[-5000:]\n",
    "y_train, y_val = y_train[:-5000], y_train[-5000:]\n",
    "ids = a_test['axis1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Are there zero columns?\n",
    "def is_zero_vector(x):\n",
    "    count = 0\n",
    "    for i in arange(len(x)):\n",
    "        if x[i] == 0.:\n",
    "            count += 1\n",
    "    if count == len(x):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zero_i = list([])\n",
    "for k in arange(len(X_tr[0,:])):   \n",
    "    if is_zero_vector(X_tr[:,k]):\n",
    "        zero_i.append(k)\n",
    "X_tr = delete(X_tr, zero_i, 1)\n",
    "X_va = delete(X_va, zero_i, 1)\n",
    "X_te = delete(X_te, zero_i, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40324, 90)\n",
      "(8137, 90)\n",
      "(5000, 90)\n"
     ]
    }
   ],
   "source": [
    "print(X_tr.shape)\n",
    "print(X_te.shape)\n",
    "print(X_va.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_va -= np.mean(X_tr, axis = 0)\n",
    "X_te -= np.mean(X_tr, axis = 0)\n",
    "X_tr -= np.mean(X_tr, axis = 0)\n",
    "\n",
    "X_va /= np.std(X_tr, axis = 0)\n",
    "X_te /= np.std(X_tr, axis = 0)\n",
    "X_tr /= np.std(X_tr, axis = 0)\n",
    "\n",
    "#print(is_zero_vector(X_tr[:,4]))\n",
    "#print(np.std(X_tr, axis = 0))\n",
    "#print(X_tr[:,4])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#X_train_c = X_train.reshape(len(X_train),1,20,5)\n",
    "#X_val_c = X_val.reshape(len(X_val),1,20,5)\n",
    "#X_test_c = X_test.reshape(len(X_test),1,20,5) \n",
    "\n",
    "if len(X_tr[0,:]) == 91:\n",
    "    new_col = ones(len(X_tr))\n",
    "    new_col = new_col.reshape((len(X_tr),1))\n",
    "    X_train = np.append(new_col,X_tr, 1)\n",
    "if len(X_va[0,:]) == 91:\n",
    "    new_col = ones(len(X_va))\n",
    "    new_col = new_col.reshape((len(X_va),1))\n",
    "    X_val = np.append(new_col,X_va, 1)\n",
    "if len(X_te[0,:]) == 91:\n",
    "    new_col = ones(len(X_te))\n",
    "    new_col = new_col.reshape((len(X_te),1))\n",
    "    X_test = np.append(new_col,X_te, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_tr\n",
    "X_test = X_te\n",
    "X_val = X_va"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "-0.101106\n",
      "(40324, 90)\n",
      "<type 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train[0,:]))\n",
    "print(X_train[1,0])\n",
    "print(X_train.shape)\n",
    "print(type(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x108281e90>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAEACAYAAABWLgY0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ne9gAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1038599d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scatter(arange(100), y_train[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from theano import *\n",
    "import theano.tensor as T\n",
    "from __future__ import print_function\n",
    "import time\n",
    "import lasagne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_NN(num_epochs=500, width = 100, d_i = 0.3, d_h = 0.5, l_r = 0.01, la1p = 0.1, la2p = 0.1, la3p = 0.5):\n",
    "    \n",
    "    input_var = T.matrix('inputs')\n",
    "    target_var = T.ivector('targets')\n",
    "    \n",
    "    print(\"Building model and compiling functions...\")\n",
    "    \n",
    "    layer_in = lasagne.layers.InputLayer(shape=(None, 90), input_var=input_var)\n",
    "    if d_i:\n",
    "        layer_in = lasagne.layers.dropout(layer_in, p=d_i)\n",
    "    \n",
    "    # Hidden layers and dropout:\n",
    "    \n",
    "    nonlin = lasagne.nonlinearities.rectify\n",
    "    layer1 = lasagne.layers.DenseLayer(layer_in, width, nonlinearity=nonlin)\n",
    "    if d_h:\n",
    "        layer1 = lasagne.layers.dropout(layer1, p=d_h)\n",
    "        \n",
    "    layer2 = lasagne.layers.DenseLayer(layer1, width, nonlinearity=nonlin)\n",
    "    if d_h:\n",
    "        layer2 = lasagne.layers.dropout(layer2, p=d_h)\n",
    "        \n",
    "    #layer3 = lasagne.layers.DenseLayer(layer2, width, nonlinearity=nonlin)\n",
    "    #if d_h:\n",
    "    #    layer3 = lasagne.layers.dropout(layer3, p=d_h)\n",
    "        \n",
    "    #layer4 = lasagne.layers.DenseLayer(layer3, width, nonlinearity=nonlin)\n",
    "    #if d_h:\n",
    "    #    layer4 = lasagne.layers.dropout(layer4, p=d_h)\n",
    "        \n",
    "    # Output layer:\n",
    "    softmax = lasagne.nonlinearities.softmax\n",
    "    network = lasagne.layers.DenseLayer(layer2, 5, nonlinearity=softmax)\n",
    "    \n",
    "    # Create a loss expression for training, i.e., a scalar objective we want\n",
    "    # to minimize (for our multi-class problem, it is the cross-entropy loss):\n",
    "    \n",
    "    prediction = lasagne.layers.get_output(network)\n",
    "    loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "    #layers = {layer1: la1p, layer2: la2p, network: la3p}\n",
    "    #l2_penalty_1 = lasagne.regularization.regularize_layer_params_weighted({layer1:la1p}, lasagne.regularization.l2)\n",
    "    #l2_penalty = lasagne.regularization.regularize_layer_params(network, lasagne.regularization.l2)*1e-03\n",
    "    #loss = loss.mean() + l2_penalty + l1_penalty\n",
    "    loss = loss.mean()\n",
    "    \n",
    "    # We could add some weight decay as well here, see lasagne.regularization.\n",
    "\n",
    "    # Create update expressions for training, i.e., how to modify the\n",
    "    # parameters at each training step. Here, we'll use Stochastic Gradient\n",
    "    # Descent (SGD) with Nesterov momentum, but Lasagne offers plenty more.\n",
    "    \n",
    "    params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "    updates = lasagne.updates.nesterov_momentum(loss, params, learning_rate=l_r, momentum=0.9)\n",
    "    \n",
    "    # Create a loss expression for validation/testing. The crucial difference\n",
    "    # here is that we do a deterministic forward pass through the network,\n",
    "    # disabling dropout layers.\n",
    "    \n",
    "    test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "    test_loss = lasagne.objectives.categorical_crossentropy(test_prediction,target_var)\n",
    "    #test_loss = test_loss.mean() + l2_penalty + l1_penalty\n",
    "    test_loss = test_loss.mean()\n",
    "    \n",
    "    # As a bonus, also create an expression for the classification accuracy:\n",
    "    \n",
    "    test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),dtype=theano.config.floatX)\n",
    "\n",
    "    # Compile a function performing a training step on a mini-batch (by giving\n",
    "    # the updates dictionary) and returning the corresponding training loss:\n",
    "    train_fn = theano.function([input_var, target_var], loss, updates=updates, allow_input_downcast=True)\n",
    "\n",
    "    # Compile a second function computing the validation loss and accuracy:\n",
    "    val_fn = theano.function([input_var, target_var], [test_loss, test_acc], allow_input_downcast=True)\n",
    "\n",
    "    # Finally, launch the training loop.\n",
    "    print(\"Starting training...\")\n",
    "    # We iterate over epochs:\n",
    "    global percs\n",
    "    percs = array([])\n",
    "    for epoch in range(num_epochs):\n",
    "        # In each epoch, we do a full pass over the training data:\n",
    "        train_err = 0\n",
    "        train_batches = 0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for batch in iterate_minibatches(X_train, y_train, 500, shuffle=True):\n",
    "            inputs, targets = batch\n",
    "            train_err += train_fn(inputs, targets)\n",
    "            train_batches += 1\n",
    "\n",
    "        # And a full pass over the validation data:\n",
    "        val_err = 0\n",
    "        val_acc = 0\n",
    "        val_batches = 0\n",
    "        for batch in iterate_minibatches(X_val, y_val, 500, shuffle=False):\n",
    "            inputs, targets = batch\n",
    "            err, acc = val_fn(inputs, targets)\n",
    "            val_err += err\n",
    "            val_acc += acc\n",
    "            val_batches += 1\n",
    "\n",
    "        # Then we print the results for this epoch:\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(epoch + 1, num_epochs, time.time() - start_time))\n",
    "        print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "        print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "        perc = val_acc / val_batches\n",
    "        percs = append(percs, perc)\n",
    "        print(\"  validation accuracy:\\t\\t{:.2f} %\".format(perc * 100))\n",
    "\n",
    "    # After training, we compute and print the test error:\n",
    "    test_err = 0\n",
    "    test_acc = 0\n",
    "    test_batches = 0\n",
    "    for batch in iterate_minibatches(X_test, zeros(len(X_test)), 500, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        err, acc = val_fn(inputs, targets)\n",
    "        test_err += err\n",
    "        test_acc += acc\n",
    "        test_batches += 1\n",
    "    print(\"Final results:\")\n",
    "    print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "    print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "        test_acc / test_batches * 100))\n",
    "    print(\"Percentages average:\\t\\t\\t{:.5f}\".format(mean(percs)))\n",
    "    print(\"\\a\")\n",
    "    global result\n",
    "    result = lasagne.layers.get_output(network, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model and compiling functions...\n",
      "Starting training...\n",
      "Epoch 1 of 500 took 1.419s\n",
      "  training loss:\t\t0.501720\n",
      "  validation loss:\t\t0.346436\n",
      "  validation accuracy:\t\t90.92 %\n",
      "Epoch 2 of 500 took 1.418s\n",
      "  training loss:\t\t0.300597\n",
      "  validation loss:\t\t0.329439\n",
      "  validation accuracy:\t\t92.44 %\n",
      "Epoch 3 of 500 took 1.393s\n",
      "  training loss:\t\t0.263717\n",
      "  validation loss:\t\t0.235182\n",
      "  validation accuracy:\t\t93.40 %\n",
      "Epoch 4 of 500 took 1.391s\n",
      "  training loss:\t\t0.238578\n",
      "  validation loss:\t\t0.220453\n",
      "  validation accuracy:\t\t93.62 %\n",
      "Epoch 5 of 500 took 1.424s\n",
      "  training loss:\t\t0.223651\n",
      "  validation loss:\t\t0.245173\n",
      "  validation accuracy:\t\t93.98 %\n",
      "Epoch 6 of 500 took 1.412s\n",
      "  training loss:\t\t0.210002\n",
      "  validation loss:\t\t0.194501\n",
      "  validation accuracy:\t\t94.36 %\n",
      "Epoch 7 of 500 took 1.393s\n",
      "  training loss:\t\t0.201228\n",
      "  validation loss:\t\t0.238338\n",
      "  validation accuracy:\t\t94.50 %\n",
      "Epoch 8 of 500 took 1.456s\n",
      "  training loss:\t\t0.184029\n",
      "  validation loss:\t\t0.284045\n",
      "  validation accuracy:\t\t94.52 %\n",
      "Epoch 9 of 500 took 1.474s\n",
      "  training loss:\t\t0.188934\n",
      "  validation loss:\t\t0.229020\n",
      "  validation accuracy:\t\t94.84 %\n",
      "Epoch 10 of 500 took 1.452s\n",
      "  training loss:\t\t0.179030\n",
      "  validation loss:\t\t0.183423\n",
      "  validation accuracy:\t\t94.72 %\n",
      "Epoch 11 of 500 took 1.632s\n",
      "  training loss:\t\t0.174702\n",
      "  validation loss:\t\t0.213193\n",
      "  validation accuracy:\t\t94.92 %\n",
      "Epoch 12 of 500 took 2.365s\n",
      "  training loss:\t\t0.164716\n",
      "  validation loss:\t\t0.166247\n",
      "  validation accuracy:\t\t94.84 %\n",
      "Epoch 13 of 500 took 1.849s\n",
      "  training loss:\t\t0.163131\n",
      "  validation loss:\t\t0.170102\n",
      "  validation accuracy:\t\t95.38 %\n",
      "Epoch 14 of 500 took 1.929s\n",
      "  training loss:\t\t0.156101\n",
      "  validation loss:\t\t0.208192\n",
      "  validation accuracy:\t\t94.84 %\n",
      "Epoch 15 of 500 took 2.005s\n",
      "  training loss:\t\t0.160098\n",
      "  validation loss:\t\t0.192105\n",
      "  validation accuracy:\t\t94.98 %\n",
      "Epoch 16 of 500 took 1.729s\n",
      "  training loss:\t\t0.146983\n",
      "  validation loss:\t\t0.165245\n",
      "  validation accuracy:\t\t95.16 %\n",
      "Epoch 17 of 500 took 1.679s\n",
      "  training loss:\t\t0.140765\n",
      "  validation loss:\t\t0.159843\n",
      "  validation accuracy:\t\t95.28 %\n",
      "Epoch 18 of 500 took 1.402s\n",
      "  training loss:\t\t0.143904\n",
      "  validation loss:\t\t0.174818\n",
      "  validation accuracy:\t\t95.06 %\n",
      "Epoch 19 of 500 took 1.483s\n",
      "  training loss:\t\t0.133432\n",
      "  validation loss:\t\t0.164801\n",
      "  validation accuracy:\t\t95.32 %\n",
      "Epoch 20 of 500 took 1.452s\n",
      "  training loss:\t\t0.127731\n",
      "  validation loss:\t\t0.164649\n",
      "  validation accuracy:\t\t95.30 %\n",
      "Epoch 21 of 500 took 1.585s\n",
      "  training loss:\t\t0.124742\n",
      "  validation loss:\t\t0.160668\n",
      "  validation accuracy:\t\t95.48 %\n",
      "Epoch 22 of 500 took 2.530s\n",
      "  training loss:\t\t0.124927\n",
      "  validation loss:\t\t0.172143\n",
      "  validation accuracy:\t\t95.30 %\n",
      "Epoch 23 of 500 took 3.143s\n",
      "  training loss:\t\t0.119308\n",
      "  validation loss:\t\t0.173480\n",
      "  validation accuracy:\t\t95.28 %\n",
      "Epoch 24 of 500 took 2.868s\n",
      "  training loss:\t\t0.114745\n",
      "  validation loss:\t\t0.166523\n",
      "  validation accuracy:\t\t95.34 %\n",
      "Epoch 25 of 500 took 2.296s\n",
      "  training loss:\t\t0.115428\n",
      "  validation loss:\t\t0.171042\n",
      "  validation accuracy:\t\t95.40 %\n",
      "Epoch 26 of 500 took 1.946s\n",
      "  training loss:\t\t0.117371\n",
      "  validation loss:\t\t0.166801\n",
      "  validation accuracy:\t\t95.28 %\n",
      "Epoch 27 of 500 took 2.276s\n",
      "  training loss:\t\t0.111572\n",
      "  validation loss:\t\t0.162287\n",
      "  validation accuracy:\t\t95.56 %\n",
      "Epoch 28 of 500 took 1.789s\n",
      "  training loss:\t\t0.108494\n",
      "  validation loss:\t\t0.214170\n",
      "  validation accuracy:\t\t95.46 %\n",
      "Epoch 29 of 500 took 1.853s\n",
      "  training loss:\t\t0.106475\n",
      "  validation loss:\t\t0.160831\n",
      "  validation accuracy:\t\t95.28 %\n",
      "Epoch 30 of 500 took 2.838s\n",
      "  training loss:\t\t0.104612\n",
      "  validation loss:\t\t0.183522\n",
      "  validation accuracy:\t\t95.42 %\n",
      "Epoch 31 of 500 took 2.699s\n",
      "  training loss:\t\t0.101853\n",
      "  validation loss:\t\t0.188460\n",
      "  validation accuracy:\t\t95.28 %\n",
      "Epoch 32 of 500 took 1.794s\n",
      "  training loss:\t\t0.103166\n",
      "  validation loss:\t\t0.152337\n",
      "  validation accuracy:\t\t95.32 %\n",
      "Epoch 33 of 500 took 1.493s\n",
      "  training loss:\t\t0.102109\n",
      "  validation loss:\t\t0.185957\n",
      "  validation accuracy:\t\t95.20 %\n",
      "Epoch 34 of 500 took 1.602s\n",
      "  training loss:\t\t0.098267\n",
      "  validation loss:\t\t0.155469\n",
      "  validation accuracy:\t\t95.66 %\n",
      "Epoch 35 of 500 took 1.774s\n",
      "  training loss:\t\t0.096307\n",
      "  validation loss:\t\t0.155319\n",
      "  validation accuracy:\t\t95.62 %\n",
      "Epoch 36 of 500 took 1.589s\n",
      "  training loss:\t\t0.093317\n",
      "  validation loss:\t\t0.149916\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 37 of 500 took 1.621s\n",
      "  training loss:\t\t0.090662\n",
      "  validation loss:\t\t0.151627\n",
      "  validation accuracy:\t\t95.62 %\n",
      "Epoch 38 of 500 took 1.504s\n",
      "  training loss:\t\t0.091008\n",
      "  validation loss:\t\t0.149334\n",
      "  validation accuracy:\t\t95.54 %\n",
      "Epoch 39 of 500 took 1.595s\n",
      "  training loss:\t\t0.089901\n",
      "  validation loss:\t\t0.152343\n",
      "  validation accuracy:\t\t95.90 %\n",
      "Epoch 40 of 500 took 1.545s\n",
      "  training loss:\t\t0.091570\n",
      "  validation loss:\t\t0.269710\n",
      "  validation accuracy:\t\t95.64 %\n",
      "Epoch 41 of 500 took 1.488s\n",
      "  training loss:\t\t0.086969\n",
      "  validation loss:\t\t0.156401\n",
      "  validation accuracy:\t\t95.46 %\n",
      "Epoch 42 of 500 took 1.755s\n",
      "  training loss:\t\t0.087628\n",
      "  validation loss:\t\t0.244294\n",
      "  validation accuracy:\t\t95.48 %\n",
      "Epoch 43 of 500 took 1.571s\n",
      "  training loss:\t\t0.083923\n",
      "  validation loss:\t\t0.166506\n",
      "  validation accuracy:\t\t95.60 %\n",
      "Epoch 44 of 500 took 1.820s\n",
      "  training loss:\t\t0.087054\n",
      "  validation loss:\t\t0.170723\n",
      "  validation accuracy:\t\t95.38 %\n",
      "Epoch 45 of 500 took 1.733s\n",
      "  training loss:\t\t0.084172\n",
      "  validation loss:\t\t0.236845\n",
      "  validation accuracy:\t\t95.54 %\n",
      "Epoch 46 of 500 took 1.700s\n",
      "  training loss:\t\t0.082712\n",
      "  validation loss:\t\t0.198606\n",
      "  validation accuracy:\t\t95.52 %\n",
      "Epoch 47 of 500 took 1.562s\n",
      "  training loss:\t\t0.083071\n",
      "  validation loss:\t\t0.156878\n",
      "  validation accuracy:\t\t95.54 %\n",
      "Epoch 48 of 500 took 1.577s\n",
      "  training loss:\t\t0.080991\n",
      "  validation loss:\t\t0.158853\n",
      "  validation accuracy:\t\t95.56 %\n",
      "Epoch 49 of 500 took 1.503s\n",
      "  training loss:\t\t0.078206\n",
      "  validation loss:\t\t0.160239\n",
      "  validation accuracy:\t\t95.64 %\n",
      "Epoch 50 of 500 took 1.687s\n",
      "  training loss:\t\t0.080584\n",
      "  validation loss:\t\t0.236166\n",
      "  validation accuracy:\t\t95.68 %\n",
      "Epoch 51 of 500 took 1.485s\n",
      "  training loss:\t\t0.080605\n",
      "  validation loss:\t\t0.284585\n",
      "  validation accuracy:\t\t95.46 %\n",
      "Epoch 52 of 500 took 1.591s\n",
      "  training loss:\t\t0.079352\n",
      "  validation loss:\t\t0.208758\n",
      "  validation accuracy:\t\t95.60 %\n",
      "Epoch 53 of 500 took 1.572s\n",
      "  training loss:\t\t0.076188\n",
      "  validation loss:\t\t0.176454\n",
      "  validation accuracy:\t\t95.66 %\n",
      "Epoch 54 of 500 took 2.101s\n",
      "  training loss:\t\t0.074081\n",
      "  validation loss:\t\t0.176204\n",
      "  validation accuracy:\t\t95.52 %\n",
      "Epoch 55 of 500 took 1.650s\n",
      "  training loss:\t\t0.074245\n",
      "  validation loss:\t\t0.201750\n",
      "  validation accuracy:\t\t95.62 %\n",
      "Epoch 56 of 500 took 2.110s\n",
      "  training loss:\t\t0.074854\n",
      "  validation loss:\t\t0.155322\n",
      "  validation accuracy:\t\t95.46 %\n",
      "Epoch 57 of 500 took 2.219s\n",
      "  training loss:\t\t0.070002\n",
      "  validation loss:\t\t0.230994\n",
      "  validation accuracy:\t\t95.62 %\n",
      "Epoch 58 of 500 took 1.541s\n",
      "  training loss:\t\t0.074727\n",
      "  validation loss:\t\t0.254016\n",
      "  validation accuracy:\t\t95.68 %\n",
      "Epoch 59 of 500 took 1.536s\n",
      "  training loss:\t\t0.071498\n",
      "  validation loss:\t\t0.214354\n",
      "  validation accuracy:\t\t95.52 %\n",
      "Epoch 60 of 500 took 1.611s\n",
      "  training loss:\t\t0.070274\n",
      "  validation loss:\t\t0.189402\n",
      "  validation accuracy:\t\t95.46 %\n",
      "Epoch 61 of 500 took 1.594s\n",
      "  training loss:\t\t0.069289\n",
      "  validation loss:\t\t0.156883\n",
      "  validation accuracy:\t\t95.56 %\n",
      "Epoch 62 of 500 took 1.619s\n",
      "  training loss:\t\t0.068759\n",
      "  validation loss:\t\t0.188303\n",
      "  validation accuracy:\t\t95.58 %\n",
      "Epoch 63 of 500 took 1.531s\n",
      "  training loss:\t\t0.068145\n",
      "  validation loss:\t\t0.188665\n",
      "  validation accuracy:\t\t95.44 %\n",
      "Epoch 64 of 500 took 1.502s\n",
      "  training loss:\t\t0.067748\n",
      "  validation loss:\t\t0.187058\n",
      "  validation accuracy:\t\t95.44 %\n",
      "Epoch 65 of 500 took 1.522s\n",
      "  training loss:\t\t0.064566\n",
      "  validation loss:\t\t0.189362\n",
      "  validation accuracy:\t\t95.64 %\n",
      "Epoch 66 of 500 took 1.666s\n",
      "  training loss:\t\t0.065596\n",
      "  validation loss:\t\t0.164032\n",
      "  validation accuracy:\t\t95.92 %\n",
      "Epoch 67 of 500 took 1.587s\n",
      "  training loss:\t\t0.067269\n",
      "  validation loss:\t\t0.162332\n",
      "  validation accuracy:\t\t95.66 %\n",
      "Epoch 68 of 500 took 1.599s\n",
      "  training loss:\t\t0.069060\n",
      "  validation loss:\t\t0.162982\n",
      "  validation accuracy:\t\t95.62 %\n",
      "Epoch 69 of 500 took 1.791s\n",
      "  training loss:\t\t0.063418\n",
      "  validation loss:\t\t0.166420\n",
      "  validation accuracy:\t\t95.60 %\n",
      "Epoch 70 of 500 took 1.498s\n",
      "  training loss:\t\t0.062854\n",
      "  validation loss:\t\t0.170968\n",
      "  validation accuracy:\t\t95.50 %\n",
      "Epoch 71 of 500 took 1.523s\n",
      "  training loss:\t\t0.063320\n",
      "  validation loss:\t\t0.170669\n",
      "  validation accuracy:\t\t95.68 %\n",
      "Epoch 72 of 500 took 1.595s\n",
      "  training loss:\t\t0.059248\n",
      "  validation loss:\t\t0.170207\n",
      "  validation accuracy:\t\t95.84 %\n",
      "Epoch 73 of 500 took 1.571s\n",
      "  training loss:\t\t0.060886\n",
      "  validation loss:\t\t0.169033\n",
      "  validation accuracy:\t\t95.44 %\n",
      "Epoch 74 of 500 took 1.662s\n",
      "  training loss:\t\t0.060294\n",
      "  validation loss:\t\t0.220219\n",
      "  validation accuracy:\t\t95.56 %\n",
      "Epoch 75 of 500 took 1.553s\n",
      "  training loss:\t\t0.059067\n",
      "  validation loss:\t\t0.192055\n",
      "  validation accuracy:\t\t95.56 %\n",
      "Epoch 76 of 500 took 1.575s\n",
      "  training loss:\t\t0.056651\n",
      "  validation loss:\t\t0.188060\n",
      "  validation accuracy:\t\t95.60 %\n",
      "Epoch 77 of 500 took 1.603s\n",
      "  training loss:\t\t0.059965\n",
      "  validation loss:\t\t0.185037\n",
      "  validation accuracy:\t\t95.42 %\n",
      "Epoch 78 of 500 took 1.605s\n",
      "  training loss:\t\t0.059639\n",
      "  validation loss:\t\t0.183834\n",
      "  validation accuracy:\t\t95.52 %\n",
      "Epoch 79 of 500 took 1.566s\n",
      "  training loss:\t\t0.057049\n",
      "  validation loss:\t\t0.187088\n",
      "  validation accuracy:\t\t95.50 %\n",
      "Epoch 80 of 500 took 1.607s\n",
      "  training loss:\t\t0.056204\n",
      "  validation loss:\t\t0.167695\n",
      "  validation accuracy:\t\t95.72 %\n",
      "Epoch 81 of 500 took 1.478s\n",
      "  training loss:\t\t0.054930\n",
      "  validation loss:\t\t0.224211\n",
      "  validation accuracy:\t\t95.58 %\n",
      "Epoch 82 of 500 took 1.504s\n",
      "  training loss:\t\t0.061170\n",
      "  validation loss:\t\t0.199225\n",
      "  validation accuracy:\t\t95.68 %\n",
      "Epoch 83 of 500 took 1.522s\n",
      "  training loss:\t\t0.057407\n",
      "  validation loss:\t\t0.189403\n",
      "  validation accuracy:\t\t95.62 %\n",
      "Epoch 84 of 500 took 1.649s\n",
      "  training loss:\t\t0.054509\n",
      "  validation loss:\t\t0.188533\n",
      "  validation accuracy:\t\t95.60 %\n",
      "Epoch 85 of 500 took 1.599s\n",
      "  training loss:\t\t0.057224\n",
      "  validation loss:\t\t0.192992\n",
      "  validation accuracy:\t\t95.66 %\n",
      "Epoch 86 of 500 took 1.601s\n",
      "  training loss:\t\t0.058733\n",
      "  validation loss:\t\t0.164475\n",
      "  validation accuracy:\t\t95.78 %\n",
      "Epoch 87 of 500 took 1.487s\n",
      "  training loss:\t\t0.057863\n",
      "  validation loss:\t\t0.179840\n",
      "  validation accuracy:\t\t95.74 %\n",
      "Epoch 88 of 500 took 1.570s\n",
      "  training loss:\t\t0.056649\n",
      "  validation loss:\t\t0.175602\n",
      "  validation accuracy:\t\t95.44 %\n",
      "Epoch 89 of 500 took 1.634s\n",
      "  training loss:\t\t0.056439\n",
      "  validation loss:\t\t0.171229\n",
      "  validation accuracy:\t\t95.72 %\n",
      "Epoch 90 of 500 took 1.525s\n",
      "  training loss:\t\t0.054641\n",
      "  validation loss:\t\t0.171035\n",
      "  validation accuracy:\t\t95.58 %\n",
      "Epoch 91 of 500 took 1.565s\n",
      "  training loss:\t\t0.054054\n",
      "  validation loss:\t\t0.174830\n",
      "  validation accuracy:\t\t95.48 %\n",
      "Epoch 92 of 500 took 1.702s\n",
      "  training loss:\t\t0.053626\n",
      "  validation loss:\t\t0.180184\n",
      "  validation accuracy:\t\t95.54 %\n",
      "Epoch 93 of 500 took 1.613s\n",
      "  training loss:\t\t0.051297\n",
      "  validation loss:\t\t0.175610\n",
      "  validation accuracy:\t\t95.42 %\n",
      "Epoch 94 of 500 took 1.570s\n",
      "  training loss:\t\t0.052839\n",
      "  validation loss:\t\t0.239979\n",
      "  validation accuracy:\t\t95.50 %\n",
      "Epoch 95 of 500 took 1.659s\n",
      "  training loss:\t\t0.054226\n",
      "  validation loss:\t\t0.191344\n",
      "  validation accuracy:\t\t95.58 %\n",
      "Epoch 96 of 500 took 1.541s\n",
      "  training loss:\t\t0.050106\n",
      "  validation loss:\t\t0.194741\n",
      "  validation accuracy:\t\t95.58 %\n",
      "Epoch 97 of 500 took 1.517s\n",
      "  training loss:\t\t0.047850\n",
      "  validation loss:\t\t0.197388\n",
      "  validation accuracy:\t\t95.62 %\n",
      "Epoch 98 of 500 took 1.618s\n",
      "  training loss:\t\t0.052297\n",
      "  validation loss:\t\t0.185685\n",
      "  validation accuracy:\t\t95.62 %\n",
      "Epoch 99 of 500 took 1.695s\n",
      "  training loss:\t\t0.049742\n",
      "  validation loss:\t\t0.179317\n",
      "  validation accuracy:\t\t95.60 %\n",
      "Epoch 100 of 500 took 1.555s\n",
      "  training loss:\t\t0.048313\n",
      "  validation loss:\t\t0.185108\n",
      "  validation accuracy:\t\t95.68 %\n",
      "Epoch 101 of 500 took 1.544s\n",
      "  training loss:\t\t0.048489\n",
      "  validation loss:\t\t0.188110\n",
      "  validation accuracy:\t\t95.74 %\n",
      "Epoch 102 of 500 took 1.540s\n",
      "  training loss:\t\t0.050344\n",
      "  validation loss:\t\t0.186796\n",
      "  validation accuracy:\t\t95.64 %\n",
      "Epoch 103 of 500 took 1.531s\n",
      "  training loss:\t\t0.048993\n",
      "  validation loss:\t\t0.186973\n",
      "  validation accuracy:\t\t95.56 %\n",
      "Epoch 104 of 500 took 1.519s\n",
      "  training loss:\t\t0.048491\n",
      "  validation loss:\t\t0.189549\n",
      "  validation accuracy:\t\t95.60 %\n",
      "Epoch 105 of 500 took 1.579s\n",
      "  training loss:\t\t0.045782\n",
      "  validation loss:\t\t0.188064\n",
      "  validation accuracy:\t\t95.72 %\n",
      "Epoch 106 of 500 took 1.537s\n",
      "  training loss:\t\t0.047741\n",
      "  validation loss:\t\t0.192195\n",
      "  validation accuracy:\t\t95.64 %\n",
      "Epoch 107 of 500 took 1.622s\n",
      "  training loss:\t\t0.046451\n",
      "  validation loss:\t\t0.193224\n",
      "  validation accuracy:\t\t95.56 %\n",
      "Epoch 108 of 500 took 1.538s\n",
      "  training loss:\t\t0.045242\n",
      "  validation loss:\t\t0.193960\n",
      "  validation accuracy:\t\t95.48 %\n",
      "Epoch 109 of 500 took 1.485s\n",
      "  training loss:\t\t0.045806\n",
      "  validation loss:\t\t0.190902\n",
      "  validation accuracy:\t\t95.48 %\n",
      "Epoch 110 of 500 took 1.528s\n",
      "  training loss:\t\t0.047076\n",
      "  validation loss:\t\t0.197089\n",
      "  validation accuracy:\t\t95.58 %\n",
      "Epoch 111 of 500 took 1.501s\n",
      "  training loss:\t\t0.045416\n",
      "  validation loss:\t\t0.181933\n",
      "  validation accuracy:\t\t95.60 %\n",
      "Epoch 112 of 500 took 1.543s\n",
      "  training loss:\t\t0.044804\n",
      "  validation loss:\t\t0.177091\n",
      "  validation accuracy:\t\t95.68 %\n",
      "Epoch 113 of 500 took 1.553s\n",
      "  training loss:\t\t0.045215\n",
      "  validation loss:\t\t0.306744\n",
      "  validation accuracy:\t\t95.92 %\n",
      "Epoch 114 of 500 took 1.498s\n",
      "  training loss:\t\t0.045737\n",
      "  validation loss:\t\t0.218729\n",
      "  validation accuracy:\t\t95.48 %\n",
      "Epoch 115 of 500 took 1.496s\n",
      "  training loss:\t\t0.044890\n",
      "  validation loss:\t\t0.217368\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 116 of 500 took 1.506s\n",
      "  training loss:\t\t0.043737\n",
      "  validation loss:\t\t0.222587\n",
      "  validation accuracy:\t\t95.66 %\n",
      "Epoch 117 of 500 took 1.485s\n",
      "  training loss:\t\t0.046085\n",
      "  validation loss:\t\t0.225902\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 118 of 500 took 1.563s\n",
      "  training loss:\t\t0.045090\n",
      "  validation loss:\t\t0.199455\n",
      "  validation accuracy:\t\t95.66 %\n",
      "Epoch 119 of 500 took 1.525s\n",
      "  training loss:\t\t0.047389\n",
      "  validation loss:\t\t0.190595\n",
      "  validation accuracy:\t\t95.62 %\n",
      "Epoch 120 of 500 took 1.627s\n",
      "  training loss:\t\t0.046688\n",
      "  validation loss:\t\t0.305134\n",
      "  validation accuracy:\t\t95.68 %\n",
      "Epoch 121 of 500 took 1.549s\n",
      "  training loss:\t\t0.045329\n",
      "  validation loss:\t\t0.232090\n",
      "  validation accuracy:\t\t95.78 %\n",
      "Epoch 122 of 500 took 1.486s\n",
      "  training loss:\t\t0.040419\n",
      "  validation loss:\t\t0.177747\n",
      "  validation accuracy:\t\t95.52 %\n",
      "Epoch 123 of 500 took 1.493s\n",
      "  training loss:\t\t0.040374\n",
      "  validation loss:\t\t0.180888\n",
      "  validation accuracy:\t\t95.72 %\n",
      "Epoch 124 of 500 took 1.555s\n",
      "  training loss:\t\t0.042633\n",
      "  validation loss:\t\t0.183110\n",
      "  validation accuracy:\t\t95.62 %\n",
      "Epoch 125 of 500 took 1.572s\n",
      "  training loss:\t\t0.041354\n",
      "  validation loss:\t\t0.183638\n",
      "  validation accuracy:\t\t95.78 %\n",
      "Epoch 126 of 500 took 1.560s\n",
      "  training loss:\t\t0.040451\n",
      "  validation loss:\t\t0.185011\n",
      "  validation accuracy:\t\t95.64 %\n",
      "Epoch 127 of 500 took 1.577s\n",
      "  training loss:\t\t0.044542\n",
      "  validation loss:\t\t0.313020\n",
      "  validation accuracy:\t\t95.88 %\n",
      "Epoch 128 of 500 took 1.549s\n",
      "  training loss:\t\t0.040691\n",
      "  validation loss:\t\t0.257945\n",
      "  validation accuracy:\t\t95.64 %\n",
      "Epoch 129 of 500 took 1.574s\n",
      "  training loss:\t\t0.041410\n",
      "  validation loss:\t\t0.187068\n",
      "  validation accuracy:\t\t95.58 %\n",
      "Epoch 130 of 500 took 1.519s\n",
      "  training loss:\t\t0.046549\n",
      "  validation loss:\t\t0.304073\n",
      "  validation accuracy:\t\t95.86 %\n",
      "Epoch 131 of 500 took 1.561s\n",
      "  training loss:\t\t0.040703\n",
      "  validation loss:\t\t0.240686\n",
      "  validation accuracy:\t\t95.64 %\n",
      "Epoch 132 of 500 took 1.586s\n",
      "  training loss:\t\t0.037703\n",
      "  validation loss:\t\t0.247283\n",
      "  validation accuracy:\t\t95.44 %\n",
      "Epoch 133 of 500 took 1.545s\n",
      "  training loss:\t\t0.040228\n",
      "  validation loss:\t\t0.195054\n",
      "  validation accuracy:\t\t95.56 %\n",
      "Epoch 134 of 500 took 1.586s\n",
      "  training loss:\t\t0.041294\n",
      "  validation loss:\t\t0.193996\n",
      "  validation accuracy:\t\t95.54 %\n",
      "Epoch 135 of 500 took 1.523s\n",
      "  training loss:\t\t0.037374\n",
      "  validation loss:\t\t0.197792\n",
      "  validation accuracy:\t\t95.86 %\n",
      "Epoch 136 of 500 took 1.580s\n",
      "  training loss:\t\t0.037856\n",
      "  validation loss:\t\t0.198468\n",
      "  validation accuracy:\t\t95.72 %\n",
      "Epoch 137 of 500 took 1.514s\n",
      "  training loss:\t\t0.041682\n",
      "  validation loss:\t\t0.235453\n",
      "  validation accuracy:\t\t95.52 %\n",
      "Epoch 138 of 500 took 1.544s\n",
      "  training loss:\t\t0.038304\n",
      "  validation loss:\t\t0.220671\n",
      "  validation accuracy:\t\t95.66 %\n",
      "Epoch 139 of 500 took 1.532s\n",
      "  training loss:\t\t0.040338\n",
      "  validation loss:\t\t0.282333\n",
      "  validation accuracy:\t\t95.60 %\n",
      "Epoch 140 of 500 took 1.526s\n",
      "  training loss:\t\t0.036169\n",
      "  validation loss:\t\t0.259454\n",
      "  validation accuracy:\t\t95.54 %\n",
      "Epoch 141 of 500 took 1.583s\n",
      "  training loss:\t\t0.038775\n",
      "  validation loss:\t\t0.220483\n",
      "  validation accuracy:\t\t95.56 %\n",
      "Epoch 142 of 500 took 1.529s\n",
      "  training loss:\t\t0.036729\n",
      "  validation loss:\t\t0.225835\n",
      "  validation accuracy:\t\t95.44 %\n",
      "Epoch 143 of 500 took 1.541s\n",
      "  training loss:\t\t0.037581\n",
      "  validation loss:\t\t0.228905\n",
      "  validation accuracy:\t\t95.46 %\n",
      "Epoch 144 of 500 took 1.571s\n",
      "  training loss:\t\t0.034779\n",
      "  validation loss:\t\t0.230510\n",
      "  validation accuracy:\t\t95.58 %\n",
      "Epoch 145 of 500 took 1.576s\n",
      "  training loss:\t\t0.038140\n",
      "  validation loss:\t\t0.227635\n",
      "  validation accuracy:\t\t95.72 %\n",
      "Epoch 146 of 500 took 1.610s\n",
      "  training loss:\t\t0.034576\n",
      "  validation loss:\t\t0.230141\n",
      "  validation accuracy:\t\t95.44 %\n",
      "Epoch 147 of 500 took 1.534s\n",
      "  training loss:\t\t0.035860\n",
      "  validation loss:\t\t0.229016\n",
      "  validation accuracy:\t\t95.66 %\n",
      "Epoch 148 of 500 took 1.530s\n",
      "  training loss:\t\t0.037088\n",
      "  validation loss:\t\t0.230229\n",
      "  validation accuracy:\t\t95.66 %\n",
      "Epoch 149 of 500 took 1.507s\n",
      "  training loss:\t\t0.037362\n",
      "  validation loss:\t\t0.228392\n",
      "  validation accuracy:\t\t95.58 %\n",
      "Epoch 150 of 500 took 1.526s\n",
      "  training loss:\t\t0.036104\n",
      "  validation loss:\t\t0.235047\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 151 of 500 took 1.650s\n",
      "  training loss:\t\t0.036472\n",
      "  validation loss:\t\t0.268116\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 152 of 500 took 1.533s\n",
      "  training loss:\t\t0.035759\n",
      "  validation loss:\t\t0.267530\n",
      "  validation accuracy:\t\t95.60 %\n",
      "Epoch 153 of 500 took 1.526s\n",
      "  training loss:\t\t0.035679\n",
      "  validation loss:\t\t0.239108\n",
      "  validation accuracy:\t\t95.78 %\n",
      "Epoch 154 of 500 took 1.541s\n",
      "  training loss:\t\t0.038253\n",
      "  validation loss:\t\t0.247096\n",
      "  validation accuracy:\t\t95.64 %\n",
      "Epoch 155 of 500 took 1.537s\n",
      "  training loss:\t\t0.036386\n",
      "  validation loss:\t\t0.248121\n",
      "  validation accuracy:\t\t95.64 %\n",
      "Epoch 156 of 500 took 1.517s\n",
      "  training loss:\t\t0.037785\n",
      "  validation loss:\t\t0.248778\n",
      "  validation accuracy:\t\t95.44 %\n",
      "Epoch 157 of 500 took 1.553s\n",
      "  training loss:\t\t0.036060\n",
      "  validation loss:\t\t0.224515\n",
      "  validation accuracy:\t\t95.82 %\n",
      "Epoch 158 of 500 took 1.563s\n",
      "  training loss:\t\t0.033525\n",
      "  validation loss:\t\t0.223376\n",
      "  validation accuracy:\t\t95.54 %\n",
      "Epoch 159 of 500 took 1.502s\n",
      "  training loss:\t\t0.034100\n",
      "  validation loss:\t\t0.229294\n",
      "  validation accuracy:\t\t95.60 %\n",
      "Epoch 160 of 500 took 1.529s\n",
      "  training loss:\t\t0.033371\n",
      "  validation loss:\t\t0.232274\n",
      "  validation accuracy:\t\t95.62 %\n",
      "Epoch 161 of 500 took 1.511s\n",
      "  training loss:\t\t0.032360\n",
      "  validation loss:\t\t0.235276\n",
      "  validation accuracy:\t\t95.66 %\n",
      "Epoch 162 of 500 took 1.557s\n",
      "  training loss:\t\t0.036709\n",
      "  validation loss:\t\t0.227630\n",
      "  validation accuracy:\t\t95.62 %\n",
      "Epoch 163 of 500 took 1.502s\n",
      "  training loss:\t\t0.037173\n",
      "  validation loss:\t\t0.226899\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 164 of 500 took 1.552s\n",
      "  training loss:\t\t0.034214\n",
      "  validation loss:\t\t0.236373\n",
      "  validation accuracy:\t\t95.62 %\n",
      "Epoch 165 of 500 took 1.513s\n",
      "  training loss:\t\t0.033638\n",
      "  validation loss:\t\t0.234895\n",
      "  validation accuracy:\t\t95.68 %\n",
      "Epoch 166 of 500 took 1.618s\n",
      "  training loss:\t\t0.036396\n",
      "  validation loss:\t\t0.230503\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 167 of 500 took 1.472s\n",
      "  training loss:\t\t0.034918\n",
      "  validation loss:\t\t0.233640\n",
      "  validation accuracy:\t\t95.76 %\n",
      "Epoch 168 of 500 took 1.532s\n",
      "  training loss:\t\t0.033835\n",
      "  validation loss:\t\t0.230876\n",
      "  validation accuracy:\t\t95.74 %\n",
      "Epoch 169 of 500 took 1.515s\n",
      "  training loss:\t\t0.031516\n",
      "  validation loss:\t\t0.230520\n",
      "  validation accuracy:\t\t95.82 %\n",
      "Epoch 170 of 500 took 1.553s\n",
      "  training loss:\t\t0.032963\n",
      "  validation loss:\t\t0.231289\n",
      "  validation accuracy:\t\t95.76 %\n",
      "Epoch 171 of 500 took 1.560s\n",
      "  training loss:\t\t0.032365\n",
      "  validation loss:\t\t0.229453\n",
      "  validation accuracy:\t\t95.78 %\n",
      "Epoch 172 of 500 took 1.565s\n",
      "  training loss:\t\t0.032411\n",
      "  validation loss:\t\t0.234983\n",
      "  validation accuracy:\t\t95.68 %\n",
      "Epoch 173 of 500 took 1.492s\n",
      "  training loss:\t\t0.033308\n",
      "  validation loss:\t\t0.230534\n",
      "  validation accuracy:\t\t95.74 %\n",
      "Epoch 174 of 500 took 1.538s\n",
      "  training loss:\t\t0.033653\n",
      "  validation loss:\t\t0.201289\n",
      "  validation accuracy:\t\t95.64 %\n",
      "Epoch 175 of 500 took 1.542s\n",
      "  training loss:\t\t0.037606\n",
      "  validation loss:\t\t0.280939\n",
      "  validation accuracy:\t\t95.82 %\n",
      "Epoch 176 of 500 took 1.568s\n",
      "  training loss:\t\t0.031889\n",
      "  validation loss:\t\t0.255850\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 177 of 500 took 1.618s\n",
      "  training loss:\t\t0.033454\n",
      "  validation loss:\t\t0.228112\n",
      "  validation accuracy:\t\t95.68 %\n",
      "Epoch 178 of 500 took 1.561s\n",
      "  training loss:\t\t0.032122\n",
      "  validation loss:\t\t0.232140\n",
      "  validation accuracy:\t\t95.72 %\n",
      "Epoch 179 of 500 took 1.550s\n",
      "  training loss:\t\t0.033868\n",
      "  validation loss:\t\t0.234133\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 180 of 500 took 1.504s\n",
      "  training loss:\t\t0.031862\n",
      "  validation loss:\t\t0.236773\n",
      "  validation accuracy:\t\t95.64 %\n",
      "Epoch 181 of 500 took 1.525s\n",
      "  training loss:\t\t0.031913\n",
      "  validation loss:\t\t0.236257\n",
      "  validation accuracy:\t\t95.74 %\n",
      "Epoch 182 of 500 took 1.550s\n",
      "  training loss:\t\t0.030162\n",
      "  validation loss:\t\t0.236694\n",
      "  validation accuracy:\t\t95.64 %\n",
      "Epoch 183 of 500 took 1.534s\n",
      "  training loss:\t\t0.033640\n",
      "  validation loss:\t\t0.243173\n",
      "  validation accuracy:\t\t95.74 %\n",
      "Epoch 184 of 500 took 1.510s\n",
      "  training loss:\t\t0.034998\n",
      "  validation loss:\t\t0.241729\n",
      "  validation accuracy:\t\t95.96 %\n",
      "Epoch 185 of 500 took 1.504s\n",
      "  training loss:\t\t0.030746\n",
      "  validation loss:\t\t0.237018\n",
      "  validation accuracy:\t\t95.68 %\n",
      "Epoch 186 of 500 took 1.610s\n",
      "  training loss:\t\t0.031472\n",
      "  validation loss:\t\t0.245043\n",
      "  validation accuracy:\t\t95.74 %\n",
      "Epoch 187 of 500 took 1.581s\n",
      "  training loss:\t\t0.030442\n",
      "  validation loss:\t\t0.241354\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 188 of 500 took 1.526s\n",
      "  training loss:\t\t0.029730\n",
      "  validation loss:\t\t0.244456\n",
      "  validation accuracy:\t\t95.68 %\n",
      "Epoch 189 of 500 took 1.498s\n",
      "  training loss:\t\t0.029658\n",
      "  validation loss:\t\t0.243660\n",
      "  validation accuracy:\t\t95.58 %\n",
      "Epoch 190 of 500 took 1.483s\n",
      "  training loss:\t\t0.030377\n",
      "  validation loss:\t\t0.244910\n",
      "  validation accuracy:\t\t95.68 %\n",
      "Epoch 191 of 500 took 1.502s\n",
      "  training loss:\t\t0.030189\n",
      "  validation loss:\t\t0.245508\n",
      "  validation accuracy:\t\t95.46 %\n",
      "Epoch 192 of 500 took 1.506s\n",
      "  training loss:\t\t0.029228\n",
      "  validation loss:\t\t0.238743\n",
      "  validation accuracy:\t\t95.86 %\n",
      "Epoch 193 of 500 took 1.610s\n",
      "  training loss:\t\t0.028540\n",
      "  validation loss:\t\t0.242916\n",
      "  validation accuracy:\t\t95.86 %\n",
      "Epoch 194 of 500 took 1.542s\n",
      "  training loss:\t\t0.029336\n",
      "  validation loss:\t\t0.248513\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 195 of 500 took 1.542s\n",
      "  training loss:\t\t0.031848\n",
      "  validation loss:\t\t0.242954\n",
      "  validation accuracy:\t\t95.86 %\n",
      "Epoch 196 of 500 took 1.483s\n",
      "  training loss:\t\t0.027512\n",
      "  validation loss:\t\t0.239673\n",
      "  validation accuracy:\t\t95.92 %\n",
      "Epoch 197 of 500 took 1.529s\n",
      "  training loss:\t\t0.028215\n",
      "  validation loss:\t\t0.237062\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 198 of 500 took 1.542s\n",
      "  training loss:\t\t0.027995\n",
      "  validation loss:\t\t0.234823\n",
      "  validation accuracy:\t\t95.94 %\n",
      "Epoch 199 of 500 took 1.512s\n",
      "  training loss:\t\t0.029201\n",
      "  validation loss:\t\t0.245124\n",
      "  validation accuracy:\t\t95.74 %\n",
      "Epoch 200 of 500 took 1.526s\n",
      "  training loss:\t\t0.026374\n",
      "  validation loss:\t\t0.243713\n",
      "  validation accuracy:\t\t95.76 %\n",
      "Epoch 201 of 500 took 1.620s\n",
      "  training loss:\t\t0.029925\n",
      "  validation loss:\t\t0.241835\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 202 of 500 took 1.514s\n",
      "  training loss:\t\t0.027432\n",
      "  validation loss:\t\t0.240958\n",
      "  validation accuracy:\t\t95.90 %\n",
      "Epoch 203 of 500 took 1.492s\n",
      "  training loss:\t\t0.027727\n",
      "  validation loss:\t\t0.225158\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 204 of 500 took 1.519s\n",
      "  training loss:\t\t0.028100\n",
      "  validation loss:\t\t0.227266\n",
      "  validation accuracy:\t\t95.84 %\n",
      "Epoch 205 of 500 took 1.529s\n",
      "  training loss:\t\t0.026692\n",
      "  validation loss:\t\t0.223562\n",
      "  validation accuracy:\t\t95.86 %\n",
      "Epoch 206 of 500 took 1.584s\n",
      "  training loss:\t\t0.028791\n",
      "  validation loss:\t\t0.225080\n",
      "  validation accuracy:\t\t95.78 %\n",
      "Epoch 207 of 500 took 1.511s\n",
      "  training loss:\t\t0.028470\n",
      "  validation loss:\t\t0.229180\n",
      "  validation accuracy:\t\t95.54 %\n",
      "Epoch 208 of 500 took 1.570s\n",
      "  training loss:\t\t0.026659\n",
      "  validation loss:\t\t0.231130\n",
      "  validation accuracy:\t\t95.64 %\n",
      "Epoch 209 of 500 took 1.541s\n",
      "  training loss:\t\t0.026335\n",
      "  validation loss:\t\t0.231820\n",
      "  validation accuracy:\t\t95.68 %\n",
      "Epoch 210 of 500 took 1.504s\n",
      "  training loss:\t\t0.029544\n",
      "  validation loss:\t\t0.232687\n",
      "  validation accuracy:\t\t95.90 %\n",
      "Epoch 211 of 500 took 1.494s\n",
      "  training loss:\t\t0.028742\n",
      "  validation loss:\t\t0.240649\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 212 of 500 took 1.616s\n",
      "  training loss:\t\t0.026174\n",
      "  validation loss:\t\t0.236766\n",
      "  validation accuracy:\t\t95.72 %\n",
      "Epoch 213 of 500 took 1.534s\n",
      "  training loss:\t\t0.027935\n",
      "  validation loss:\t\t0.237613\n",
      "  validation accuracy:\t\t95.76 %\n",
      "Epoch 214 of 500 took 1.510s\n",
      "  training loss:\t\t0.027281\n",
      "  validation loss:\t\t0.233970\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 215 of 500 took 1.543s\n",
      "  training loss:\t\t0.028673\n",
      "  validation loss:\t\t0.236340\n",
      "  validation accuracy:\t\t95.76 %\n",
      "Epoch 216 of 500 took 1.541s\n",
      "  training loss:\t\t0.026995\n",
      "  validation loss:\t\t0.258365\n",
      "  validation accuracy:\t\t95.74 %\n",
      "Epoch 217 of 500 took 1.649s\n",
      "  training loss:\t\t0.026841\n",
      "  validation loss:\t\t0.254598\n",
      "  validation accuracy:\t\t95.74 %\n",
      "Epoch 218 of 500 took 1.502s\n",
      "  training loss:\t\t0.026602\n",
      "  validation loss:\t\t0.257022\n",
      "  validation accuracy:\t\t95.62 %\n",
      "Epoch 219 of 500 took 1.538s\n",
      "  training loss:\t\t0.028209\n",
      "  validation loss:\t\t0.247507\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 220 of 500 took 1.567s\n",
      "  training loss:\t\t0.026721\n",
      "  validation loss:\t\t0.257532\n",
      "  validation accuracy:\t\t95.46 %\n",
      "Epoch 221 of 500 took 1.504s\n",
      "  training loss:\t\t0.028730\n",
      "  validation loss:\t\t0.251331\n",
      "  validation accuracy:\t\t95.42 %\n",
      "Epoch 222 of 500 took 1.506s\n",
      "  training loss:\t\t0.026998\n",
      "  validation loss:\t\t0.244034\n",
      "  validation accuracy:\t\t95.76 %\n",
      "Epoch 223 of 500 took 1.527s\n",
      "  training loss:\t\t0.025782\n",
      "  validation loss:\t\t0.252444\n",
      "  validation accuracy:\t\t95.60 %\n",
      "Epoch 224 of 500 took 1.627s\n",
      "  training loss:\t\t0.024843\n",
      "  validation loss:\t\t0.258020\n",
      "  validation accuracy:\t\t95.36 %\n",
      "Epoch 225 of 500 took 1.507s\n",
      "  training loss:\t\t0.027576\n",
      "  validation loss:\t\t0.249098\n",
      "  validation accuracy:\t\t95.76 %\n",
      "Epoch 226 of 500 took 1.583s\n",
      "  training loss:\t\t0.027018\n",
      "  validation loss:\t\t0.253985\n",
      "  validation accuracy:\t\t95.60 %\n",
      "Epoch 227 of 500 took 1.518s\n",
      "  training loss:\t\t0.026430\n",
      "  validation loss:\t\t0.245174\n",
      "  validation accuracy:\t\t95.64 %\n",
      "Epoch 228 of 500 took 1.583s\n",
      "  training loss:\t\t0.029663\n",
      "  validation loss:\t\t0.248753\n",
      "  validation accuracy:\t\t95.72 %\n",
      "Epoch 229 of 500 took 1.552s\n",
      "  training loss:\t\t0.025247\n",
      "  validation loss:\t\t0.383820\n",
      "  validation accuracy:\t\t95.68 %\n",
      "Epoch 230 of 500 took 1.545s\n",
      "  training loss:\t\t0.028289\n",
      "  validation loss:\t\t0.275143\n",
      "  validation accuracy:\t\t95.66 %\n",
      "Epoch 231 of 500 took 1.586s\n",
      "  training loss:\t\t0.025210\n",
      "  validation loss:\t\t0.272455\n",
      "  validation accuracy:\t\t95.66 %\n",
      "Epoch 232 of 500 took 1.534s\n",
      "  training loss:\t\t0.028498\n",
      "  validation loss:\t\t0.280559\n",
      "  validation accuracy:\t\t95.54 %\n",
      "Epoch 233 of 500 took 1.557s\n",
      "  training loss:\t\t0.026895\n",
      "  validation loss:\t\t0.271610\n",
      "  validation accuracy:\t\t95.46 %\n",
      "Epoch 234 of 500 took 1.539s\n",
      "  training loss:\t\t0.027248\n",
      "  validation loss:\t\t0.276250\n",
      "  validation accuracy:\t\t95.44 %\n",
      "Epoch 235 of 500 took 1.537s\n",
      "  training loss:\t\t0.026152\n",
      "  validation loss:\t\t0.273428\n",
      "  validation accuracy:\t\t95.50 %\n",
      "Epoch 236 of 500 took 1.578s\n",
      "  training loss:\t\t0.028982\n",
      "  validation loss:\t\t0.277393\n",
      "  validation accuracy:\t\t95.40 %\n",
      "Epoch 237 of 500 took 1.506s\n",
      "  training loss:\t\t0.025401\n",
      "  validation loss:\t\t0.281218\n",
      "  validation accuracy:\t\t95.54 %\n",
      "Epoch 238 of 500 took 1.620s\n",
      "  training loss:\t\t0.026725\n",
      "  validation loss:\t\t0.275303\n",
      "  validation accuracy:\t\t95.58 %\n",
      "Epoch 239 of 500 took 1.510s\n",
      "  training loss:\t\t0.024258\n",
      "  validation loss:\t\t0.278450\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 240 of 500 took 1.517s\n",
      "  training loss:\t\t0.024581\n",
      "  validation loss:\t\t0.272402\n",
      "  validation accuracy:\t\t95.76 %\n",
      "Epoch 241 of 500 took 1.579s\n",
      "  training loss:\t\t0.026565\n",
      "  validation loss:\t\t0.272520\n",
      "  validation accuracy:\t\t95.86 %\n",
      "Epoch 242 of 500 took 1.564s\n",
      "  training loss:\t\t0.026170\n",
      "  validation loss:\t\t0.278014\n",
      "  validation accuracy:\t\t95.74 %\n",
      "Epoch 243 of 500 took 1.601s\n",
      "  training loss:\t\t0.026000\n",
      "  validation loss:\t\t0.275804\n",
      "  validation accuracy:\t\t95.48 %\n",
      "Epoch 244 of 500 took 1.559s\n",
      "  training loss:\t\t0.025182\n",
      "  validation loss:\t\t0.272598\n",
      "  validation accuracy:\t\t95.50 %\n",
      "Epoch 245 of 500 took 1.571s\n",
      "  training loss:\t\t0.025037\n",
      "  validation loss:\t\t0.274821\n",
      "  validation accuracy:\t\t95.56 %\n",
      "Epoch 246 of 500 took 1.522s\n",
      "  training loss:\t\t0.025753\n",
      "  validation loss:\t\t0.267972\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 247 of 500 took 1.553s\n",
      "  training loss:\t\t0.024467\n",
      "  validation loss:\t\t0.270984\n",
      "  validation accuracy:\t\t95.72 %\n",
      "Epoch 248 of 500 took 1.613s\n",
      "  training loss:\t\t0.024227\n",
      "  validation loss:\t\t0.268472\n",
      "  validation accuracy:\t\t95.82 %\n",
      "Epoch 249 of 500 took 1.555s\n",
      "  training loss:\t\t0.025909\n",
      "  validation loss:\t\t0.275034\n",
      "  validation accuracy:\t\t95.76 %\n",
      "Epoch 250 of 500 took 1.524s\n",
      "  training loss:\t\t0.025172\n",
      "  validation loss:\t\t0.271411\n",
      "  validation accuracy:\t\t95.66 %\n",
      "Epoch 251 of 500 took 1.555s\n",
      "  training loss:\t\t0.025178\n",
      "  validation loss:\t\t0.272754\n",
      "  validation accuracy:\t\t95.78 %\n",
      "Epoch 252 of 500 took 1.510s\n",
      "  training loss:\t\t0.024251\n",
      "  validation loss:\t\t0.267067\n",
      "  validation accuracy:\t\t95.66 %\n",
      "Epoch 253 of 500 took 1.571s\n",
      "  training loss:\t\t0.024958\n",
      "  validation loss:\t\t0.266598\n",
      "  validation accuracy:\t\t95.78 %\n",
      "Epoch 254 of 500 took 1.537s\n",
      "  training loss:\t\t0.023293\n",
      "  validation loss:\t\t0.269807\n",
      "  validation accuracy:\t\t95.68 %\n",
      "Epoch 255 of 500 took 1.544s\n",
      "  training loss:\t\t0.025668\n",
      "  validation loss:\t\t0.267931\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 256 of 500 took 1.543s\n",
      "  training loss:\t\t0.021268\n",
      "  validation loss:\t\t0.261664\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 257 of 500 took 1.484s\n",
      "  training loss:\t\t0.023554\n",
      "  validation loss:\t\t0.263147\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 258 of 500 took 1.589s\n",
      "  training loss:\t\t0.024642\n",
      "  validation loss:\t\t0.261796\n",
      "  validation accuracy:\t\t95.78 %\n",
      "Epoch 259 of 500 took 1.553s\n",
      "  training loss:\t\t0.024817\n",
      "  validation loss:\t\t0.262422\n",
      "  validation accuracy:\t\t95.82 %\n",
      "Epoch 260 of 500 took 1.552s\n",
      "  training loss:\t\t0.021359\n",
      "  validation loss:\t\t0.264504\n",
      "  validation accuracy:\t\t95.66 %\n",
      "Epoch 261 of 500 took 1.529s\n",
      "  training loss:\t\t0.026287\n",
      "  validation loss:\t\t0.253290\n",
      "  validation accuracy:\t\t95.88 %\n",
      "Epoch 262 of 500 took 1.509s\n",
      "  training loss:\t\t0.024661\n",
      "  validation loss:\t\t0.264838\n",
      "  validation accuracy:\t\t95.62 %\n",
      "Epoch 263 of 500 took 1.557s\n",
      "  training loss:\t\t0.026736\n",
      "  validation loss:\t\t0.262683\n",
      "  validation accuracy:\t\t95.74 %\n",
      "Epoch 264 of 500 took 1.516s\n",
      "  training loss:\t\t0.028470\n",
      "  validation loss:\t\t0.255701\n",
      "  validation accuracy:\t\t95.84 %\n",
      "Epoch 265 of 500 took 1.511s\n",
      "  training loss:\t\t0.026222\n",
      "  validation loss:\t\t0.254105\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 266 of 500 took 1.517s\n",
      "  training loss:\t\t0.024808\n",
      "  validation loss:\t\t0.256505\n",
      "  validation accuracy:\t\t95.86 %\n",
      "Epoch 267 of 500 took 1.540s\n",
      "  training loss:\t\t0.022925\n",
      "  validation loss:\t\t0.265104\n",
      "  validation accuracy:\t\t95.74 %\n",
      "Epoch 268 of 500 took 1.511s\n",
      "  training loss:\t\t0.023874\n",
      "  validation loss:\t\t0.263603\n",
      "  validation accuracy:\t\t95.80 %\n",
      "Epoch 269 of 500 took 1.554s\n",
      "  training loss:\t\t0.024057\n",
      "  validation loss:\t\t0.272118\n",
      "  validation accuracy:\t\t95.86 %\n",
      "Epoch 270 of 500 took 1.601s\n",
      "  training loss:\t\t0.024485\n",
      "  validation loss:\t\t0.259680\n",
      "  validation accuracy:\t\t95.86 %\n",
      "Epoch 271 of 500 took 1.824s\n",
      "  training loss:\t\t0.022292\n",
      "  validation loss:\t\t0.261612\n",
      "  validation accuracy:\t\t95.82 %\n",
      "Epoch 272 of 500 took 1.699s\n",
      "  training loss:\t\t0.022593\n",
      "  validation loss:\t\t0.266152\n",
      "  validation accuracy:\t\t95.58 %\n",
      "Epoch 273 of 500 took 1.881s\n",
      "  training loss:\t\t0.021519\n",
      "  validation loss:\t\t0.265166\n",
      "  validation accuracy:\t\t95.78 %\n",
      "Epoch 274 of 500 took 2.398s\n",
      "  training loss:\t\t0.022485\n",
      "  validation loss:\t\t0.266888\n",
      "  validation accuracy:\t\t95.78 %\n",
      "Epoch 275 of 500 took 2.121s\n",
      "  training loss:\t\t0.022437\n",
      "  validation loss:\t\t0.267004\n",
      "  validation accuracy:\t\t95.84 %\n",
      "Epoch 276 of 500 took 1.843s\n",
      "  training loss:\t\t0.023531\n",
      "  validation loss:\t\t0.264202\n",
      "  validation accuracy:\t\t95.58 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-1399c9bb33ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_NN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mla1p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mla2p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mla3p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-53-def21ad9f6aa>\u001b[0m in \u001b[0;36mtrain_NN\u001b[0;34m(num_epochs, width, d_i, d_h, l_r, la1p, la2p, la3p)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterate_minibatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0mtrain_err\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0mtrain_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Camilo/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    893\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Camilo/anaconda2/lib/python2.7/site-packages/theano/gof/op.pyc\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n)\u001b[0m\n\u001b[1;32m    903\u001b[0m             \u001b[0;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 905\u001b[0;31m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    906\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m                     \u001b[0mcompute_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Camilo/anaconda2/lib/python2.7/site-packages/theano/tensor/blas.pyc\u001b[0m in \u001b[0;36mperform\u001b[0;34m(self, node, inp, out)\u001b[0m\n\u001b[1;32m   1550\u001b[0m         \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1552\u001b[0;31m             \u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1553\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m             \u001b[0;31m# The error raised by numpy has no shape information, we mean to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_NN(num_epochs=500, width = 500, d_i = 0.0, d_h = 0.3, l_r = 0.1, la1p = 0.5, la2p = 0.5, la3p = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x117a9d290>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117a9d7d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scatter(arange(len(percs)),percs)\n",
    "#axis([70,100,0.95,0.97])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.65624736e-06,   5.51162638e-11,   7.83802570e-10,\n",
       "          8.67723025e-10,   9.99997342e-01],\n",
       "       [  3.16494035e-17,   1.32525981e-18,   2.79829011e-09,\n",
       "          9.99999991e-01,   6.49761880e-09],\n",
       "       [  2.83983952e-07,   3.41552425e-11,   5.52019690e-09,\n",
       "          9.82585330e-01,   1.74143802e-02],\n",
       "       ..., \n",
       "       [  1.00000000e+00,   1.25255476e-23,   8.75010059e-28,\n",
       "          7.05382549e-27,   3.88421002e-32],\n",
       "       [  9.99999998e-01,   6.00710407e-17,   4.67632957e-17,\n",
       "          2.23735897e-09,   1.46682642e-10],\n",
       "       [  4.93925598e-09,   1.15504990e-09,   9.99996013e-01,\n",
       "          1.59446014e-07,   3.82154478e-06]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final = result.eval()\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(final[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3894 index: 1339\n"
     ]
    }
   ],
   "source": [
    "y_output = list([])\n",
    "p_output = list([])\n",
    "for i in arange(len(final)):\n",
    "    my_list = final[i,:]\n",
    "    max_index = my_list.argmax()\n",
    "    p_output.append(my_list[max_index])\n",
    "    y_output.append(max_index)\n",
    "p_output = asarray(p_output)\n",
    "mini = p_output.argmin()\n",
    "print('{:.4f} index: {:d}'.format(p_output[mini],mini)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8137"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('result22.csv', 'wb') as fp:\n",
    "    a = csv.writer(fp, delimiter=',')\n",
    "    a.writerow(('Id','y'))\n",
    "    a.writerows(zip(ids,y_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
